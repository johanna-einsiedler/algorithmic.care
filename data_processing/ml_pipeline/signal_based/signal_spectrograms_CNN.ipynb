{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "# cnn model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import glob\n",
    "from skimage import util\n",
    "import pydub\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils')\n",
    "\n",
    "# importing\n",
    "from emg_data_loading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "person ='Amanda'\n",
    "emg_path ='../../../data/data_pre_processed/'\n",
    "survey_file = '../../../data/survey_data/pre_processed_survey_features.csv'\n",
    "survey = pd.read_csv(survey_file)\n",
    "identifiers  = pd.concat([get_dates('Amanda', survey, emg_path, 'hooper_muscles_sore')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader(identifiers,survey,emg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data = data[(data['muscle_contraction_1']==1)|(data['muscle_contraction_2']==1)|(data['muscle_contraction_3']==1)]\n",
    "data = data[(data['sit_relaxed_1']==1)|(data['sit_relaxed_2']==1)]\n",
    "\n",
    "#data = data[data['wallsit']==1]\n",
    "for num, ids in identifiers.iterrows():\n",
    "    date = ids['date']\n",
    "    person = ids['person']\n",
    "    timing = ids['timing']\n",
    "    signal = data.loc[(data['date']==date)&(data['person']==person)&(data['timing']==timing),'signal_emg']\n",
    "    # Lens Size\n",
    "    M = 250\n",
    "    try:\n",
    "        # cut into slices for calculating FFT\n",
    "        slices = util.view_as_windows(np.array(signal), window_shape =(M,), step=int(M/4))\n",
    "\n",
    "        # create a window function\n",
    "        win = np.hanning(M+1)[:-1]\n",
    "        slices = slices * win\n",
    "\n",
    "        # take transpose to have one slice per column\n",
    "        slices = slices.T\n",
    "        # print('Shape of Slices:', slices.shape)\n",
    "\n",
    "        # for each slice calculate DFT and slice out positive values\n",
    "        spectrum = np.fft.fft(slices, axis = 0)[:M // 2]\n",
    "        spectrum = np.abs(spectrum)\n",
    "\n",
    "        # Calculate length of spectrum and number of samples of length 256 in this spectrum\n",
    "        N = spectrum.shape[1]\n",
    "        K = round((N/(M/4))-0.5)\n",
    "\n",
    "        for k in range(1,K-1):\n",
    "\n",
    "            # slice out pieces of size 256 of the spectrum\n",
    "            spectrum_slice = spectrum[:,(k-1)*int(M/4):(k+1)*int(M/4)]\n",
    "\n",
    "            # plot spectrogram\n",
    "            fig = plt.figure() # create plot\n",
    "            S = np.abs(spectrum_slice) # take absolute values\n",
    "            S = 20 * np.log10(S / np.max(S)) # adjust data scale\n",
    "            sizes = np.shape(S)  # get shape of data\n",
    "            fig.set_size_inches(1. * sizes[0] / sizes[1], 1, forward=False) # set figure size\n",
    "            ax = plt.Axes(fig, [0., 0., 1., 1.]) # set axis size so we don't have a border\n",
    "            ax.set_axis_off()\n",
    "            fig.add_axes(ax)\n",
    "            ax.imshow(S, cmap='jet')  # plot in kHz, log of spectrogram, decide on colouring\n",
    "\n",
    "\n",
    "            # specify name of plot\n",
    "            name=date+'_'+person+'_'+timing\n",
    "\n",
    "            # save figure without border to folder for respective emotion, named after the original wav File\n",
    "            save_results_to = '/Users/htr365/no_icloud/quantified_self_all/data/spectrograms/AB_sit_relaxed/'\n",
    "            plt.savefig(save_results_to + name +'_'+ str(k) + '.png', dpi = sizes[0])\n",
    "            plt.close()\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'cis_subjective_fatigue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-27_Amanda_AM_5.png\n",
      "2024-06-25_Amanda_AM_3.png\n",
      "2024-07-16_Amanda_PM_16.png\n",
      "2024-07-20_Amanda_AM_1.png\n",
      "2024-07-19_Amanda_PM_11.png\n",
      "2024-07-27_Amanda_PM_11.png\n",
      "2024-08-13_Amanda_AM_20.png\n",
      "2024-07-09_Amanda_PM_17.png\n",
      "2024-07-09_Amanda_PM_7.png\n",
      "2024-08-03_Amanda_PM_12.png\n",
      "2024-07-17_Amanda_PM_2.png\n",
      "2024-07-09_Amanda_AM_23.png\n",
      "2024-08-13_Amanda_PM_14.png\n",
      "2024-07-27_Amanda_AM_25.png\n",
      "2024-07-16_Amanda_AM_22.png\n",
      "2024-07-10_Amanda_PM_6.png\n",
      "2024-07-19_Amanda_AM_19.png\n",
      "2024-08-13_Amanda_PM_28.png\n",
      "2024-07-27_Amanda_AM_19.png\n",
      "2024-07-24_Amanda_AM_13.png\n",
      "2024-08-10_Amanda_AM_1.png\n",
      "2024-07-15_Amanda_AM_28.png\n",
      "2024-07-27_Amanda_PM_8.png\n",
      "2024-07-15_Amanda_AM_14.png\n",
      "2024-08-04_Amanda_PM_7.png\n",
      "2024-08-03_Amanda_PM_3.png\n",
      "2024-07-24_Amanda_PM_27.png\n",
      "2024-07-22_Amanda_PM_4.png\n",
      "2024-07-12_Amanda_PM_12.png\n",
      "2024-08-08_Amanda_AM_25.png\n",
      "2024-08-12_Amanda_AM_9.png\n",
      "2024-08-08_Amanda_AM_19.png\n",
      "2024-07-28_Amanda_AM_7.png\n",
      "2024-07-23_Amanda_PM_15.png\n",
      "2024-07-23_Amanda_AM_21.png\n",
      "2024-07-12_Amanda_AM_3.png\n",
      "2024-07-15_Amanda_AM_7.png\n",
      "2024-07-31_Amanda_AM_6.png\n",
      "2024-08-08_Amanda_PM_11.png\n",
      "2024-07-12_Amanda_AM_26.png\n",
      "2024-07-30_Amanda_AM_11.png\n",
      "2024-08-04_Amanda_PM_20.png\n",
      "2024-07-20_Amanda_AM_17.png\n",
      "2024-08-14_Amanda_PM_26.png\n",
      "2024-07-22_Amanda_AM_9.png\n",
      "2024-07-11_Amanda_AM_10.png\n",
      "2024-08-12_Amanda_PM_4.png\n",
      "2024-07-30_Amanda_PM_19.png\n",
      "2024-07-11_Amanda_PM_24.png\n",
      "2024-08-04_Amanda_AM_14.png\n",
      "2024-07-30_Amanda_PM_25.png\n",
      "2024-08-14_Amanda_AM_12.png\n",
      "2024-07-11_Amanda_PM_18.png\n",
      "2024-07-11_Amanda_PM_19.png\n",
      "2024-08-04_Amanda_AM_15.png\n",
      "2024-07-30_Amanda_PM_24.png\n",
      "2024-08-14_Amanda_AM_13.png\n",
      "2024-07-11_Amanda_PM_25.png\n",
      "2024-07-30_Amanda_PM_18.png\n",
      "2024-07-22_Amanda_AM_8.png\n",
      "2024-08-12_Amanda_PM_5.png\n",
      "2024-07-11_Amanda_AM_11.png\n",
      "2024-07-30_Amanda_AM_10.png\n",
      "2024-08-04_Amanda_PM_21.png\n",
      "2024-07-20_Amanda_AM_16.png\n",
      "2024-08-14_Amanda_PM_27.png\n",
      "2024-07-31_Amanda_AM_7.png\n",
      "2024-07-15_Amanda_AM_6.png\n",
      "2024-07-12_Amanda_AM_27.png\n",
      "2024-08-08_Amanda_PM_10.png\n",
      "2024-07-18_Amanda_PM_1.png\n",
      "2024-07-12_Amanda_AM_2.png\n",
      "2024-07-23_Amanda_AM_20.png\n",
      "2024-07-28_Amanda_AM_6.png\n",
      "2024-07-23_Amanda_PM_14.png\n",
      "2024-08-08_Amanda_AM_18.png\n",
      "2024-07-23_Amanda_PM_28.png\n",
      "2024-07-22_Amanda_PM_5.png\n",
      "2024-08-12_Amanda_AM_8.png\n",
      "2024-08-08_Amanda_AM_24.png\n",
      "2024-07-12_Amanda_PM_13.png\n",
      "2024-07-24_Amanda_PM_26.png\n",
      "2024-08-03_Amanda_PM_2.png\n",
      "2024-08-04_Amanda_PM_6.png\n",
      "2024-08-09_Amanda_AM_1.png\n",
      "2024-07-15_Amanda_AM_15.png\n",
      "2024-07-27_Amanda_PM_9.png\n",
      "2024-07-24_Amanda_AM_12.png\n",
      "2024-07-19_Amanda_AM_18.png\n",
      "2024-07-10_Amanda_PM_7.png\n",
      "2024-07-27_Amanda_AM_18.png\n",
      "2024-07-16_Amanda_AM_23.png\n",
      "2024-07-17_Amanda_PM_3.png\n",
      "2024-08-13_Amanda_PM_15.png\n",
      "2024-07-27_Amanda_AM_24.png\n",
      "2024-07-09_Amanda_AM_22.png\n",
      "2024-07-09_Amanda_PM_6.png\n",
      "2024-07-19_Amanda_PM_10.png\n",
      "2024-07-09_Amanda_PM_16.png\n",
      "2024-07-27_Amanda_PM_10.png\n",
      "2024-06-25_Amanda_AM_2.png\n",
      "2024-07-16_Amanda_PM_17.png\n",
      "2024-07-27_Amanda_AM_4.png\n",
      "2024-07-16_Amanda_PM_15.png\n",
      "2024-07-27_Amanda_AM_6.png\n",
      "2024-07-09_Amanda_PM_4.png\n",
      "2024-07-09_Amanda_PM_14.png\n",
      "2024-07-27_Amanda_PM_12.png\n",
      "2024-07-20_Amanda_AM_2.png\n",
      "2024-07-19_Amanda_PM_12.png\n",
      "2024-07-27_Amanda_AM_26.png\n",
      "2024-08-13_Amanda_PM_17.png\n",
      "2024-07-09_Amanda_AM_20.png\n",
      "2024-08-03_Amanda_PM_11.png\n",
      "2024-07-17_Amanda_PM_1.png\n",
      "2024-08-04_Amanda_AM_9.png\n",
      "2024-07-10_Amanda_PM_5.png\n",
      "2024-07-16_Amanda_AM_21.png\n",
      "2024-07-09_Amanda_AM_9.png\n",
      "2024-07-24_Amanda_AM_10.png\n",
      "2024-08-10_Amanda_AM_2.png\n",
      "2024-07-15_Amanda_AM_17.png\n",
      "2024-07-24_Amanda_PM_18.png\n",
      "2024-08-04_Amanda_PM_4.png\n",
      "2024-07-10_Amanda_AM_8.png\n",
      "2024-08-09_Amanda_AM_3.png\n",
      "2024-07-24_Amanda_PM_24.png\n",
      "2024-08-08_Amanda_AM_26.png\n",
      "2024-07-12_Amanda_PM_11.png\n",
      "2024-07-22_Amanda_PM_7.png\n",
      "2024-07-23_Amanda_PM_16.png\n",
      "2024-07-28_Amanda_AM_4.png\n",
      "2024-07-12_Amanda_AM_19.png\n",
      "2024-07-23_Amanda_AM_22.png\n",
      "2024-07-12_Amanda_AM_25.png\n",
      "2024-08-08_Amanda_PM_12.png\n",
      "2024-07-15_Amanda_AM_4.png\n",
      "2024-07-31_Amanda_AM_5.png\n",
      "2024-07-18_Amanda_PM_3.png\n",
      "2024-08-14_Amanda_PM_25.png\n",
      "2024-07-20_Amanda_AM_14.png\n",
      "2024-08-04_Amanda_PM_23.png\n",
      "2024-07-30_Amanda_AM_12.png\n",
      "2024-08-14_Amanda_PM_19.png\n",
      "2024-07-11_Amanda_AM_13.png\n",
      "2024-08-12_Amanda_PM_7.png\n",
      "2024-07-11_Amanda_PM_27.png\n",
      "2024-07-15_Amanda_PM_9.png\n",
      "2024-07-20_Amanda_PM_20.png\n",
      "2024-08-14_Amanda_AM_11.png\n",
      "2024-07-30_Amanda_PM_26.png\n",
      "2024-08-04_Amanda_AM_17.png\n",
      "2024-08-14_Amanda_AM_10.png\n",
      "2024-07-30_Amanda_PM_27.png\n",
      "2024-08-04_Amanda_AM_16.png\n",
      "2024-07-11_Amanda_PM_26.png\n",
      "2024-07-15_Amanda_PM_8.png\n",
      "2024-08-12_Amanda_PM_6.png\n",
      "2024-07-11_Amanda_AM_12.png\n",
      "2024-08-14_Amanda_PM_18.png\n",
      "2024-08-14_Amanda_PM_24.png\n",
      "2024-07-20_Amanda_AM_15.png\n",
      "2024-08-04_Amanda_PM_22.png\n",
      "2024-07-30_Amanda_AM_13.png\n",
      "2024-07-18_Amanda_PM_2.png\n",
      "2024-08-08_Amanda_PM_13.png\n",
      "2024-07-12_Amanda_AM_24.png\n",
      "2024-07-31_Amanda_AM_4.png\n",
      "2024-07-15_Amanda_AM_5.png\n",
      "2024-07-23_Amanda_AM_23.png\n",
      "2024-07-12_Amanda_AM_18.png\n",
      "2024-07-12_Amanda_AM_1.png\n",
      "2024-07-23_Amanda_PM_17.png\n",
      "2024-07-28_Amanda_AM_5.png\n",
      "2024-07-12_Amanda_PM_10.png\n",
      "2024-08-08_Amanda_AM_27.png\n",
      "2024-07-22_Amanda_PM_6.png\n",
      "2024-07-24_Amanda_PM_25.png\n",
      "2024-08-03_Amanda_PM_1.png\n",
      "2024-08-09_Amanda_AM_2.png\n",
      "2024-08-04_Amanda_PM_5.png\n",
      "2024-07-24_Amanda_PM_19.png\n",
      "2024-07-10_Amanda_AM_9.png\n",
      "2024-07-15_Amanda_AM_16.png\n",
      "2024-08-10_Amanda_AM_3.png\n",
      "2024-07-24_Amanda_AM_11.png\n",
      "2024-07-09_Amanda_AM_8.png\n",
      "2024-07-16_Amanda_AM_20.png\n",
      "2024-08-04_Amanda_AM_8.png\n",
      "2024-07-10_Amanda_PM_4.png\n",
      "2024-07-09_Amanda_AM_21.png\n",
      "2024-07-27_Amanda_AM_27.png\n",
      "2024-08-13_Amanda_PM_16.png\n",
      "2024-08-03_Amanda_PM_10.png\n",
      "2024-07-27_Amanda_PM_13.png\n",
      "2024-07-09_Amanda_PM_15.png\n",
      "2024-07-19_Amanda_PM_13.png\n",
      "2024-07-20_Amanda_AM_3.png\n",
      "2024-07-09_Amanda_PM_5.png\n",
      "2024-07-27_Amanda_AM_7.png\n",
      "2024-07-16_Amanda_PM_14.png\n",
      "2024-06-25_Amanda_AM_1.png\n",
      "2024-07-09_Amanda_PM_1.png\n",
      "2024-07-20_Amanda_AM_7.png\n",
      "2024-07-19_Amanda_PM_17.png\n",
      "2024-07-09_Amanda_PM_11.png\n",
      "2024-07-27_Amanda_PM_17.png\n",
      "2024-06-25_Amanda_AM_5.png\n",
      "2024-07-16_Amanda_PM_10.png\n",
      "2024-07-27_Amanda_AM_3.png\n",
      "2024-07-09_Amanda_AM_19.png\n",
      "2024-07-16_Amanda_AM_24.png\n",
      "2024-07-19_Amanda_AM_23.png\n",
      "2024-07-17_Amanda_PM_4.png\n",
      "2024-08-13_Amanda_PM_12.png\n",
      "2024-07-27_Amanda_AM_23.png\n",
      "2024-07-09_Amanda_AM_25.png\n",
      "2024-07-16_Amanda_AM_18.png\n",
      "2024-06-25_Amanda_PM_8.png\n",
      "2024-07-15_Amanda_AM_12.png\n",
      "2024-07-24_Amanda_AM_15.png\n",
      "2024-08-10_Amanda_AM_7.png\n",
      "2024-07-17_Amanda_AM_9.png\n",
      "2024-08-10_Amanda_AM_10.png\n",
      "2024-07-24_Amanda_PM_21.png\n",
      "2024-08-03_Amanda_PM_5.png\n",
      "2024-08-04_Amanda_PM_1.png\n",
      "2024-08-09_Amanda_AM_6.png\n",
      "2024-07-28_Amanda_AM_1.png\n",
      "2024-07-23_Amanda_PM_13.png\n",
      "2024-07-22_Amanda_PM_2.png\n",
      "2024-08-08_Amanda_AM_23.png\n",
      "2024-07-12_Amanda_PM_14.png\n",
      "2024-07-15_Amanda_AM_1.png\n",
      "2024-07-12_Amanda_AM_20.png\n",
      "2024-08-08_Amanda_PM_17.png\n",
      "2024-07-18_Amanda_PM_6.png\n",
      "2024-07-12_Amanda_AM_5.png\n",
      "2024-08-07_Amanda_PM_10.png\n",
      "2024-07-23_Amanda_AM_27.png\n",
      "2024-07-11_Amanda_AM_16.png\n",
      "2024-08-12_Amanda_PM_2.png\n",
      "2024-07-30_Amanda_AM_17.png\n",
      "2024-08-04_Amanda_PM_26.png\n",
      "2024-07-20_Amanda_AM_11.png\n",
      "2024-08-14_Amanda_PM_20.png\n",
      "2024-07-12_Amanda_PM_8.png\n",
      "2024-08-04_Amanda_AM_12.png\n",
      "2024-07-30_Amanda_PM_23.png\n",
      "2024-08-14_Amanda_AM_14.png\n",
      "2024-07-11_Amanda_PM_22.png\n",
      "2024-08-14_Amanda_AM_28.png\n",
      "2024-07-20_Amanda_PM_19.png\n",
      "2024-07-20_Amanda_PM_18.png\n",
      "2024-07-11_Amanda_PM_23.png\n",
      "2024-08-04_Amanda_AM_13.png\n",
      "2024-07-30_Amanda_PM_22.png\n",
      "2024-08-14_Amanda_AM_15.png\n",
      "2024-07-12_Amanda_PM_9.png\n",
      "2024-07-30_Amanda_AM_16.png\n",
      "2024-07-20_Amanda_AM_10.png\n",
      "2024-08-14_Amanda_PM_21.png\n",
      "2024-08-12_Amanda_PM_3.png\n",
      "2024-07-11_Amanda_AM_17.png\n",
      "2024-08-07_Amanda_PM_11.png\n",
      "2024-07-23_Amanda_AM_26.png\n",
      "2024-07-12_Amanda_AM_4.png\n",
      "2024-07-18_Amanda_PM_7.png\n",
      "2024-07-31_Amanda_AM_1.png\n",
      "2024-08-08_Amanda_PM_16.png\n",
      "2024-07-12_Amanda_AM_21.png\n",
      "2024-07-22_Amanda_PM_3.png\n",
      "2024-07-12_Amanda_PM_15.png\n",
      "2024-08-08_Amanda_AM_22.png\n",
      "2024-07-23_Amanda_PM_12.png\n",
      "2024-08-09_Amanda_AM_7.png\n",
      "2024-07-17_Amanda_AM_8.png\n",
      "2024-08-03_Amanda_PM_4.png\n",
      "2024-08-10_Amanda_AM_11.png\n",
      "2024-07-24_Amanda_PM_20.png\n",
      "2024-08-10_Amanda_AM_6.png\n",
      "2024-07-24_Amanda_AM_14.png\n",
      "2024-06-25_Amanda_PM_9.png\n",
      "2024-07-15_Amanda_AM_13.png\n",
      "2024-07-16_Amanda_AM_19.png\n",
      "2024-07-17_Amanda_PM_5.png\n",
      "2024-07-19_Amanda_AM_22.png\n",
      "2024-07-09_Amanda_AM_24.png\n",
      "2024-08-13_Amanda_PM_13.png\n",
      "2024-07-27_Amanda_AM_22.png\n",
      "2024-07-16_Amanda_AM_25.png\n",
      "2024-07-10_Amanda_PM_1.png\n",
      "2024-07-09_Amanda_AM_18.png\n",
      "2024-07-27_Amanda_AM_2.png\n",
      "2024-06-25_Amanda_AM_4.png\n",
      "2024-07-16_Amanda_PM_11.png\n",
      "2024-07-19_Amanda_PM_16.png\n",
      "2024-07-20_Amanda_AM_6.png\n",
      "2024-07-27_Amanda_PM_16.png\n",
      "2024-07-09_Amanda_PM_10.png\n",
      "2024-07-27_Amanda_PM_14.png\n",
      "2024-07-09_Amanda_PM_12.png\n",
      "2024-07-20_Amanda_AM_4.png\n",
      "2024-07-19_Amanda_PM_14.png\n",
      "2024-07-09_Amanda_PM_2.png\n",
      "2024-08-13_Amanda_AM_19.png\n",
      "2024-07-16_Amanda_PM_13.png\n",
      "2024-06-25_Amanda_AM_6.png\n",
      "2024-07-16_Amanda_AM_27.png\n",
      "2024-08-09_Amanda_PM_8.png\n",
      "2024-07-10_Amanda_PM_3.png\n",
      "2024-07-09_Amanda_AM_26.png\n",
      "2024-07-27_Amanda_AM_20.png\n",
      "2024-08-13_Amanda_PM_11.png\n",
      "2024-07-19_Amanda_AM_20.png\n",
      "2024-07-17_Amanda_PM_7.png\n",
      "2024-07-15_Amanda_AM_11.png\n",
      "2024-07-24_Amanda_AM_16.png\n",
      "2024-08-10_Amanda_AM_4.png\n",
      "2024-07-20_Amanda_PM_9.png\n",
      "2024-07-24_Amanda_PM_22.png\n",
      "2024-08-10_Amanda_AM_13.png\n",
      "2024-08-03_Amanda_PM_6.png\n",
      "2024-08-09_Amanda_AM_5.png\n",
      "2024-08-04_Amanda_PM_2.png\n",
      "2024-07-23_Amanda_PM_10.png\n",
      "2024-07-28_Amanda_AM_2.png\n",
      "2024-07-12_Amanda_PM_17.png\n",
      "2024-08-08_Amanda_AM_20.png\n",
      "2024-07-22_Amanda_PM_1.png\n",
      "2024-07-23_Amanda_AM_18.png\n",
      "2024-07-18_Amanda_PM_5.png\n",
      "2024-08-08_Amanda_PM_14.png\n",
      "2024-07-12_Amanda_AM_23.png\n",
      "2024-07-15_Amanda_AM_2.png\n",
      "2024-07-31_Amanda_AM_3.png\n",
      "2024-07-23_Amanda_AM_24.png\n",
      "2024-08-07_Amanda_PM_13.png\n",
      "2024-07-12_Amanda_AM_6.png\n",
      "2024-07-11_Amanda_AM_15.png\n",
      "2024-08-12_Amanda_PM_1.png\n",
      "2024-08-04_Amanda_PM_19.png\n",
      "2024-08-14_Amanda_PM_23.png\n",
      "2024-07-20_Amanda_AM_12.png\n",
      "2024-08-04_Amanda_PM_25.png\n",
      "2024-07-30_Amanda_AM_14.png\n",
      "2024-08-14_Amanda_AM_17.png\n",
      "2024-07-30_Amanda_PM_20.png\n",
      "2024-08-04_Amanda_AM_11.png\n",
      "2024-07-18_Amanda_AM_8.png\n",
      "2024-07-11_Amanda_PM_21.png\n",
      "2024-07-11_Amanda_PM_20.png\n",
      "2024-07-18_Amanda_AM_9.png\n",
      "2024-08-14_Amanda_AM_16.png\n",
      "2024-07-30_Amanda_PM_21.png\n",
      "2024-08-04_Amanda_AM_10.png\n",
      "2024-08-14_Amanda_PM_22.png\n",
      "2024-07-20_Amanda_AM_13.png\n",
      "2024-08-04_Amanda_PM_24.png\n",
      "2024-07-30_Amanda_AM_15.png\n",
      "2024-08-04_Amanda_PM_18.png\n",
      "2024-07-11_Amanda_AM_14.png\n",
      "2024-07-12_Amanda_AM_7.png\n",
      "2024-07-23_Amanda_AM_25.png\n",
      "2024-08-07_Amanda_PM_12.png\n",
      "2024-07-12_Amanda_AM_22.png\n",
      "2024-08-08_Amanda_PM_15.png\n",
      "2024-07-31_Amanda_AM_2.png\n",
      "2024-07-15_Amanda_AM_3.png\n",
      "2024-07-23_Amanda_AM_19.png\n",
      "2024-07-18_Amanda_PM_4.png\n",
      "2024-08-08_Amanda_AM_21.png\n",
      "2024-07-12_Amanda_PM_16.png\n",
      "2024-07-23_Amanda_PM_11.png\n",
      "2024-07-28_Amanda_AM_3.png\n",
      "2024-08-04_Amanda_PM_3.png\n",
      "2024-08-09_Amanda_AM_4.png\n",
      "2024-08-03_Amanda_PM_7.png\n",
      "2024-07-24_Amanda_PM_23.png\n",
      "2024-08-10_Amanda_AM_12.png\n",
      "2024-08-10_Amanda_AM_5.png\n",
      "2024-07-24_Amanda_AM_17.png\n",
      "2024-07-20_Amanda_PM_8.png\n",
      "2024-07-15_Amanda_AM_10.png\n",
      "2024-07-27_Amanda_AM_21.png\n",
      "2024-08-13_Amanda_PM_10.png\n",
      "2024-07-09_Amanda_AM_27.png\n",
      "2024-07-17_Amanda_PM_6.png\n",
      "2024-07-19_Amanda_AM_21.png\n",
      "2024-07-10_Amanda_PM_2.png\n",
      "2024-08-09_Amanda_PM_9.png\n",
      "2024-07-16_Amanda_AM_26.png\n",
      "2024-07-16_Amanda_PM_12.png\n",
      "2024-06-25_Amanda_AM_7.png\n",
      "2024-08-13_Amanda_AM_18.png\n",
      "2024-07-27_Amanda_AM_1.png\n",
      "2024-07-09_Amanda_PM_3.png\n",
      "2024-07-09_Amanda_PM_13.png\n",
      "2024-07-27_Amanda_PM_15.png\n",
      "2024-07-19_Amanda_PM_15.png\n",
      "2024-07-20_Amanda_AM_5.png\n",
      "2024-07-23_Amanda_PM_2.png\n",
      "2024-07-18_Amanda_AM_24.png\n",
      "2024-07-08_Amanda_AM_22.png\n",
      "2024-08-12_Amanda_PM_15.png\n",
      "2024-07-29_Amanda_AM_1.png\n",
      "2024-07-17_Amanda_AM_23.png\n",
      "2024-07-18_Amanda_AM_18.png\n",
      "2024-07-24_Amanda_PM_6.png\n",
      "2024-07-26_Amanda_AM_18.png\n",
      "2024-07-13_Amanda_AM_5.png\n",
      "2024-08-07_Amanda_PM_9.png\n",
      "2024-07-29_Amanda_PM_17.png\n",
      "2024-07-17_Amanda_PM_17.png\n",
      "2024-07-18_Amanda_PM_10.png\n",
      "2024-07-14_Amanda_AM_1.png\n",
      "2024-07-26_Amanda_PM_10.png\n",
      "2024-08-12_Amanda_AM_21.png\n",
      "2024-07-08_Amanda_PM_16.png\n",
      "2024-07-19_Amanda_PM_6.png\n",
      "2024-07-14_Amanda_PM_21.png\n",
      "2024-08-14_Amanda_PM_6.png\n",
      "2024-08-13_Amanda_PM_2.png\n",
      "2024-07-13_Amanda_PM_8.png\n",
      "2024-07-14_Amanda_AM_15.png\n",
      "2024-07-22_Amanda_AM_20.png\n",
      "2024-07-26_Amanda_AM_3.png\n",
      "2024-07-08_Amanda_PM_1.png\n",
      "2024-07-21_Amanda_AM_7.png\n",
      "2024-08-09_Amanda_PM_10.png\n",
      "2024-07-13_Amanda_AM_27.png\n",
      "2024-07-16_Amanda_PM_4.png\n",
      "2024-08-02_Amanda_AM_8.png\n",
      "2024-07-13_Amanda_PM_13.png\n",
      "2024-08-09_Amanda_AM_24.png\n",
      "2024-08-09_Amanda_AM_18.png\n",
      "2024-07-22_Amanda_PM_14.png\n",
      "2024-06-25_Amanda_PM_27.png\n",
      "2024-07-10_Amanda_PM_25.png\n",
      "2024-07-21_Amanda_PM_22.png\n",
      "2024-07-10_Amanda_PM_19.png\n",
      "2024-08-05_Amanda_PM_1.png\n",
      "2024-07-31_Amanda_AM_10.png\n",
      "2024-07-21_Amanda_AM_16.png\n",
      "2024-08-08_Amanda_AM_6.png\n",
      "2024-06-25_Amanda_AM_13.png\n",
      "2024-07-16_Amanda_AM_9.png\n",
      "2024-07-10_Amanda_AM_11.png\n",
      "2024-07-16_Amanda_AM_8.png\n",
      "2024-06-25_Amanda_AM_12.png\n",
      "2024-07-10_Amanda_AM_10.png\n",
      "2024-08-08_Amanda_AM_7.png\n",
      "2024-07-21_Amanda_AM_17.png\n",
      "2024-07-10_Amanda_PM_18.png\n",
      "2024-07-21_Amanda_PM_23.png\n",
      "2024-06-25_Amanda_PM_26.png\n",
      "2024-07-10_Amanda_PM_24.png\n",
      "2024-07-22_Amanda_PM_15.png\n",
      "2024-07-11_Amanda_PM_1.png\n",
      "2024-08-09_Amanda_AM_19.png\n",
      "2024-07-16_Amanda_PM_5.png\n",
      "2024-08-09_Amanda_AM_25.png\n",
      "2024-07-13_Amanda_PM_12.png\n",
      "2024-08-02_Amanda_AM_9.png\n",
      "2024-07-21_Amanda_AM_6.png\n",
      "2024-07-13_Amanda_AM_26.png\n",
      "2024-08-09_Amanda_PM_11.png\n",
      "2024-07-26_Amanda_AM_2.png\n",
      "2024-07-22_Amanda_AM_21.png\n",
      "2024-07-14_Amanda_AM_14.png\n",
      "2024-07-13_Amanda_PM_9.png\n",
      "2024-08-13_Amanda_PM_3.png\n",
      "2024-08-14_Amanda_PM_7.png\n",
      "2024-07-14_Amanda_PM_20.png\n",
      "2024-07-19_Amanda_PM_7.png\n",
      "2024-07-30_Amanda_AM_1.png\n",
      "2024-07-18_Amanda_PM_11.png\n",
      "2024-07-08_Amanda_PM_17.png\n",
      "2024-07-26_Amanda_PM_11.png\n",
      "2024-08-12_Amanda_AM_20.png\n",
      "2024-07-29_Amanda_PM_16.png\n",
      "2024-07-17_Amanda_PM_16.png\n",
      "2024-07-13_Amanda_AM_4.png\n",
      "2024-08-07_Amanda_PM_8.png\n",
      "2024-07-24_Amanda_PM_7.png\n",
      "2024-07-18_Amanda_AM_19.png\n",
      "2024-08-12_Amanda_PM_28.png\n",
      "2024-07-26_Amanda_AM_19.png\n",
      "2024-07-17_Amanda_AM_22.png\n",
      "2024-07-18_Amanda_AM_25.png\n",
      "2024-07-23_Amanda_PM_3.png\n",
      "2024-08-12_Amanda_PM_14.png\n",
      "2024-07-08_Amanda_AM_23.png\n",
      "2024-08-12_Amanda_PM_16.png\n",
      "2024-07-08_Amanda_AM_21.png\n",
      "2024-07-23_Amanda_PM_1.png\n",
      "2024-07-18_Amanda_AM_27.png\n",
      "2024-08-14_Amanda_AM_8.png\n",
      "2024-07-24_Amanda_PM_5.png\n",
      "2024-07-17_Amanda_AM_20.png\n",
      "2024-07-29_Amanda_AM_2.png\n",
      "2024-07-17_Amanda_PM_14.png\n",
      "2024-07-29_Amanda_PM_14.png\n",
      "2024-07-13_Amanda_AM_6.png\n",
      "2024-07-19_Amanda_PM_5.png\n",
      "2024-07-08_Amanda_PM_15.png\n",
      "2024-08-12_Amanda_AM_22.png\n",
      "2024-07-26_Amanda_PM_13.png\n",
      "2024-07-18_Amanda_PM_13.png\n",
      "2024-07-14_Amanda_AM_2.png\n",
      "2024-07-30_Amanda_AM_3.png\n",
      "2024-08-14_Amanda_PM_5.png\n",
      "2024-07-24_Amanda_AM_8.png\n",
      "2024-07-14_Amanda_PM_22.png\n",
      "2024-08-13_Amanda_PM_1.png\n",
      "2024-07-19_Amanda_AM_8.png\n",
      "2024-07-14_Amanda_AM_16.png\n",
      "2024-07-13_Amanda_AM_18.png\n",
      "2024-07-22_Amanda_AM_23.png\n",
      "2024-07-13_Amanda_AM_24.png\n",
      "2024-08-09_Amanda_PM_13.png\n",
      "2024-07-21_Amanda_AM_4.png\n",
      "2024-07-08_Amanda_PM_2.png\n",
      "2024-08-09_Amanda_AM_27.png\n",
      "2024-07-13_Amanda_PM_10.png\n",
      "2024-07-16_Amanda_PM_7.png\n",
      "2024-08-08_Amanda_PM_8.png\n",
      "2024-07-22_Amanda_PM_17.png\n",
      "2024-07-11_Amanda_PM_3.png\n",
      "2024-07-10_Amanda_PM_26.png\n",
      "2024-07-21_Amanda_PM_9.png\n",
      "2024-06-25_Amanda_PM_24.png\n",
      "2024-06-25_Amanda_PM_18.png\n",
      "2024-07-21_Amanda_PM_21.png\n",
      "2024-07-21_Amanda_AM_15.png\n",
      "2024-08-08_Amanda_AM_5.png\n",
      "2024-08-05_Amanda_PM_2.png\n",
      "2024-07-10_Amanda_AM_12.png\n",
      "2024-06-25_Amanda_AM_10.png\n",
      "2024-07-10_Amanda_AM_13.png\n",
      "2024-06-25_Amanda_AM_11.png\n",
      "2024-08-05_Amanda_PM_3.png\n",
      "2024-08-08_Amanda_AM_4.png\n",
      "2024-07-21_Amanda_AM_14.png\n",
      "2024-07-21_Amanda_PM_20.png\n",
      "2024-06-25_Amanda_PM_19.png\n",
      "2024-07-10_Amanda_PM_27.png\n",
      "2024-06-25_Amanda_PM_25.png\n",
      "2024-07-21_Amanda_PM_8.png\n",
      "2024-07-11_Amanda_PM_2.png\n",
      "2024-07-22_Amanda_PM_16.png\n",
      "2024-08-08_Amanda_PM_9.png\n",
      "2024-07-13_Amanda_PM_11.png\n",
      "2024-08-09_Amanda_AM_26.png\n",
      "2024-07-16_Amanda_PM_6.png\n",
      "2024-07-08_Amanda_PM_3.png\n",
      "2024-08-09_Amanda_PM_12.png\n",
      "2024-07-13_Amanda_AM_25.png\n",
      "2024-07-21_Amanda_AM_5.png\n",
      "2024-07-22_Amanda_AM_22.png\n",
      "2024-07-13_Amanda_AM_19.png\n",
      "2024-07-26_Amanda_AM_1.png\n",
      "2024-07-14_Amanda_AM_17.png\n",
      "2024-07-19_Amanda_AM_9.png\n",
      "2024-07-14_Amanda_PM_23.png\n",
      "2024-08-14_Amanda_PM_4.png\n",
      "2024-07-24_Amanda_AM_9.png\n",
      "2024-08-12_Amanda_AM_23.png\n",
      "2024-07-26_Amanda_PM_12.png\n",
      "2024-07-08_Amanda_PM_14.png\n",
      "2024-07-30_Amanda_AM_2.png\n",
      "2024-07-14_Amanda_AM_3.png\n",
      "2024-07-18_Amanda_PM_12.png\n",
      "2024-07-19_Amanda_PM_4.png\n",
      "2024-07-08_Amanda_PM_28.png\n",
      "2024-07-13_Amanda_AM_7.png\n",
      "2024-07-17_Amanda_PM_15.png\n",
      "2024-07-29_Amanda_PM_15.png\n",
      "2024-07-17_Amanda_AM_21.png\n",
      "2024-07-29_Amanda_AM_3.png\n",
      "2024-08-14_Amanda_AM_9.png\n",
      "2024-07-24_Amanda_PM_4.png\n",
      "2024-07-08_Amanda_AM_20.png\n",
      "2024-08-12_Amanda_PM_17.png\n",
      "2024-07-18_Amanda_AM_26.png\n",
      "2024-07-08_Amanda_AM_18.png\n",
      "2024-07-29_Amanda_AM_7.png\n",
      "2024-07-17_Amanda_AM_25.png\n",
      "2024-07-23_Amanda_PM_4.png\n",
      "2024-07-18_Amanda_AM_22.png\n",
      "2024-08-12_Amanda_PM_13.png\n",
      "2024-07-08_Amanda_AM_24.png\n",
      "2024-08-13_Amanda_AM_9.png\n",
      "2024-07-17_Amanda_AM_19.png\n",
      "2024-07-18_Amanda_PM_16.png\n",
      "2024-07-14_Amanda_AM_7.png\n",
      "2024-07-30_Amanda_AM_6.png\n",
      "2024-07-08_Amanda_PM_10.png\n",
      "2024-07-26_Amanda_PM_16.png\n",
      "2024-08-12_Amanda_AM_27.png\n",
      "2024-07-29_Amanda_PM_11.png\n",
      "2024-07-17_Amanda_PM_11.png\n",
      "2024-07-13_Amanda_AM_3.png\n",
      "2024-07-23_Amanda_AM_9.png\n",
      "2024-08-13_Amanda_PM_4.png\n",
      "2024-07-14_Amanda_PM_27.png\n",
      "2024-07-14_Amanda_AM_13.png\n",
      "2024-07-21_Amanda_AM_1.png\n",
      "2024-07-13_Amanda_AM_21.png\n",
      "2024-08-09_Amanda_PM_16.png\n",
      "2024-07-08_Amanda_PM_7.png\n",
      "2024-07-26_Amanda_AM_5.png\n",
      "2024-07-22_Amanda_AM_26.png\n",
      "2024-07-22_Amanda_PM_12.png\n",
      "2024-07-11_Amanda_PM_6.png\n",
      "2024-07-16_Amanda_PM_2.png\n",
      "2024-08-09_Amanda_AM_22.png\n",
      "2024-07-13_Amanda_PM_15.png\n",
      "2024-07-26_Amanda_PM_8.png\n",
      "2024-07-21_Amanda_PM_24.png\n",
      "2024-06-25_Amanda_PM_21.png\n",
      "2024-07-10_Amanda_PM_23.png\n",
      "2024-07-21_Amanda_PM_18.png\n",
      "2024-06-25_Amanda_AM_15.png\n",
      "2024-07-10_Amanda_AM_17.png\n",
      "2024-07-21_Amanda_AM_10.png\n",
      "2024-08-05_Amanda_PM_7.png\n",
      "2024-08-05_Amanda_PM_6.png\n",
      "2024-08-08_Amanda_AM_1.png\n",
      "2024-07-21_Amanda_AM_11.png\n",
      "2024-06-25_Amanda_AM_14.png\n",
      "2024-07-10_Amanda_AM_16.png\n",
      "2024-07-21_Amanda_PM_19.png\n",
      "2024-06-25_Amanda_PM_20.png\n",
      "2024-07-10_Amanda_PM_22.png\n",
      "2024-07-21_Amanda_PM_25.png\n",
      "2024-07-26_Amanda_PM_9.png\n",
      "2024-07-16_Amanda_PM_3.png\n",
      "2024-07-13_Amanda_PM_14.png\n",
      "2024-08-09_Amanda_AM_23.png\n",
      "2024-07-11_Amanda_PM_7.png\n",
      "2024-07-22_Amanda_PM_13.png\n",
      "2024-07-22_Amanda_AM_27.png\n",
      "2024-07-26_Amanda_AM_4.png\n",
      "2024-07-08_Amanda_PM_6.png\n",
      "2024-08-09_Amanda_PM_17.png\n",
      "2024-07-13_Amanda_AM_20.png\n",
      "2024-07-14_Amanda_AM_12.png\n",
      "2024-07-14_Amanda_PM_26.png\n",
      "2024-08-14_Amanda_PM_1.png\n",
      "2024-07-23_Amanda_AM_8.png\n",
      "2024-08-13_Amanda_PM_5.png\n",
      "2024-07-13_Amanda_AM_2.png\n",
      "2024-07-29_Amanda_PM_10.png\n",
      "2024-07-17_Amanda_PM_10.png\n",
      "2024-07-30_Amanda_AM_7.png\n",
      "2024-07-14_Amanda_AM_6.png\n",
      "2024-07-18_Amanda_PM_17.png\n",
      "2024-07-26_Amanda_PM_17.png\n",
      "2024-08-12_Amanda_AM_26.png\n",
      "2024-07-08_Amanda_PM_11.png\n",
      "2024-07-19_Amanda_PM_1.png\n",
      "2024-07-29_Amanda_AM_18.png\n",
      "2024-07-17_Amanda_AM_18.png\n",
      "2024-07-18_Amanda_AM_23.png\n",
      "2024-07-23_Amanda_PM_5.png\n",
      "2024-08-13_Amanda_AM_8.png\n",
      "2024-07-08_Amanda_AM_25.png\n",
      "2024-08-12_Amanda_PM_12.png\n",
      "2024-07-29_Amanda_AM_6.png\n",
      "2024-07-17_Amanda_AM_24.png\n",
      "2024-07-24_Amanda_PM_1.png\n",
      "2024-07-08_Amanda_AM_19.png\n",
      "2024-07-17_Amanda_AM_26.png\n",
      "2024-07-29_Amanda_AM_4.png\n",
      "2024-07-24_Amanda_PM_3.png\n",
      "2024-07-08_Amanda_AM_27.png\n",
      "2024-07-26_Amanda_AM_21.png\n",
      "2024-08-12_Amanda_PM_10.png\n",
      "2024-07-23_Amanda_PM_7.png\n",
      "2024-07-18_Amanda_AM_21.png\n",
      "2024-08-12_Amanda_AM_24.png\n",
      "2024-07-26_Amanda_PM_15.png\n",
      "2024-07-08_Amanda_PM_13.png\n",
      "2024-07-18_Amanda_PM_15.png\n",
      "2024-07-14_Amanda_AM_4.png\n",
      "2024-07-30_Amanda_AM_5.png\n",
      "2024-07-19_Amanda_PM_3.png\n",
      "2024-08-12_Amanda_AM_18.png\n",
      "2024-07-17_Amanda_PM_12.png\n",
      "2024-07-29_Amanda_PM_12.png\n",
      "2024-07-14_Amanda_PM_18.png\n",
      "2024-08-13_Amanda_PM_7.png\n",
      "2024-07-14_Amanda_PM_24.png\n",
      "2024-07-29_Amanda_PM_9.png\n",
      "2024-08-14_Amanda_PM_3.png\n",
      "2024-07-14_Amanda_AM_10.png\n",
      "2024-07-14_Amanda_PM_9.png\n",
      "2024-07-30_Amanda_PM_8.png\n",
      "2024-07-22_Amanda_AM_19.png\n",
      "2024-07-08_Amanda_PM_4.png\n",
      "2024-08-09_Amanda_PM_15.png\n",
      "2024-07-13_Amanda_AM_22.png\n",
      "2024-07-21_Amanda_AM_2.png\n",
      "2024-07-22_Amanda_AM_25.png\n",
      "2024-07-26_Amanda_AM_6.png\n",
      "2024-07-11_Amanda_PM_5.png\n",
      "2024-07-22_Amanda_PM_11.png\n",
      "2024-07-13_Amanda_PM_16.png\n",
      "2024-08-09_Amanda_AM_21.png\n",
      "2024-07-16_Amanda_PM_1.png\n",
      "2024-07-21_Amanda_PM_27.png\n",
      "2024-07-08_Amanda_AM_9.png\n",
      "2024-07-10_Amanda_PM_20.png\n",
      "2024-06-25_Amanda_PM_22.png\n",
      "2024-07-10_Amanda_AM_14.png\n",
      "2024-06-25_Amanda_AM_16.png\n",
      "2024-08-05_Amanda_PM_4.png\n",
      "2024-07-11_Amanda_AM_8.png\n",
      "2024-07-21_Amanda_AM_13.png\n",
      "2024-08-08_Amanda_AM_3.png\n",
      "2024-08-08_Amanda_AM_2.png\n",
      "2024-07-21_Amanda_AM_12.png\n",
      "2024-08-05_Amanda_PM_5.png\n",
      "2024-07-11_Amanda_AM_9.png\n",
      "2024-07-10_Amanda_AM_15.png\n",
      "2024-06-25_Amanda_AM_17.png\n",
      "2024-07-10_Amanda_PM_21.png\n",
      "2024-06-25_Amanda_PM_23.png\n",
      "2024-07-08_Amanda_AM_8.png\n",
      "2024-07-21_Amanda_PM_26.png\n",
      "2024-08-09_Amanda_AM_20.png\n",
      "2024-07-13_Amanda_PM_17.png\n",
      "2024-07-22_Amanda_PM_10.png\n",
      "2024-07-11_Amanda_PM_4.png\n",
      "2024-08-09_Amanda_PM_28.png\n",
      "2024-07-26_Amanda_AM_7.png\n",
      "2024-07-22_Amanda_AM_24.png\n",
      "2024-07-13_Amanda_AM_23.png\n",
      "2024-08-09_Amanda_PM_14.png\n",
      "2024-07-21_Amanda_AM_3.png\n",
      "2024-07-22_Amanda_AM_18.png\n",
      "2024-07-08_Amanda_PM_5.png\n",
      "2024-07-30_Amanda_PM_9.png\n",
      "2024-07-14_Amanda_PM_8.png\n",
      "2024-07-14_Amanda_AM_11.png\n",
      "2024-08-14_Amanda_PM_2.png\n",
      "2024-07-14_Amanda_PM_25.png\n",
      "2024-07-29_Amanda_PM_8.png\n",
      "2024-08-13_Amanda_PM_6.png\n",
      "2024-07-14_Amanda_PM_19.png\n",
      "2024-07-17_Amanda_PM_13.png\n",
      "2024-07-29_Amanda_PM_13.png\n",
      "2024-08-12_Amanda_AM_19.png\n",
      "2024-07-13_Amanda_AM_1.png\n",
      "2024-07-19_Amanda_PM_2.png\n",
      "2024-07-08_Amanda_PM_12.png\n",
      "2024-08-12_Amanda_AM_25.png\n",
      "2024-07-26_Amanda_PM_14.png\n",
      "2024-07-30_Amanda_AM_4.png\n",
      "2024-07-14_Amanda_AM_5.png\n",
      "2024-07-18_Amanda_PM_14.png\n",
      "2024-07-26_Amanda_AM_20.png\n",
      "2024-08-12_Amanda_PM_11.png\n",
      "2024-07-08_Amanda_AM_26.png\n",
      "2024-07-18_Amanda_AM_20.png\n",
      "2024-07-23_Amanda_PM_6.png\n",
      "2024-07-24_Amanda_PM_2.png\n",
      "2024-07-29_Amanda_AM_5.png\n",
      "2024-08-14_Amanda_AM_2.png\n",
      "2024-07-08_Amanda_AM_17.png\n",
      "2024-08-12_Amanda_PM_20.png\n",
      "2024-07-26_Amanda_AM_11.png\n",
      "2024-07-18_Amanda_AM_11.png\n",
      "2024-07-29_Amanda_AM_8.png\n",
      "2024-08-13_Amanda_AM_6.png\n",
      "2024-07-17_Amanda_AM_16.png\n",
      "2024-07-29_Amanda_AM_16.png\n",
      "2024-07-29_Amanda_PM_22.png\n",
      "2024-07-26_Amanda_PM_19.png\n",
      "2024-08-12_Amanda_AM_28.png\n",
      "2024-07-14_Amanda_AM_8.png\n",
      "2024-07-30_Amanda_AM_9.png\n",
      "2024-07-18_Amanda_PM_19.png\n",
      "2024-07-26_Amanda_PM_25.png\n",
      "2024-08-12_Amanda_AM_14.png\n",
      "2024-07-08_Amanda_PM_23.png\n",
      "2024-08-02_Amanda_AM_12.png\n",
      "2024-07-23_Amanda_AM_6.png\n",
      "2024-07-14_Amanda_PM_14.png\n",
      "2024-07-24_Amanda_AM_2.png\n",
      "2024-07-14_Amanda_PM_28.png\n",
      "2024-07-29_Amanda_PM_5.png\n",
      "2024-07-13_Amanda_PM_1.png\n",
      "2024-07-14_Amanda_AM_20.png\n",
      "2024-07-19_Amanda_AM_2.png\n",
      "2024-07-14_Amanda_PM_5.png\n",
      "2024-07-30_Amanda_PM_4.png\n",
      "2024-08-09_Amanda_PM_19.png\n",
      "2024-07-22_Amanda_AM_15.png\n",
      "2024-07-08_Amanda_PM_8.png\n",
      "2024-08-09_Amanda_PM_25.png\n",
      "2024-07-13_Amanda_AM_12.png\n",
      "2024-08-08_Amanda_PM_2.png\n",
      "2024-07-13_Amanda_PM_26.png\n",
      "2024-08-09_Amanda_AM_11.png\n",
      "2024-07-11_Amanda_PM_9.png\n",
      "2024-08-02_Amanda_AM_1.png\n",
      "2024-07-10_Amanda_PM_10.png\n",
      "2024-07-26_Amanda_PM_7.png\n",
      "2024-06-25_Amanda_PM_12.png\n",
      "2024-07-21_Amanda_PM_3.png\n",
      "2024-07-21_Amanda_PM_17.png\n",
      "2024-07-08_Amanda_AM_5.png\n",
      "2024-07-21_Amanda_AM_23.png\n",
      "2024-07-10_Amanda_AM_18.png\n",
      "2024-07-10_Amanda_AM_24.png\n",
      "2024-08-05_Amanda_PM_8.png\n",
      "2024-07-11_Amanda_AM_4.png\n",
      "2024-06-25_Amanda_AM_26.png\n",
      "2024-08-05_Amanda_PM_9.png\n",
      "2024-07-10_Amanda_AM_25.png\n",
      "2024-07-11_Amanda_AM_5.png\n",
      "2024-07-10_Amanda_AM_19.png\n",
      "2024-07-16_Amanda_AM_1.png\n",
      "2024-07-21_Amanda_AM_22.png\n",
      "2024-07-21_Amanda_PM_16.png\n",
      "2024-07-08_Amanda_AM_4.png\n",
      "2024-07-21_Amanda_PM_2.png\n",
      "2024-07-10_Amanda_PM_11.png\n",
      "2024-06-25_Amanda_PM_13.png\n",
      "2024-07-26_Amanda_PM_6.png\n",
      "2024-07-22_Amanda_PM_20.png\n",
      "2024-08-09_Amanda_AM_10.png\n",
      "2024-07-11_Amanda_PM_8.png\n",
      "2024-08-08_Amanda_PM_3.png\n",
      "2024-07-22_Amanda_AM_28.png\n",
      "2024-07-13_Amanda_AM_13.png\n",
      "2024-08-09_Amanda_PM_24.png\n",
      "2024-07-22_Amanda_AM_14.png\n",
      "2024-07-08_Amanda_PM_9.png\n",
      "2024-08-09_Amanda_PM_18.png\n",
      "2024-07-30_Amanda_PM_5.png\n",
      "2024-07-14_Amanda_PM_4.png\n",
      "2024-07-14_Amanda_AM_21.png\n",
      "2024-07-19_Amanda_AM_3.png\n",
      "2024-07-29_Amanda_PM_4.png\n",
      "2024-07-24_Amanda_AM_3.png\n",
      "2024-07-14_Amanda_PM_15.png\n",
      "2024-07-23_Amanda_AM_7.png\n",
      "2024-07-08_Amanda_PM_22.png\n",
      "2024-07-26_Amanda_PM_24.png\n",
      "2024-08-12_Amanda_AM_15.png\n",
      "2024-08-07_Amanda_PM_1.png\n",
      "2024-08-02_Amanda_AM_13.png\n",
      "2024-07-26_Amanda_PM_18.png\n",
      "2024-07-18_Amanda_PM_18.png\n",
      "2024-07-30_Amanda_AM_8.png\n",
      "2024-07-14_Amanda_AM_9.png\n",
      "2024-07-29_Amanda_PM_23.png\n",
      "2024-07-17_Amanda_AM_17.png\n",
      "2024-07-29_Amanda_AM_17.png\n",
      "2024-08-13_Amanda_AM_7.png\n",
      "2024-07-29_Amanda_AM_9.png\n",
      "2024-08-12_Amanda_PM_21.png\n",
      "2024-07-26_Amanda_AM_10.png\n",
      "2024-07-08_Amanda_AM_16.png\n",
      "2024-08-14_Amanda_AM_3.png\n",
      "2024-07-18_Amanda_AM_10.png\n",
      "2024-07-18_Amanda_AM_12.png\n",
      "2024-08-14_Amanda_AM_1.png\n",
      "2024-07-26_Amanda_AM_12.png\n",
      "2024-08-12_Amanda_PM_23.png\n",
      "2024-07-08_Amanda_AM_14.png\n",
      "2024-07-29_Amanda_AM_15.png\n",
      "2024-07-17_Amanda_AM_15.png\n",
      "2024-07-23_Amanda_PM_8.png\n",
      "2024-08-13_Amanda_AM_5.png\n",
      "2024-07-08_Amanda_AM_28.png\n",
      "2024-07-29_Amanda_PM_21.png\n",
      "2024-08-02_Amanda_AM_11.png\n",
      "2024-08-07_Amanda_PM_3.png\n",
      "2024-07-08_Amanda_PM_20.png\n",
      "2024-08-12_Amanda_AM_17.png\n",
      "2024-07-26_Amanda_PM_26.png\n",
      "2024-07-14_Amanda_PM_17.png\n",
      "2024-07-23_Amanda_AM_5.png\n",
      "2024-08-13_Amanda_PM_8.png\n",
      "2024-07-29_Amanda_PM_6.png\n",
      "2024-07-24_Amanda_AM_1.png\n",
      "2024-07-13_Amanda_PM_2.png\n",
      "2024-07-14_Amanda_PM_6.png\n",
      "2024-07-30_Amanda_PM_7.png\n",
      "2024-07-19_Amanda_AM_1.png\n",
      "2024-07-22_Amanda_AM_16.png\n",
      "2024-07-26_Amanda_AM_9.png\n",
      "2024-07-13_Amanda_AM_11.png\n",
      "2024-08-09_Amanda_PM_26.png\n",
      "2024-08-09_Amanda_AM_12.png\n",
      "2024-07-13_Amanda_PM_25.png\n",
      "2024-08-08_Amanda_PM_1.png\n",
      "2024-08-02_Amanda_AM_2.png\n",
      "2024-07-13_Amanda_PM_19.png\n",
      "2024-07-21_Amanda_PM_28.png\n",
      "2024-07-26_Amanda_PM_4.png\n",
      "2024-06-25_Amanda_PM_11.png\n",
      "2024-07-10_Amanda_PM_13.png\n",
      "2024-07-08_Amanda_AM_6.png\n",
      "2024-07-21_Amanda_PM_14.png\n",
      "2024-07-16_Amanda_AM_3.png\n",
      "2024-06-25_Amanda_AM_19.png\n",
      "2024-07-21_Amanda_AM_20.png\n",
      "2024-07-11_Amanda_AM_7.png\n",
      "2024-06-25_Amanda_AM_25.png\n",
      "2024-07-10_Amanda_AM_27.png\n",
      "2024-06-25_Amanda_AM_24.png\n",
      "2024-07-11_Amanda_AM_6.png\n",
      "2024-07-10_Amanda_AM_26.png\n",
      "2024-07-21_Amanda_AM_21.png\n",
      "2024-06-25_Amanda_AM_18.png\n",
      "2024-07-16_Amanda_AM_2.png\n",
      "2024-07-21_Amanda_PM_1.png\n",
      "2024-07-08_Amanda_AM_7.png\n",
      "2024-07-21_Amanda_PM_15.png\n",
      "2024-06-25_Amanda_PM_10.png\n",
      "2024-07-26_Amanda_PM_5.png\n",
      "2024-07-10_Amanda_PM_12.png\n",
      "2024-07-13_Amanda_PM_18.png\n",
      "2024-08-02_Amanda_AM_3.png\n",
      "2024-07-13_Amanda_PM_24.png\n",
      "2024-08-09_Amanda_AM_13.png\n",
      "2024-07-26_Amanda_AM_8.png\n",
      "2024-08-09_Amanda_PM_27.png\n",
      "2024-07-13_Amanda_AM_10.png\n",
      "2024-07-22_Amanda_AM_17.png\n",
      "2024-07-30_Amanda_PM_6.png\n",
      "2024-07-14_Amanda_PM_7.png\n",
      "2024-07-13_Amanda_PM_3.png\n",
      "2024-07-29_Amanda_PM_7.png\n",
      "2024-07-23_Amanda_AM_4.png\n",
      "2024-08-13_Amanda_PM_9.png\n",
      "2024-07-14_Amanda_PM_16.png\n",
      "2024-08-02_Amanda_AM_10.png\n",
      "2024-08-12_Amanda_AM_16.png\n",
      "2024-07-26_Amanda_PM_27.png\n",
      "2024-07-08_Amanda_PM_21.png\n",
      "2024-08-07_Amanda_PM_2.png\n",
      "2024-07-29_Amanda_PM_20.png\n",
      "2024-07-17_Amanda_PM_20.png\n",
      "2024-07-23_Amanda_PM_9.png\n",
      "2024-08-13_Amanda_AM_4.png\n",
      "2024-07-29_Amanda_AM_14.png\n",
      "2024-07-17_Amanda_AM_14.png\n",
      "2024-07-18_Amanda_AM_13.png\n",
      "2024-07-08_Amanda_AM_15.png\n",
      "2024-07-26_Amanda_AM_13.png\n",
      "2024-08-12_Amanda_PM_22.png\n",
      "2024-07-17_Amanda_AM_10.png\n",
      "2024-07-29_Amanda_AM_10.png\n",
      "2024-08-12_Amanda_PM_26.png\n",
      "2024-07-26_Amanda_AM_17.png\n",
      "2024-07-08_Amanda_AM_11.png\n",
      "2024-08-14_Amanda_AM_4.png\n",
      "2024-07-18_Amanda_AM_17.png\n",
      "2024-07-24_Amanda_PM_9.png\n",
      "2024-08-07_Amanda_PM_6.png\n",
      "2024-07-08_Amanda_PM_25.png\n",
      "2024-07-26_Amanda_PM_23.png\n",
      "2024-08-12_Amanda_AM_12.png\n",
      "2024-07-18_Amanda_PM_23.png\n",
      "2024-07-17_Amanda_PM_18.png\n",
      "2024-07-29_Amanda_PM_18.png\n",
      "2024-07-08_Amanda_PM_19.png\n",
      "2024-07-19_Amanda_PM_9.png\n",
      "2024-07-29_Amanda_PM_24.png\n",
      "2024-07-29_Amanda_PM_3.png\n",
      "2024-08-14_Amanda_PM_9.png\n",
      "2024-07-24_Amanda_AM_4.png\n",
      "2024-07-14_Amanda_PM_12.png\n",
      "2024-07-14_Amanda_PM_3.png\n",
      "2024-07-30_Amanda_PM_2.png\n",
      "2024-07-19_Amanda_AM_4.png\n",
      "2024-07-13_Amanda_PM_7.png\n",
      "2024-07-13_Amanda_AM_14.png\n",
      "2024-08-09_Amanda_PM_23.png\n",
      "2024-07-22_Amanda_AM_13.png\n",
      "2024-07-13_Amanda_AM_28.png\n",
      "2024-07-21_Amanda_AM_8.png\n",
      "2024-08-02_Amanda_AM_7.png\n",
      "2024-08-09_Amanda_AM_17.png\n",
      "2024-07-13_Amanda_PM_20.png\n",
      "2024-08-08_Amanda_PM_4.png\n",
      "2024-07-21_Amanda_PM_11.png\n",
      "2024-07-08_Amanda_AM_3.png\n",
      "2024-07-21_Amanda_PM_5.png\n",
      "2024-07-10_Amanda_PM_16.png\n",
      "2024-06-25_Amanda_PM_14.png\n",
      "2024-07-26_Amanda_PM_1.png\n",
      "2024-07-10_Amanda_AM_22.png\n",
      "2024-06-25_Amanda_AM_20.png\n",
      "2024-07-11_Amanda_AM_2.png\n",
      "2024-08-08_Amanda_AM_9.png\n",
      "2024-07-21_Amanda_AM_19.png\n",
      "2024-07-16_Amanda_AM_6.png\n",
      "2024-07-21_Amanda_AM_25.png\n",
      "2024-08-05_Amanda_PM_12.png\n",
      "2024-07-21_Amanda_AM_24.png\n",
      "2024-08-05_Amanda_PM_13.png\n",
      "2024-07-16_Amanda_AM_7.png\n",
      "2024-07-21_Amanda_AM_18.png\n",
      "2024-08-08_Amanda_AM_8.png\n",
      "2024-07-10_Amanda_AM_23.png\n",
      "2024-07-11_Amanda_AM_3.png\n",
      "2024-06-25_Amanda_AM_21.png\n",
      "2024-07-10_Amanda_PM_17.png\n",
      "2024-06-25_Amanda_PM_15.png\n",
      "2024-07-21_Amanda_PM_4.png\n",
      "2024-07-21_Amanda_PM_10.png\n",
      "2024-07-08_Amanda_AM_2.png\n",
      "2024-08-08_Amanda_PM_5.png\n",
      "2024-07-13_Amanda_PM_21.png\n",
      "2024-08-09_Amanda_AM_16.png\n",
      "2024-08-02_Amanda_AM_6.png\n",
      "2024-07-21_Amanda_AM_9.png\n",
      "2024-07-22_Amanda_AM_12.png\n",
      "2024-08-09_Amanda_PM_22.png\n",
      "2024-07-13_Amanda_AM_15.png\n",
      "2024-07-13_Amanda_PM_6.png\n",
      "2024-07-19_Amanda_AM_5.png\n",
      "2024-07-30_Amanda_PM_3.png\n",
      "2024-07-14_Amanda_PM_2.png\n",
      "2024-07-23_Amanda_AM_1.png\n",
      "2024-07-14_Amanda_PM_13.png\n",
      "2024-08-14_Amanda_PM_8.png\n",
      "2024-07-24_Amanda_AM_5.png\n",
      "2024-07-29_Amanda_PM_2.png\n",
      "2024-07-19_Amanda_PM_8.png\n",
      "2024-07-08_Amanda_PM_18.png\n",
      "2024-07-17_Amanda_PM_19.png\n",
      "2024-07-29_Amanda_PM_19.png\n",
      "2024-07-26_Amanda_PM_22.png\n",
      "2024-08-12_Amanda_AM_13.png\n",
      "2024-07-08_Amanda_PM_24.png\n",
      "2024-08-07_Amanda_PM_7.png\n",
      "2024-07-18_Amanda_PM_22.png\n",
      "2024-08-14_Amanda_AM_5.png\n",
      "2024-07-08_Amanda_AM_10.png\n",
      "2024-08-12_Amanda_PM_27.png\n",
      "2024-07-26_Amanda_AM_16.png\n",
      "2024-07-24_Amanda_PM_8.png\n",
      "2024-07-18_Amanda_AM_16.png\n",
      "2024-08-13_Amanda_AM_1.png\n",
      "2024-07-17_Amanda_AM_11.png\n",
      "2024-07-29_Amanda_AM_11.png\n",
      "2024-08-13_Amanda_AM_3.png\n",
      "2024-08-12_Amanda_PM_19.png\n",
      "2024-07-29_Amanda_AM_13.png\n",
      "2024-07-17_Amanda_AM_13.png\n",
      "2024-07-18_Amanda_AM_14.png\n",
      "2024-07-08_Amanda_AM_12.png\n",
      "2024-07-26_Amanda_AM_14.png\n",
      "2024-08-12_Amanda_PM_25.png\n",
      "2024-08-14_Amanda_AM_7.png\n",
      "2024-07-18_Amanda_PM_20.png\n",
      "2024-07-13_Amanda_AM_9.png\n",
      "2024-08-07_Amanda_PM_5.png\n",
      "2024-08-12_Amanda_AM_11.png\n",
      "2024-07-26_Amanda_PM_20.png\n",
      "2024-07-08_Amanda_PM_26.png\n",
      "2024-07-24_Amanda_AM_7.png\n",
      "2024-07-23_Amanda_AM_3.png\n",
      "2024-07-14_Amanda_PM_11.png\n",
      "2024-07-19_Amanda_AM_7.png\n",
      "2024-07-30_Amanda_PM_1.png\n",
      "2024-07-14_Amanda_AM_19.png\n",
      "2024-07-13_Amanda_PM_4.png\n",
      "2024-08-09_Amanda_PM_20.png\n",
      "2024-07-13_Amanda_AM_17.png\n",
      "2024-07-22_Amanda_AM_10.png\n",
      "2024-07-16_Amanda_PM_8.png\n",
      "2024-08-09_Amanda_AM_28.png\n",
      "2024-08-02_Amanda_AM_4.png\n",
      "2024-07-22_Amanda_PM_18.png\n",
      "2024-08-08_Amanda_PM_7.png\n",
      "2024-07-13_Amanda_PM_23.png\n",
      "2024-08-09_Amanda_AM_14.png\n",
      "2024-07-21_Amanda_PM_6.png\n",
      "2024-07-21_Amanda_PM_12.png\n",
      "2024-06-25_Amanda_PM_17.png\n",
      "2024-07-26_Amanda_PM_2.png\n",
      "2024-07-10_Amanda_PM_15.png\n",
      "2024-06-25_Amanda_AM_23.png\n",
      "2024-07-11_Amanda_AM_1.png\n",
      "2024-07-10_Amanda_AM_21.png\n",
      "2024-08-05_Amanda_PM_11.png\n",
      "2024-07-16_Amanda_AM_5.png\n",
      "2024-07-16_Amanda_AM_4.png\n",
      "2024-08-05_Amanda_PM_10.png\n",
      "2024-06-25_Amanda_AM_22.png\n",
      "2024-07-10_Amanda_AM_20.png\n",
      "2024-07-26_Amanda_PM_3.png\n",
      "2024-06-25_Amanda_PM_16.png\n",
      "2024-07-10_Amanda_PM_14.png\n",
      "2024-07-08_Amanda_AM_1.png\n",
      "2024-07-21_Amanda_PM_13.png\n",
      "2024-07-21_Amanda_PM_7.png\n",
      "2024-07-10_Amanda_PM_28.png\n",
      "2024-08-09_Amanda_AM_15.png\n",
      "2024-07-13_Amanda_PM_22.png\n",
      "2024-08-08_Amanda_PM_6.png\n",
      "2024-07-22_Amanda_PM_19.png\n",
      "2024-07-16_Amanda_PM_9.png\n",
      "2024-08-02_Amanda_AM_5.png\n",
      "2024-07-22_Amanda_AM_11.png\n",
      "2024-07-13_Amanda_AM_16.png\n",
      "2024-08-09_Amanda_PM_21.png\n",
      "2024-07-13_Amanda_PM_5.png\n",
      "2024-07-14_Amanda_AM_18.png\n",
      "2024-07-14_Amanda_PM_1.png\n",
      "2024-07-19_Amanda_AM_6.png\n",
      "2024-07-14_Amanda_PM_10.png\n",
      "2024-07-23_Amanda_AM_2.png\n",
      "2024-07-29_Amanda_PM_1.png\n",
      "2024-07-24_Amanda_AM_6.png\n",
      "2024-07-13_Amanda_AM_8.png\n",
      "2024-07-18_Amanda_PM_21.png\n",
      "2024-07-08_Amanda_PM_27.png\n",
      "2024-08-12_Amanda_AM_10.png\n",
      "2024-07-26_Amanda_PM_21.png\n",
      "2024-08-07_Amanda_PM_4.png\n",
      "2024-07-18_Amanda_AM_15.png\n",
      "2024-08-14_Amanda_AM_6.png\n",
      "2024-07-26_Amanda_AM_15.png\n",
      "2024-08-12_Amanda_PM_24.png\n",
      "2024-07-08_Amanda_AM_13.png\n",
      "2024-07-29_Amanda_AM_12.png\n",
      "2024-07-17_Amanda_AM_12.png\n",
      "2024-08-12_Amanda_PM_18.png\n",
      "2024-08-13_Amanda_AM_2.png\n",
      "2024-07-16_Amanda_PM_23.png\n",
      "2024-07-27_Amanda_PM_18.png\n",
      "2024-07-19_Amanda_PM_18.png\n",
      "2024-07-20_Amanda_AM_8.png\n",
      "2024-08-13_Amanda_AM_15.png\n",
      "2024-07-09_Amanda_PM_22.png\n",
      "2024-07-19_Amanda_PM_24.png\n",
      "2024-07-09_Amanda_AM_16.png\n",
      "2024-08-13_Amanda_PM_21.png\n",
      "2024-07-27_Amanda_AM_10.png\n",
      "2024-08-04_Amanda_AM_3.png\n",
      "2024-07-19_Amanda_AM_10.png\n",
      "2024-08-09_Amanda_PM_4.png\n",
      "2024-07-16_Amanda_AM_17.png\n",
      "2024-06-25_Amanda_PM_7.png\n",
      "2024-07-24_Amanda_AM_26.png\n",
      "2024-07-27_Amanda_PM_1.png\n",
      "2024-07-15_Amanda_AM_21.png\n",
      "2024-07-09_Amanda_AM_3.png\n",
      "2024-08-10_Amanda_AM_8.png\n",
      "2024-07-20_Amanda_PM_5.png\n",
      "2024-07-17_Amanda_AM_6.png\n",
      "2024-07-15_Amanda_PM_15.png\n",
      "2024-07-24_Amanda_PM_12.png\n",
      "2024-07-10_Amanda_AM_2.png\n",
      "2024-08-09_Amanda_AM_9.png\n",
      "2024-08-08_Amanda_AM_10.png\n",
      "2024-07-23_Amanda_PM_20.png\n",
      "2024-08-08_Amanda_PM_18.png\n",
      "2024-07-23_Amanda_AM_14.png\n",
      "2024-07-18_Amanda_PM_9.png\n",
      "2024-07-12_Amanda_AM_13.png\n",
      "2024-07-20_Amanda_AM_22.png\n",
      "2024-08-14_Amanda_PM_13.png\n",
      "2024-08-04_Amanda_PM_15.png\n",
      "2024-07-11_Amanda_AM_19.png\n",
      "2024-07-30_Amanda_AM_18.png\n",
      "2024-07-11_Amanda_AM_25.png\n",
      "2024-07-11_Amanda_PM_11.png\n",
      "2024-07-12_Amanda_PM_7.png\n",
      "2024-07-15_Amanda_PM_3.png\n",
      "2024-08-14_Amanda_AM_27.png\n",
      "2024-07-20_Amanda_PM_16.png\n",
      "2024-07-18_Amanda_AM_4.png\n",
      "2024-08-04_Amanda_AM_21.png\n",
      "2024-07-30_Amanda_PM_10.png\n",
      "2024-08-14_Amanda_AM_26.png\n",
      "2024-07-20_Amanda_PM_17.png\n",
      "2024-08-04_Amanda_AM_20.png\n",
      "2024-07-30_Amanda_PM_11.png\n",
      "2024-07-18_Amanda_AM_5.png\n",
      "2024-07-15_Amanda_PM_2.png\n",
      "2024-07-11_Amanda_PM_10.png\n",
      "2024-07-12_Amanda_PM_6.png\n",
      "2024-07-11_Amanda_AM_24.png\n",
      "2024-07-30_Amanda_AM_19.png\n",
      "2024-07-11_Amanda_AM_18.png\n",
      "2024-07-22_Amanda_AM_1.png\n",
      "2024-07-20_Amanda_AM_23.png\n",
      "2024-08-14_Amanda_PM_12.png\n",
      "2024-08-04_Amanda_PM_14.png\n",
      "2024-07-12_Amanda_AM_12.png\n",
      "2024-07-23_Amanda_AM_15.png\n",
      "2024-07-18_Amanda_PM_8.png\n",
      "2024-08-08_Amanda_PM_19.png\n",
      "2024-08-12_Amanda_AM_1.png\n",
      "2024-07-23_Amanda_PM_21.png\n",
      "2024-08-08_Amanda_AM_11.png\n",
      "2024-08-09_Amanda_AM_8.png\n",
      "2024-07-24_Amanda_PM_13.png\n",
      "2024-07-10_Amanda_AM_3.png\n",
      "2024-07-15_Amanda_PM_14.png\n",
      "2024-07-17_Amanda_AM_7.png\n",
      "2024-08-10_Amanda_AM_9.png\n",
      "2024-07-20_Amanda_PM_4.png\n",
      "2024-07-15_Amanda_AM_20.png\n",
      "2024-07-09_Amanda_AM_2.png\n",
      "2024-07-24_Amanda_AM_27.png\n",
      "2024-06-25_Amanda_PM_6.png\n",
      "2024-07-16_Amanda_AM_16.png\n",
      "2024-08-09_Amanda_PM_5.png\n",
      "2024-08-04_Amanda_AM_2.png\n",
      "2024-08-13_Amanda_PM_20.png\n",
      "2024-07-27_Amanda_AM_11.png\n",
      "2024-07-09_Amanda_AM_17.png\n",
      "2024-07-19_Amanda_AM_11.png\n",
      "2024-07-09_Amanda_PM_23.png\n",
      "2024-08-13_Amanda_AM_14.png\n",
      "2024-07-19_Amanda_PM_25.png\n",
      "2024-07-27_Amanda_PM_19.png\n",
      "2024-07-20_Amanda_AM_9.png\n",
      "2024-07-19_Amanda_PM_19.png\n",
      "2024-07-16_Amanda_PM_22.png\n",
      "2024-07-16_Amanda_PM_20.png\n",
      "2024-07-19_Amanda_PM_27.png\n",
      "2024-07-09_Amanda_PM_21.png\n",
      "2024-08-13_Amanda_AM_16.png\n",
      "2024-06-25_Amanda_AM_9.png\n",
      "2024-08-09_Amanda_PM_7.png\n",
      "2024-07-19_Amanda_AM_13.png\n",
      "2024-07-27_Amanda_AM_13.png\n",
      "2024-08-13_Amanda_PM_22.png\n",
      "2024-07-09_Amanda_AM_15.png\n",
      "2024-07-16_Amanda_AM_14.png\n",
      "2024-07-17_Amanda_PM_8.png\n",
      "2024-07-27_Amanda_PM_2.png\n",
      "2024-07-24_Amanda_AM_25.png\n",
      "2024-06-25_Amanda_PM_4.png\n",
      "2024-07-20_Amanda_PM_6.png\n",
      "2024-07-24_Amanda_AM_19.png\n",
      "2024-07-15_Amanda_AM_22.png\n",
      "2024-07-15_Amanda_PM_16.png\n",
      "2024-07-17_Amanda_AM_5.png\n",
      "2024-08-03_Amanda_PM_9.png\n",
      "2024-07-10_Amanda_AM_1.png\n",
      "2024-07-24_Amanda_PM_11.png\n",
      "2024-08-08_Amanda_AM_13.png\n",
      "2024-07-12_Amanda_PM_18.png\n",
      "2024-08-12_Amanda_AM_3.png\n",
      "2024-07-23_Amanda_PM_23.png\n",
      "2024-07-23_Amanda_AM_17.png\n",
      "2024-07-12_Amanda_AM_9.png\n",
      "2024-07-12_Amanda_AM_10.png\n",
      "2024-07-22_Amanda_AM_3.png\n",
      "2024-08-04_Amanda_PM_16.png\n",
      "2024-08-14_Amanda_PM_10.png\n",
      "2024-07-20_Amanda_AM_21.png\n",
      "2024-07-11_Amanda_AM_26.png\n",
      "2024-08-14_Amanda_AM_18.png\n",
      "2024-07-12_Amanda_PM_4.png\n",
      "2024-07-11_Amanda_PM_12.png\n",
      "2024-07-18_Amanda_AM_7.png\n",
      "2024-07-30_Amanda_PM_13.png\n",
      "2024-08-04_Amanda_AM_22.png\n",
      "2024-07-20_Amanda_PM_15.png\n",
      "2024-08-14_Amanda_AM_24.png\n",
      "2024-07-15_Amanda_PM_1.png\n",
      "2024-07-30_Amanda_PM_12.png\n",
      "2024-08-04_Amanda_AM_23.png\n",
      "2024-07-18_Amanda_AM_6.png\n",
      "2024-07-20_Amanda_PM_14.png\n",
      "2024-08-14_Amanda_AM_25.png\n",
      "2024-07-12_Amanda_PM_5.png\n",
      "2024-07-11_Amanda_PM_13.png\n",
      "2024-08-14_Amanda_AM_19.png\n",
      "2024-07-11_Amanda_AM_27.png\n",
      "2024-08-04_Amanda_PM_17.png\n",
      "2024-08-14_Amanda_PM_11.png\n",
      "2024-07-20_Amanda_AM_20.png\n",
      "2024-07-22_Amanda_AM_2.png\n",
      "2024-07-12_Amanda_AM_8.png\n",
      "2024-07-12_Amanda_AM_11.png\n",
      "2024-07-23_Amanda_AM_16.png\n",
      "2024-07-23_Amanda_PM_22.png\n",
      "2024-08-12_Amanda_AM_2.png\n",
      "2024-07-12_Amanda_PM_19.png\n",
      "2024-08-08_Amanda_AM_12.png\n",
      "2024-07-24_Amanda_PM_10.png\n",
      "2024-07-17_Amanda_AM_4.png\n",
      "2024-08-03_Amanda_PM_8.png\n",
      "2024-07-09_Amanda_AM_1.png\n",
      "2024-07-15_Amanda_AM_23.png\n",
      "2024-07-20_Amanda_PM_7.png\n",
      "2024-07-24_Amanda_AM_18.png\n",
      "2024-06-25_Amanda_PM_5.png\n",
      "2024-07-27_Amanda_PM_3.png\n",
      "2024-07-24_Amanda_AM_24.png\n",
      "2024-07-17_Amanda_PM_9.png\n",
      "2024-07-09_Amanda_AM_28.png\n",
      "2024-07-16_Amanda_AM_15.png\n",
      "2024-07-19_Amanda_AM_12.png\n",
      "2024-08-04_Amanda_AM_1.png\n",
      "2024-07-09_Amanda_AM_14.png\n",
      "2024-07-27_Amanda_AM_12.png\n",
      "2024-08-13_Amanda_PM_23.png\n",
      "2024-08-09_Amanda_PM_6.png\n",
      "2024-06-25_Amanda_AM_8.png\n",
      "2024-07-19_Amanda_PM_26.png\n",
      "2024-08-13_Amanda_AM_17.png\n",
      "2024-07-09_Amanda_PM_20.png\n",
      "2024-07-16_Amanda_PM_21.png\n",
      "2024-07-09_Amanda_PM_24.png\n",
      "2024-08-13_Amanda_AM_13.png\n",
      "2024-07-19_Amanda_PM_22.png\n",
      "2024-07-16_Amanda_PM_19.png\n",
      "2024-07-09_Amanda_PM_18.png\n",
      "2024-07-16_Amanda_PM_25.png\n",
      "2024-07-09_Amanda_PM_8.png\n",
      "2024-07-16_Amanda_AM_11.png\n",
      "2024-07-28_Amanda_AM_11.png\n",
      "2024-08-09_Amanda_PM_2.png\n",
      "2024-08-04_Amanda_AM_5.png\n",
      "2024-08-13_Amanda_PM_27.png\n",
      "2024-07-27_Amanda_AM_16.png\n",
      "2024-07-09_Amanda_AM_10.png\n",
      "2024-07-19_Amanda_AM_16.png\n",
      "2024-07-10_Amanda_PM_9.png\n",
      "2024-07-20_Amanda_PM_3.png\n",
      "2024-07-15_Amanda_AM_27.png\n",
      "2024-07-09_Amanda_AM_5.png\n",
      "2024-07-24_Amanda_AM_20.png\n",
      "2024-07-27_Amanda_PM_7.png\n",
      "2024-06-25_Amanda_PM_1.png\n",
      "2024-08-04_Amanda_PM_8.png\n",
      "2024-07-24_Amanda_PM_14.png\n",
      "2024-07-10_Amanda_AM_4.png\n",
      "2024-07-15_Amanda_PM_13.png\n",
      "2024-08-12_Amanda_AM_6.png\n",
      "2024-07-23_Amanda_PM_26.png\n",
      "2024-08-08_Amanda_AM_16.png\n",
      "2024-07-12_Amanda_PM_21.png\n",
      "2024-07-28_Amanda_AM_8.png\n",
      "2024-07-12_Amanda_AM_15.png\n",
      "2024-07-23_Amanda_AM_12.png\n",
      "2024-07-15_Amanda_AM_8.png\n",
      "2024-07-31_Amanda_AM_9.png\n",
      "2024-07-11_Amanda_AM_23.png\n",
      "2024-07-20_Amanda_AM_18.png\n",
      "2024-07-22_Amanda_AM_6.png\n",
      "2024-07-20_Amanda_AM_24.png\n",
      "2024-08-14_Amanda_PM_15.png\n",
      "2024-08-04_Amanda_PM_13.png\n",
      "2024-08-14_Amanda_AM_21.png\n",
      "2024-07-20_Amanda_PM_10.png\n",
      "2024-07-18_Amanda_AM_2.png\n",
      "2024-08-04_Amanda_AM_27.png\n",
      "2024-07-30_Amanda_PM_16.png\n",
      "2024-07-15_Amanda_PM_5.png\n",
      "2024-07-11_Amanda_PM_17.png\n",
      "2024-07-12_Amanda_PM_1.png\n",
      "2024-07-11_Amanda_PM_16.png\n",
      "2024-07-15_Amanda_PM_4.png\n",
      "2024-08-14_Amanda_AM_20.png\n",
      "2024-07-20_Amanda_PM_11.png\n",
      "2024-08-04_Amanda_AM_26.png\n",
      "2024-07-30_Amanda_PM_17.png\n",
      "2024-07-18_Amanda_AM_3.png\n",
      "2024-07-20_Amanda_AM_25.png\n",
      "2024-08-14_Amanda_PM_14.png\n",
      "2024-08-04_Amanda_PM_12.png\n",
      "2024-07-22_Amanda_AM_7.png\n",
      "2024-07-20_Amanda_AM_19.png\n",
      "2024-08-14_Amanda_PM_28.png\n",
      "2024-07-11_Amanda_AM_22.png\n",
      "2024-07-31_Amanda_AM_8.png\n",
      "2024-07-15_Amanda_AM_9.png\n",
      "2024-07-23_Amanda_AM_13.png\n",
      "2024-07-12_Amanda_AM_14.png\n",
      "2024-07-28_Amanda_AM_9.png\n",
      "2024-07-12_Amanda_PM_20.png\n",
      "2024-08-08_Amanda_AM_17.png\n",
      "2024-07-23_Amanda_PM_27.png\n",
      "2024-08-12_Amanda_AM_7.png\n",
      "2024-07-17_Amanda_AM_1.png\n",
      "2024-07-15_Amanda_PM_12.png\n",
      "2024-07-24_Amanda_PM_15.png\n",
      "2024-08-04_Amanda_PM_9.png\n",
      "2024-07-10_Amanda_AM_5.png\n",
      "2024-07-24_Amanda_AM_21.png\n",
      "2024-07-27_Amanda_PM_6.png\n",
      "2024-07-15_Amanda_AM_26.png\n",
      "2024-07-09_Amanda_AM_4.png\n",
      "2024-07-20_Amanda_PM_2.png\n",
      "2024-07-09_Amanda_AM_11.png\n",
      "2024-08-13_Amanda_PM_26.png\n",
      "2024-07-27_Amanda_AM_17.png\n",
      "2024-08-04_Amanda_AM_4.png\n",
      "2024-07-10_Amanda_PM_8.png\n",
      "2024-07-19_Amanda_AM_17.png\n",
      "2024-08-09_Amanda_PM_3.png\n",
      "2024-07-16_Amanda_AM_10.png\n",
      "2024-07-28_Amanda_AM_10.png\n",
      "2024-07-16_Amanda_PM_24.png\n",
      "2024-07-09_Amanda_PM_9.png\n",
      "2024-07-09_Amanda_PM_19.png\n",
      "2024-07-16_Amanda_PM_18.png\n",
      "2024-08-13_Amanda_AM_12.png\n",
      "2024-07-09_Amanda_PM_25.png\n",
      "2024-07-19_Amanda_PM_23.png\n",
      "2024-07-19_Amanda_PM_21.png\n",
      "2024-07-27_Amanda_AM_9.png\n",
      "2024-08-13_Amanda_AM_10.png\n",
      "2024-07-09_Amanda_PM_27.png\n",
      "2024-07-16_Amanda_PM_26.png\n",
      "2024-08-13_Amanda_PM_18.png\n",
      "2024-07-28_Amanda_AM_12.png\n",
      "2024-07-16_Amanda_AM_12.png\n",
      "2024-07-19_Amanda_AM_15.png\n",
      "2024-08-04_Amanda_AM_6.png\n",
      "2024-07-09_Amanda_AM_13.png\n",
      "2024-07-27_Amanda_AM_15.png\n",
      "2024-08-13_Amanda_PM_24.png\n",
      "2024-08-09_Amanda_PM_1.png\n",
      "2024-07-09_Amanda_AM_6.png\n",
      "2024-07-15_Amanda_AM_24.png\n",
      "2024-06-25_Amanda_PM_2.png\n",
      "2024-07-15_Amanda_AM_18.png\n",
      "2024-07-27_Amanda_PM_4.png\n",
      "2024-07-24_Amanda_AM_23.png\n",
      "2024-07-10_Amanda_AM_7.png\n",
      "2024-07-24_Amanda_PM_17.png\n",
      "2024-07-17_Amanda_AM_3.png\n",
      "2024-07-15_Amanda_PM_10.png\n",
      "2024-07-23_Amanda_PM_25.png\n",
      "2024-07-22_Amanda_PM_8.png\n",
      "2024-08-12_Amanda_AM_5.png\n",
      "2024-07-23_Amanda_PM_19.png\n",
      "2024-07-12_Amanda_PM_22.png\n",
      "2024-08-08_Amanda_AM_15.png\n",
      "2024-07-12_Amanda_AM_16.png\n",
      "2024-07-23_Amanda_AM_11.png\n",
      "2024-07-11_Amanda_AM_20.png\n",
      "2024-08-04_Amanda_PM_10.png\n",
      "2024-07-30_Amanda_AM_21.png\n",
      "2024-08-14_Amanda_PM_16.png\n",
      "2024-07-20_Amanda_AM_27.png\n",
      "2024-07-22_Amanda_AM_5.png\n",
      "2024-08-12_Amanda_PM_8.png\n",
      "2024-07-15_Amanda_PM_6.png\n",
      "2024-07-18_Amanda_AM_1.png\n",
      "2024-07-30_Amanda_PM_15.png\n",
      "2024-08-04_Amanda_AM_24.png\n",
      "2024-07-20_Amanda_PM_13.png\n",
      "2024-08-14_Amanda_AM_22.png\n",
      "2024-07-12_Amanda_PM_2.png\n",
      "2024-07-11_Amanda_PM_14.png\n",
      "2024-08-04_Amanda_AM_18.png\n",
      "2024-08-04_Amanda_AM_19.png\n",
      "2024-07-12_Amanda_PM_3.png\n",
      "2024-07-11_Amanda_PM_15.png\n",
      "2024-07-30_Amanda_PM_14.png\n",
      "2024-08-04_Amanda_AM_25.png\n",
      "2024-07-20_Amanda_PM_12.png\n",
      "2024-08-14_Amanda_AM_23.png\n",
      "2024-07-15_Amanda_PM_7.png\n",
      "2024-07-22_Amanda_AM_4.png\n",
      "2024-08-12_Amanda_PM_9.png\n",
      "2024-08-04_Amanda_PM_11.png\n",
      "2024-07-30_Amanda_AM_20.png\n",
      "2024-08-14_Amanda_PM_17.png\n",
      "2024-07-20_Amanda_AM_26.png\n",
      "2024-07-11_Amanda_AM_21.png\n",
      "2024-07-23_Amanda_AM_10.png\n",
      "2024-07-12_Amanda_AM_17.png\n",
      "2024-08-08_Amanda_PM_20.png\n",
      "2024-08-08_Amanda_AM_14.png\n",
      "2024-07-23_Amanda_PM_18.png\n",
      "2024-07-22_Amanda_PM_9.png\n",
      "2024-08-08_Amanda_AM_28.png\n",
      "2024-08-12_Amanda_AM_4.png\n",
      "2024-07-23_Amanda_PM_24.png\n",
      "2024-07-15_Amanda_PM_11.png\n",
      "2024-07-17_Amanda_AM_2.png\n",
      "2024-07-10_Amanda_AM_6.png\n",
      "2024-07-24_Amanda_PM_16.png\n",
      "2024-07-27_Amanda_PM_5.png\n",
      "2024-07-24_Amanda_AM_22.png\n",
      "2024-06-25_Amanda_PM_3.png\n",
      "2024-07-15_Amanda_AM_19.png\n",
      "2024-07-20_Amanda_PM_1.png\n",
      "2024-07-09_Amanda_AM_7.png\n",
      "2024-07-15_Amanda_AM_25.png\n",
      "2024-07-19_Amanda_AM_14.png\n",
      "2024-07-27_Amanda_AM_14.png\n",
      "2024-08-13_Amanda_PM_25.png\n",
      "2024-07-09_Amanda_AM_12.png\n",
      "2024-08-04_Amanda_AM_7.png\n",
      "2024-07-16_Amanda_AM_13.png\n",
      "2024-07-27_Amanda_AM_28.png\n",
      "2024-08-13_Amanda_PM_19.png\n",
      "2024-07-16_Amanda_PM_27.png\n",
      "2024-07-27_Amanda_AM_8.png\n",
      "2024-07-19_Amanda_PM_20.png\n",
      "2024-07-09_Amanda_PM_26.png\n",
      "2024-08-13_Amanda_AM_11.png\n",
      "2024-07-27_Amanda_PM_20.png\n",
      "(1395, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create placeholders for image data list and label list\n",
    "img_data_list = []\n",
    "labels_list = []\n",
    "date_list = []\n",
    "\n",
    "#  Get data path to folder that contains a folder for each label with corresponding spectrograms\n",
    "data_path = '/Users/htr365/no_icloud/quantified_self_all/data/spectrograms/AB_sit_relaxed'\n",
    "\n",
    "# read in data\n",
    "img_list = [f for f in os.listdir(data_path) if f.endswith('.png')]\n",
    "for img in img_list:\n",
    "    print(img)\n",
    "    #try:     \n",
    "    date = img.split('_')[0]\n",
    "    person = img.split('_')[1]\n",
    "    label = survey[(survey['person'] == person)*(survey['date']==(pd.to_datetime(date)+timedelta(1)).strftime(\"%Y-%m-%d\"))][outcome]\n",
    "    if len(label)==1:\n",
    "        labels_list.append(label.values[0])\n",
    "        date = img.split('_')[0]\n",
    "        person = img.split('_')[1]\n",
    "        timing =  img.split('_')[2]\n",
    "        input_img=cv2.imread(data_path + '/' + img)\n",
    "        input_img_resize = cv2.resize(input_img, (128, 128))\n",
    "        img_data_list.append(input_img_resize)\n",
    "\n",
    "\n",
    "        date_list.append(date)\n",
    "    #except:\n",
    "     #  next\n",
    "\n",
    "# convert to numpy array and then to float, normalize through dividing by 511\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 511 # divide by 511\n",
    "img_data = img_data.transpose(0,3,1,2)\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_train, indicators_val, indicators_test = identifiers[pd.to_datetime(identifiers['date'])<pd.to_datetime('2024-08-03')],\\\n",
    "    identifiers[(pd.to_datetime(identifiers['date'])<pd.to_datetime('2024-08-07'))*(pd.to_datetime(identifiers['date'])>=pd.to_datetime('2024-08-03'))],\\\n",
    "    identifiers[pd.to_datetime(identifiers['date'])>=pd.to_datetime('2024-08-07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.8750, 4.7500, 6.0000,  ..., 5.2500, 6.3750, 4.8750])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.from_numpy(np.array(labels_list,dtype=np.float32)[np.where(np.isin(date_list, indicators_train['date']))[0]])\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    This will be the very basic CNN model we will use for the regression task.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size: Tuple[int, int, int] = (3, 100, 100)):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.image_size[0], out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear_line_size = int(16*(image_size[1]//4)*(image_size[2]//4))\n",
    "        self.fc1 = nn.Linear(in_features=self.linear_line_size, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the data through the network.\n",
    "        There are commented out print statements that can be used to \n",
    "        check the size of the tensor at each layer. These are very useful when\n",
    "        the image size changes and you want to check that the network layers are \n",
    "        still the correct shape.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        # print('Size of tensor after each layer')\n",
    "        # print(f'conv1 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        # print(f'relu1 {x.size()}')\n",
    "        x = self.pool1(x)\n",
    "        # print(f'pool1 {x.size()}')\n",
    "        x = self.conv2(x)\n",
    "        # print(f'conv2 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        # print(f'relu2 {x.size()}')\n",
    "        x = self.pool2(x)\n",
    "        #print(f'pool2 {x.size()}')\n",
    "        x = x.reshape(-1, self.linear_line_size)\n",
    "        #print(f'view1 {x.size()}')\n",
    "        x = self.fc1(x)\n",
    "        # print(f'fc1 {x.size()}')\n",
    "        x = nn.functional.relu(x)\n",
    "        # print(f'relu2 {x.size()}')\n",
    "        x = self.fc2(x)\n",
    "        #print(x)\n",
    "\n",
    "        # print(f'fc2 {x.size()}')\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=tuple(img_data.shape[1:])\n",
    "\n",
    "model = CNNRegression(image_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4035"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3147, 3, 128, 128])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(img_data[np.where(np.isin(date_list, indicators_train['date']))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch [1/1000], Loss: 4.7535\n",
      "Epoch [1/1000], Loss: 2.7199\n",
      "Epoch [1/1000], Loss: 1.3199\n",
      "Epoch [1/1000], Loss: 2.0855\n",
      "Epoch [1/1000], Loss: 2.4974\n",
      "Epoch [1/1000], Loss: 1.8879\n",
      "Epoch [1/1000], Loss: 1.4670\n",
      "Epoch [1/1000], Loss: 1.1691\n",
      "Epoch [1/1000], Loss: 1.2593\n",
      "Epoch [1/1000], Loss: 1.3104\n",
      "Epoch [1/1000], Loss: 1.3610\n",
      "tensor(2.2167, grad_fn=<MeanBackward0>)\n",
      "1\n",
      "Epoch [2/1000], Loss: 1.1547\n",
      "Epoch [2/1000], Loss: 1.0533\n",
      "Epoch [2/1000], Loss: 1.1583\n",
      "Epoch [2/1000], Loss: 1.1033\n",
      "Epoch [2/1000], Loss: 1.2379\n",
      "Epoch [2/1000], Loss: 1.0840\n",
      "Epoch [2/1000], Loss: 1.2870\n",
      "Epoch [2/1000], Loss: 1.0376\n",
      "Epoch [2/1000], Loss: 0.9231\n",
      "Epoch [2/1000], Loss: 0.9754\n",
      "Epoch [2/1000], Loss: 0.9467\n",
      "tensor(2.6274, grad_fn=<MeanBackward0>)\n",
      "2\n",
      "Epoch [3/1000], Loss: 1.0448\n",
      "Epoch [3/1000], Loss: 1.0371\n",
      "Epoch [3/1000], Loss: 1.1091\n",
      "Epoch [3/1000], Loss: 0.9350\n",
      "Epoch [3/1000], Loss: 1.1223\n",
      "Epoch [3/1000], Loss: 1.0501\n",
      "Epoch [3/1000], Loss: 1.2614\n",
      "Epoch [3/1000], Loss: 0.9828\n",
      "Epoch [3/1000], Loss: 0.8341\n",
      "Epoch [3/1000], Loss: 0.9363\n",
      "Epoch [3/1000], Loss: 0.8746\n",
      "tensor(2.5967, grad_fn=<MeanBackward0>)\n",
      "3\n",
      "Epoch [4/1000], Loss: 1.0206\n",
      "Epoch [4/1000], Loss: 1.0101\n",
      "Epoch [4/1000], Loss: 1.0695\n",
      "Epoch [4/1000], Loss: 0.8805\n",
      "Epoch [4/1000], Loss: 1.0924\n",
      "Epoch [4/1000], Loss: 1.0217\n",
      "Epoch [4/1000], Loss: 1.3031\n",
      "Epoch [4/1000], Loss: 0.9731\n",
      "Epoch [4/1000], Loss: 0.8297\n",
      "Epoch [4/1000], Loss: 0.9343\n",
      "Epoch [4/1000], Loss: 0.9369\n",
      "tensor(2.4409, grad_fn=<MeanBackward0>)\n",
      "4\n",
      "Epoch [5/1000], Loss: 1.0429\n",
      "Epoch [5/1000], Loss: 1.0038\n",
      "Epoch [5/1000], Loss: 1.0401\n",
      "Epoch [5/1000], Loss: 0.8609\n",
      "Epoch [5/1000], Loss: 1.0639\n",
      "Epoch [5/1000], Loss: 1.0017\n",
      "Epoch [5/1000], Loss: 1.2529\n",
      "Epoch [5/1000], Loss: 0.9252\n",
      "Epoch [5/1000], Loss: 0.8201\n",
      "Epoch [5/1000], Loss: 0.9453\n",
      "Epoch [5/1000], Loss: 0.9171\n",
      "tensor(2.5196, grad_fn=<MeanBackward0>)\n",
      "5\n",
      "Epoch [6/1000], Loss: 1.0086\n",
      "Epoch [6/1000], Loss: 0.9954\n",
      "Epoch [6/1000], Loss: 1.0090\n",
      "Epoch [6/1000], Loss: 0.8614\n",
      "Epoch [6/1000], Loss: 1.0436\n",
      "Epoch [6/1000], Loss: 0.9929\n",
      "Epoch [6/1000], Loss: 1.1725\n",
      "Epoch [6/1000], Loss: 0.8850\n",
      "Epoch [6/1000], Loss: 0.8148\n",
      "Epoch [6/1000], Loss: 0.9676\n",
      "Epoch [6/1000], Loss: 0.8544\n",
      "tensor(2.5788, grad_fn=<MeanBackward0>)\n",
      "6\n",
      "Epoch [7/1000], Loss: 0.9869\n",
      "Epoch [7/1000], Loss: 0.9884\n",
      "Epoch [7/1000], Loss: 1.0003\n",
      "Epoch [7/1000], Loss: 0.8267\n",
      "Epoch [7/1000], Loss: 1.0184\n",
      "Epoch [7/1000], Loss: 0.9687\n",
      "Epoch [7/1000], Loss: 1.2311\n",
      "Epoch [7/1000], Loss: 0.8615\n",
      "Epoch [7/1000], Loss: 0.8186\n",
      "Epoch [7/1000], Loss: 0.9795\n",
      "Epoch [7/1000], Loss: 0.9277\n",
      "tensor(2.5605, grad_fn=<MeanBackward0>)\n",
      "7\n",
      "Epoch [8/1000], Loss: 0.9760\n",
      "Epoch [8/1000], Loss: 0.9998\n",
      "Epoch [8/1000], Loss: 0.9846\n",
      "Epoch [8/1000], Loss: 0.8029\n",
      "Epoch [8/1000], Loss: 1.0008\n",
      "Epoch [8/1000], Loss: 0.9436\n",
      "Epoch [8/1000], Loss: 1.1082\n",
      "Epoch [8/1000], Loss: 0.8116\n",
      "Epoch [8/1000], Loss: 0.8060\n",
      "Epoch [8/1000], Loss: 0.9904\n",
      "Epoch [8/1000], Loss: 0.8664\n",
      "tensor(2.5910, grad_fn=<MeanBackward0>)\n",
      "8\n",
      "Epoch [9/1000], Loss: 0.9603\n",
      "Epoch [9/1000], Loss: 0.9812\n",
      "Epoch [9/1000], Loss: 0.9443\n",
      "Epoch [9/1000], Loss: 0.7585\n",
      "Epoch [9/1000], Loss: 0.9492\n",
      "Epoch [9/1000], Loss: 0.8810\n",
      "Epoch [9/1000], Loss: 1.0909\n",
      "Epoch [9/1000], Loss: 0.7523\n",
      "Epoch [9/1000], Loss: 0.8081\n",
      "Epoch [9/1000], Loss: 1.0107\n",
      "Epoch [9/1000], Loss: 0.8693\n",
      "tensor(2.6996, grad_fn=<MeanBackward0>)\n",
      "9\n",
      "Epoch [10/1000], Loss: 0.9576\n",
      "Epoch [10/1000], Loss: 0.9934\n",
      "Epoch [10/1000], Loss: 0.9055\n",
      "Epoch [10/1000], Loss: 0.7435\n",
      "Epoch [10/1000], Loss: 0.9170\n",
      "Epoch [10/1000], Loss: 0.8737\n",
      "Epoch [10/1000], Loss: 1.0907\n",
      "Epoch [10/1000], Loss: 0.7870\n",
      "Epoch [10/1000], Loss: 0.8103\n",
      "Epoch [10/1000], Loss: 1.0122\n",
      "Epoch [10/1000], Loss: 0.8811\n",
      "tensor(2.6536, grad_fn=<MeanBackward0>)\n",
      "10\n",
      "Epoch [11/1000], Loss: 0.9489\n",
      "Epoch [11/1000], Loss: 0.9956\n",
      "Epoch [11/1000], Loss: 0.9339\n",
      "Epoch [11/1000], Loss: 0.7424\n",
      "Epoch [11/1000], Loss: 0.9520\n",
      "Epoch [11/1000], Loss: 0.9080\n",
      "Epoch [11/1000], Loss: 1.0126\n",
      "Epoch [11/1000], Loss: 0.7388\n",
      "Epoch [11/1000], Loss: 0.7939\n",
      "Epoch [11/1000], Loss: 1.0112\n",
      "Epoch [11/1000], Loss: 0.8556\n",
      "tensor(2.5461, grad_fn=<MeanBackward0>)\n",
      "11\n",
      "Epoch [12/1000], Loss: 0.9431\n",
      "Epoch [12/1000], Loss: 0.9632\n",
      "Epoch [12/1000], Loss: 0.9093\n",
      "Epoch [12/1000], Loss: 0.7348\n",
      "Epoch [12/1000], Loss: 0.9033\n",
      "Epoch [12/1000], Loss: 0.8841\n",
      "Epoch [12/1000], Loss: 1.0194\n",
      "Epoch [12/1000], Loss: 0.7488\n",
      "Epoch [12/1000], Loss: 0.7954\n",
      "Epoch [12/1000], Loss: 1.0018\n",
      "Epoch [12/1000], Loss: 0.8468\n",
      "tensor(2.5636, grad_fn=<MeanBackward0>)\n",
      "12\n",
      "Epoch [13/1000], Loss: 0.9365\n",
      "Epoch [13/1000], Loss: 0.9733\n",
      "Epoch [13/1000], Loss: 0.9225\n",
      "Epoch [13/1000], Loss: 0.7413\n",
      "Epoch [13/1000], Loss: 0.9069\n",
      "Epoch [13/1000], Loss: 0.8856\n",
      "Epoch [13/1000], Loss: 0.9900\n",
      "Epoch [13/1000], Loss: 0.7266\n",
      "Epoch [13/1000], Loss: 0.8008\n",
      "Epoch [13/1000], Loss: 1.0178\n",
      "Epoch [13/1000], Loss: 0.8093\n",
      "tensor(2.5140, grad_fn=<MeanBackward0>)\n",
      "13\n",
      "Epoch [14/1000], Loss: 0.9364\n",
      "Epoch [14/1000], Loss: 0.9490\n",
      "Epoch [14/1000], Loss: 0.9009\n",
      "Epoch [14/1000], Loss: 0.7335\n",
      "Epoch [14/1000], Loss: 0.8937\n",
      "Epoch [14/1000], Loss: 0.8602\n",
      "Epoch [14/1000], Loss: 0.9796\n",
      "Epoch [14/1000], Loss: 0.7321\n",
      "Epoch [14/1000], Loss: 0.7803\n",
      "Epoch [14/1000], Loss: 1.0200\n",
      "Epoch [14/1000], Loss: 0.7778\n",
      "tensor(2.6427, grad_fn=<MeanBackward0>)\n",
      "14\n",
      "Epoch [15/1000], Loss: 0.9305\n",
      "Epoch [15/1000], Loss: 0.9516\n",
      "Epoch [15/1000], Loss: 0.9083\n",
      "Epoch [15/1000], Loss: 0.7221\n",
      "Epoch [15/1000], Loss: 0.8907\n",
      "Epoch [15/1000], Loss: 0.8334\n",
      "Epoch [15/1000], Loss: 1.0201\n",
      "Epoch [15/1000], Loss: 0.7242\n",
      "Epoch [15/1000], Loss: 0.7722\n",
      "Epoch [15/1000], Loss: 0.9881\n",
      "Epoch [15/1000], Loss: 0.8370\n",
      "tensor(2.5477, grad_fn=<MeanBackward0>)\n",
      "15\n",
      "Epoch [16/1000], Loss: 0.9260\n",
      "Epoch [16/1000], Loss: 0.9738\n",
      "Epoch [16/1000], Loss: 0.9208\n",
      "Epoch [16/1000], Loss: 0.7289\n",
      "Epoch [16/1000], Loss: 0.8926\n",
      "Epoch [16/1000], Loss: 0.9010\n",
      "Epoch [16/1000], Loss: 0.9573\n",
      "Epoch [16/1000], Loss: 0.7053\n",
      "Epoch [16/1000], Loss: 0.7801\n",
      "Epoch [16/1000], Loss: 1.0163\n",
      "Epoch [16/1000], Loss: 0.7941\n",
      "tensor(2.4855, grad_fn=<MeanBackward0>)\n",
      "16\n",
      "Epoch [17/1000], Loss: 0.9282\n",
      "Epoch [17/1000], Loss: 0.9451\n",
      "Epoch [17/1000], Loss: 0.8894\n",
      "Epoch [17/1000], Loss: 0.7117\n",
      "Epoch [17/1000], Loss: 0.8729\n",
      "Epoch [17/1000], Loss: 0.8634\n",
      "Epoch [17/1000], Loss: 0.9476\n",
      "Epoch [17/1000], Loss: 0.7053\n",
      "Epoch [17/1000], Loss: 0.7634\n",
      "Epoch [17/1000], Loss: 1.0345\n",
      "Epoch [17/1000], Loss: 0.7422\n",
      "tensor(2.5673, grad_fn=<MeanBackward0>)\n",
      "17\n",
      "Epoch [18/1000], Loss: 0.9135\n",
      "Epoch [18/1000], Loss: 0.9465\n",
      "Epoch [18/1000], Loss: 0.8852\n",
      "Epoch [18/1000], Loss: 0.7073\n",
      "Epoch [18/1000], Loss: 0.8662\n",
      "Epoch [18/1000], Loss: 0.8455\n",
      "Epoch [18/1000], Loss: 0.9418\n",
      "Epoch [18/1000], Loss: 0.6873\n",
      "Epoch [18/1000], Loss: 0.7517\n",
      "Epoch [18/1000], Loss: 0.9949\n",
      "Epoch [18/1000], Loss: 0.7782\n",
      "tensor(2.5175, grad_fn=<MeanBackward0>)\n",
      "18\n",
      "Epoch [19/1000], Loss: 0.9184\n",
      "Epoch [19/1000], Loss: 0.9597\n",
      "Epoch [19/1000], Loss: 0.8916\n",
      "Epoch [19/1000], Loss: 0.7071\n",
      "Epoch [19/1000], Loss: 0.8659\n",
      "Epoch [19/1000], Loss: 0.8530\n",
      "Epoch [19/1000], Loss: 0.9353\n",
      "Epoch [19/1000], Loss: 0.6814\n",
      "Epoch [19/1000], Loss: 0.7444\n",
      "Epoch [19/1000], Loss: 0.9920\n",
      "Epoch [19/1000], Loss: 0.7919\n",
      "tensor(2.4832, grad_fn=<MeanBackward0>)\n",
      "19\n",
      "Epoch [20/1000], Loss: 0.9180\n",
      "Epoch [20/1000], Loss: 0.9481\n",
      "Epoch [20/1000], Loss: 0.8806\n",
      "Epoch [20/1000], Loss: 0.6991\n",
      "Epoch [20/1000], Loss: 0.8573\n",
      "Epoch [20/1000], Loss: 0.8457\n",
      "Epoch [20/1000], Loss: 0.9164\n",
      "Epoch [20/1000], Loss: 0.6716\n",
      "Epoch [20/1000], Loss: 0.7512\n",
      "Epoch [20/1000], Loss: 1.0122\n",
      "Epoch [20/1000], Loss: 0.7830\n",
      "tensor(2.4402, grad_fn=<MeanBackward0>)\n",
      "20\n",
      "Epoch [21/1000], Loss: 0.9196\n",
      "Epoch [21/1000], Loss: 0.9308\n",
      "Epoch [21/1000], Loss: 0.8650\n",
      "Epoch [21/1000], Loss: 0.6903\n",
      "Epoch [21/1000], Loss: 0.8445\n",
      "Epoch [21/1000], Loss: 0.8225\n",
      "Epoch [21/1000], Loss: 0.9012\n",
      "Epoch [21/1000], Loss: 0.6740\n",
      "Epoch [21/1000], Loss: 0.7336\n",
      "Epoch [21/1000], Loss: 1.0247\n",
      "Epoch [21/1000], Loss: 0.7599\n",
      "tensor(2.4218, grad_fn=<MeanBackward0>)\n",
      "21\n",
      "Epoch [22/1000], Loss: 0.9229\n",
      "Epoch [22/1000], Loss: 0.9288\n",
      "Epoch [22/1000], Loss: 0.8636\n",
      "Epoch [22/1000], Loss: 0.6926\n",
      "Epoch [22/1000], Loss: 0.8534\n",
      "Epoch [22/1000], Loss: 0.7794\n",
      "Epoch [22/1000], Loss: 0.9103\n",
      "Epoch [22/1000], Loss: 0.6875\n",
      "Epoch [22/1000], Loss: 0.7164\n",
      "Epoch [22/1000], Loss: 1.0093\n",
      "Epoch [22/1000], Loss: 0.7453\n",
      "tensor(2.4717, grad_fn=<MeanBackward0>)\n",
      "22\n",
      "Epoch [23/1000], Loss: 0.9218\n",
      "Epoch [23/1000], Loss: 0.9328\n",
      "Epoch [23/1000], Loss: 0.8752\n",
      "Epoch [23/1000], Loss: 0.6894\n",
      "Epoch [23/1000], Loss: 0.8333\n",
      "Epoch [23/1000], Loss: 0.7713\n",
      "Epoch [23/1000], Loss: 0.9028\n",
      "Epoch [23/1000], Loss: 0.6955\n",
      "Epoch [23/1000], Loss: 0.7215\n",
      "Epoch [23/1000], Loss: 1.0544\n",
      "Epoch [23/1000], Loss: 0.7262\n",
      "tensor(2.4947, grad_fn=<MeanBackward0>)\n",
      "23\n",
      "Epoch [24/1000], Loss: 0.9005\n",
      "Epoch [24/1000], Loss: 0.9198\n",
      "Epoch [24/1000], Loss: 0.8438\n",
      "Epoch [24/1000], Loss: 0.6778\n",
      "Epoch [24/1000], Loss: 0.8128\n",
      "Epoch [24/1000], Loss: 0.7932\n",
      "Epoch [24/1000], Loss: 0.8901\n",
      "Epoch [24/1000], Loss: 0.6489\n",
      "Epoch [24/1000], Loss: 0.7196\n",
      "Epoch [24/1000], Loss: 0.9799\n",
      "Epoch [24/1000], Loss: 0.7947\n",
      "tensor(2.3463, grad_fn=<MeanBackward0>)\n",
      "24\n",
      "Epoch [25/1000], Loss: 0.9221\n",
      "Epoch [25/1000], Loss: 0.9158\n",
      "Epoch [25/1000], Loss: 0.8442\n",
      "Epoch [25/1000], Loss: 0.6942\n",
      "Epoch [25/1000], Loss: 0.8276\n",
      "Epoch [25/1000], Loss: 0.8084\n",
      "Epoch [25/1000], Loss: 0.8975\n",
      "Epoch [25/1000], Loss: 0.6503\n",
      "Epoch [25/1000], Loss: 0.7083\n",
      "Epoch [25/1000], Loss: 1.0027\n",
      "Epoch [25/1000], Loss: 0.7656\n",
      "tensor(2.4049, grad_fn=<MeanBackward0>)\n",
      "25\n",
      "Epoch [26/1000], Loss: 0.9041\n",
      "Epoch [26/1000], Loss: 0.9125\n",
      "Epoch [26/1000], Loss: 0.8334\n",
      "Epoch [26/1000], Loss: 0.6647\n",
      "Epoch [26/1000], Loss: 0.8067\n",
      "Epoch [26/1000], Loss: 0.7793\n",
      "Epoch [26/1000], Loss: 0.8444\n",
      "Epoch [26/1000], Loss: 0.6633\n",
      "Epoch [26/1000], Loss: 0.6927\n",
      "Epoch [26/1000], Loss: 1.0041\n",
      "Epoch [26/1000], Loss: 0.7266\n",
      "tensor(2.4838, grad_fn=<MeanBackward0>)\n",
      "26\n",
      "Epoch [27/1000], Loss: 0.8876\n",
      "Epoch [27/1000], Loss: 0.9004\n",
      "Epoch [27/1000], Loss: 0.8346\n",
      "Epoch [27/1000], Loss: 0.6907\n",
      "Epoch [27/1000], Loss: 0.8116\n",
      "Epoch [27/1000], Loss: 0.7525\n",
      "Epoch [27/1000], Loss: 0.8672\n",
      "Epoch [27/1000], Loss: 0.6953\n",
      "Epoch [27/1000], Loss: 0.6865\n",
      "Epoch [27/1000], Loss: 1.0332\n",
      "Epoch [27/1000], Loss: 0.7556\n",
      "tensor(2.2990, grad_fn=<MeanBackward0>)\n",
      "27\n",
      "Epoch [28/1000], Loss: 0.9194\n",
      "Epoch [28/1000], Loss: 0.9114\n",
      "Epoch [28/1000], Loss: 0.8459\n",
      "Epoch [28/1000], Loss: 0.6658\n",
      "Epoch [28/1000], Loss: 0.7964\n",
      "Epoch [28/1000], Loss: 0.8256\n",
      "Epoch [28/1000], Loss: 0.8636\n",
      "Epoch [28/1000], Loss: 0.6303\n",
      "Epoch [28/1000], Loss: 0.6871\n",
      "Epoch [28/1000], Loss: 0.9904\n",
      "Epoch [28/1000], Loss: 0.7472\n",
      "tensor(2.4024, grad_fn=<MeanBackward0>)\n",
      "28\n",
      "Epoch [29/1000], Loss: 0.8830\n",
      "Epoch [29/1000], Loss: 0.9018\n",
      "Epoch [29/1000], Loss: 0.8198\n",
      "Epoch [29/1000], Loss: 0.6692\n",
      "Epoch [29/1000], Loss: 0.7885\n",
      "Epoch [29/1000], Loss: 0.7370\n",
      "Epoch [29/1000], Loss: 0.8563\n",
      "Epoch [29/1000], Loss: 0.6421\n",
      "Epoch [29/1000], Loss: 0.6708\n",
      "Epoch [29/1000], Loss: 0.9943\n",
      "Epoch [29/1000], Loss: 0.6987\n",
      "tensor(2.4290, grad_fn=<MeanBackward0>)\n",
      "29\n",
      "Epoch [30/1000], Loss: 0.8849\n",
      "Epoch [30/1000], Loss: 0.9089\n",
      "Epoch [30/1000], Loss: 0.8327\n",
      "Epoch [30/1000], Loss: 0.6549\n",
      "Epoch [30/1000], Loss: 0.7988\n",
      "Epoch [30/1000], Loss: 0.7335\n",
      "Epoch [30/1000], Loss: 0.8356\n",
      "Epoch [30/1000], Loss: 0.6474\n",
      "Epoch [30/1000], Loss: 0.6686\n",
      "Epoch [30/1000], Loss: 1.0052\n",
      "Epoch [30/1000], Loss: 0.7215\n",
      "tensor(2.3763, grad_fn=<MeanBackward0>)\n",
      "30\n",
      "Epoch [31/1000], Loss: 0.8764\n",
      "Epoch [31/1000], Loss: 0.8785\n",
      "Epoch [31/1000], Loss: 0.8077\n",
      "Epoch [31/1000], Loss: 0.6537\n",
      "Epoch [31/1000], Loss: 0.7767\n",
      "Epoch [31/1000], Loss: 0.7516\n",
      "Epoch [31/1000], Loss: 0.8501\n",
      "Epoch [31/1000], Loss: 0.6111\n",
      "Epoch [31/1000], Loss: 0.6569\n",
      "Epoch [31/1000], Loss: 0.9866\n",
      "Epoch [31/1000], Loss: 0.6938\n",
      "tensor(2.4304, grad_fn=<MeanBackward0>)\n",
      "31\n",
      "Epoch [32/1000], Loss: 0.8647\n",
      "Epoch [32/1000], Loss: 0.8768\n",
      "Epoch [32/1000], Loss: 0.7937\n",
      "Epoch [32/1000], Loss: 0.6433\n",
      "Epoch [32/1000], Loss: 0.7681\n",
      "Epoch [32/1000], Loss: 0.7223\n",
      "Epoch [32/1000], Loss: 0.8128\n",
      "Epoch [32/1000], Loss: 0.6255\n",
      "Epoch [32/1000], Loss: 0.6627\n",
      "Epoch [32/1000], Loss: 0.9291\n",
      "Epoch [32/1000], Loss: 0.7408\n",
      "tensor(2.4454, grad_fn=<MeanBackward0>)\n",
      "32\n",
      "Epoch [33/1000], Loss: 0.8454\n",
      "Epoch [33/1000], Loss: 0.8574\n",
      "Epoch [33/1000], Loss: 0.7584\n",
      "Epoch [33/1000], Loss: 0.6807\n",
      "Epoch [33/1000], Loss: 0.7634\n",
      "Epoch [33/1000], Loss: 0.7176\n",
      "Epoch [33/1000], Loss: 0.8344\n",
      "Epoch [33/1000], Loss: 0.6537\n",
      "Epoch [33/1000], Loss: 0.6285\n",
      "Epoch [33/1000], Loss: 1.0162\n",
      "Epoch [33/1000], Loss: 0.7182\n",
      "tensor(2.3402, grad_fn=<MeanBackward0>)\n",
      "33\n",
      "Epoch [34/1000], Loss: 0.8604\n",
      "Epoch [34/1000], Loss: 0.8528\n",
      "Epoch [34/1000], Loss: 0.7741\n",
      "Epoch [34/1000], Loss: 0.6253\n",
      "Epoch [34/1000], Loss: 0.7333\n",
      "Epoch [34/1000], Loss: 0.6995\n",
      "Epoch [34/1000], Loss: 0.8072\n",
      "Epoch [34/1000], Loss: 0.6452\n",
      "Epoch [34/1000], Loss: 0.6138\n",
      "Epoch [34/1000], Loss: 1.0217\n",
      "Epoch [34/1000], Loss: 0.7076\n",
      "tensor(2.3237, grad_fn=<MeanBackward0>)\n",
      "34\n",
      "Epoch [35/1000], Loss: 0.8554\n",
      "Epoch [35/1000], Loss: 0.8274\n",
      "Epoch [35/1000], Loss: 0.7945\n",
      "Epoch [35/1000], Loss: 0.6479\n",
      "Epoch [35/1000], Loss: 0.7740\n",
      "Epoch [35/1000], Loss: 0.7571\n",
      "Epoch [35/1000], Loss: 0.9195\n",
      "Epoch [35/1000], Loss: 0.6051\n",
      "Epoch [35/1000], Loss: 0.6664\n",
      "Epoch [35/1000], Loss: 0.8803\n",
      "Epoch [35/1000], Loss: 0.6390\n",
      "tensor(2.5820, grad_fn=<MeanBackward0>)\n",
      "35\n",
      "Epoch [36/1000], Loss: 0.8381\n",
      "Epoch [36/1000], Loss: 0.8104\n",
      "Epoch [36/1000], Loss: 0.7685\n",
      "Epoch [36/1000], Loss: 0.6200\n",
      "Epoch [36/1000], Loss: 0.7311\n",
      "Epoch [36/1000], Loss: 0.7214\n",
      "Epoch [36/1000], Loss: 0.7872\n",
      "Epoch [36/1000], Loss: 0.5748\n",
      "Epoch [36/1000], Loss: 0.5988\n",
      "Epoch [36/1000], Loss: 0.8800\n",
      "Epoch [36/1000], Loss: 0.6077\n",
      "tensor(2.4876, grad_fn=<MeanBackward0>)\n",
      "36\n",
      "Epoch [37/1000], Loss: 0.8304\n",
      "Epoch [37/1000], Loss: 0.8183\n",
      "Epoch [37/1000], Loss: 0.7500\n",
      "Epoch [37/1000], Loss: 0.6224\n",
      "Epoch [37/1000], Loss: 0.7225\n",
      "Epoch [37/1000], Loss: 0.6946\n",
      "Epoch [37/1000], Loss: 0.8242\n",
      "Epoch [37/1000], Loss: 0.5987\n",
      "Epoch [37/1000], Loss: 0.6153\n",
      "Epoch [37/1000], Loss: 0.9020\n",
      "Epoch [37/1000], Loss: 0.6569\n",
      "tensor(2.4756, grad_fn=<MeanBackward0>)\n",
      "37\n",
      "Epoch [38/1000], Loss: 0.8392\n",
      "Epoch [38/1000], Loss: 0.8233\n",
      "Epoch [38/1000], Loss: 0.7908\n",
      "Epoch [38/1000], Loss: 0.6156\n",
      "Epoch [38/1000], Loss: 0.7172\n",
      "Epoch [38/1000], Loss: 0.6758\n",
      "Epoch [38/1000], Loss: 0.8094\n",
      "Epoch [38/1000], Loss: 0.6247\n",
      "Epoch [38/1000], Loss: 0.5850\n",
      "Epoch [38/1000], Loss: 0.9164\n",
      "Epoch [38/1000], Loss: 0.6459\n",
      "tensor(2.4008, grad_fn=<MeanBackward0>)\n",
      "38\n",
      "Epoch [39/1000], Loss: 0.7976\n",
      "Epoch [39/1000], Loss: 0.7919\n",
      "Epoch [39/1000], Loss: 0.7452\n",
      "Epoch [39/1000], Loss: 0.6320\n",
      "Epoch [39/1000], Loss: 0.6998\n",
      "Epoch [39/1000], Loss: 0.6801\n",
      "Epoch [39/1000], Loss: 0.7778\n",
      "Epoch [39/1000], Loss: 0.7256\n",
      "Epoch [39/1000], Loss: 0.6625\n",
      "Epoch [39/1000], Loss: 1.0153\n",
      "Epoch [39/1000], Loss: 0.6225\n",
      "tensor(2.2823, grad_fn=<MeanBackward0>)\n",
      "39\n",
      "Epoch [40/1000], Loss: 0.8274\n",
      "Epoch [40/1000], Loss: 0.8982\n",
      "Epoch [40/1000], Loss: 0.7066\n",
      "Epoch [40/1000], Loss: 0.6822\n",
      "Epoch [40/1000], Loss: 0.6805\n",
      "Epoch [40/1000], Loss: 0.7584\n",
      "Epoch [40/1000], Loss: 0.7979\n",
      "Epoch [40/1000], Loss: 0.5637\n",
      "Epoch [40/1000], Loss: 0.5985\n",
      "Epoch [40/1000], Loss: 0.8069\n",
      "Epoch [40/1000], Loss: 0.6530\n",
      "tensor(2.5606, grad_fn=<MeanBackward0>)\n",
      "40\n",
      "Epoch [41/1000], Loss: 0.7622\n",
      "Epoch [41/1000], Loss: 0.7606\n",
      "Epoch [41/1000], Loss: 0.6685\n",
      "Epoch [41/1000], Loss: 0.6469\n",
      "Epoch [41/1000], Loss: 0.6707\n",
      "Epoch [41/1000], Loss: 0.6765\n",
      "Epoch [41/1000], Loss: 0.7689\n",
      "Epoch [41/1000], Loss: 0.6561\n",
      "Epoch [41/1000], Loss: 0.5761\n",
      "Epoch [41/1000], Loss: 0.9377\n",
      "Epoch [41/1000], Loss: 0.5667\n",
      "tensor(2.2881, grad_fn=<MeanBackward0>)\n",
      "41\n",
      "Epoch [42/1000], Loss: 0.7981\n",
      "Epoch [42/1000], Loss: 0.8313\n",
      "Epoch [42/1000], Loss: 0.7043\n",
      "Epoch [42/1000], Loss: 0.6184\n",
      "Epoch [42/1000], Loss: 0.6323\n",
      "Epoch [42/1000], Loss: 0.6908\n",
      "Epoch [42/1000], Loss: 0.7611\n",
      "Epoch [42/1000], Loss: 0.5513\n",
      "Epoch [42/1000], Loss: 0.5427\n",
      "Epoch [42/1000], Loss: 0.8303\n",
      "Epoch [42/1000], Loss: 0.5561\n",
      "tensor(2.3925, grad_fn=<MeanBackward0>)\n",
      "42\n",
      "Epoch [43/1000], Loss: 0.7586\n",
      "Epoch [43/1000], Loss: 0.7674\n",
      "Epoch [43/1000], Loss: 0.7320\n",
      "Epoch [43/1000], Loss: 0.5899\n",
      "Epoch [43/1000], Loss: 0.6746\n",
      "Epoch [43/1000], Loss: 0.6529\n",
      "Epoch [43/1000], Loss: 0.7958\n",
      "Epoch [43/1000], Loss: 0.6149\n",
      "Epoch [43/1000], Loss: 0.5826\n",
      "Epoch [43/1000], Loss: 0.8830\n",
      "Epoch [43/1000], Loss: 0.5762\n",
      "tensor(2.3398, grad_fn=<MeanBackward0>)\n",
      "43\n",
      "Epoch [44/1000], Loss: 0.7571\n",
      "Epoch [44/1000], Loss: 0.8103\n",
      "Epoch [44/1000], Loss: 0.6870\n",
      "Epoch [44/1000], Loss: 0.6255\n",
      "Epoch [44/1000], Loss: 0.6225\n",
      "Epoch [44/1000], Loss: 0.6724\n",
      "Epoch [44/1000], Loss: 0.7504\n",
      "Epoch [44/1000], Loss: 0.5280\n",
      "Epoch [44/1000], Loss: 0.5219\n",
      "Epoch [44/1000], Loss: 0.7738\n",
      "Epoch [44/1000], Loss: 0.5743\n",
      "tensor(2.5903, grad_fn=<MeanBackward0>)\n",
      "44\n",
      "Epoch [45/1000], Loss: 0.7264\n",
      "Epoch [45/1000], Loss: 0.6857\n",
      "Epoch [45/1000], Loss: 0.6694\n",
      "Epoch [45/1000], Loss: 0.5816\n",
      "Epoch [45/1000], Loss: 0.6589\n",
      "Epoch [45/1000], Loss: 0.6281\n",
      "Epoch [45/1000], Loss: 0.7569\n",
      "Epoch [45/1000], Loss: 0.5308\n",
      "Epoch [45/1000], Loss: 0.5579\n",
      "Epoch [45/1000], Loss: 0.7694\n",
      "Epoch [45/1000], Loss: 0.5493\n",
      "tensor(2.6364, grad_fn=<MeanBackward0>)\n",
      "45\n",
      "Epoch [46/1000], Loss: 0.7464\n",
      "Epoch [46/1000], Loss: 0.6887\n",
      "Epoch [46/1000], Loss: 0.6292\n",
      "Epoch [46/1000], Loss: 0.5630\n",
      "Epoch [46/1000], Loss: 0.6535\n",
      "Epoch [46/1000], Loss: 0.6197\n",
      "Epoch [46/1000], Loss: 0.7739\n",
      "Epoch [46/1000], Loss: 0.5600\n",
      "Epoch [46/1000], Loss: 0.5249\n",
      "Epoch [46/1000], Loss: 0.7373\n",
      "Epoch [46/1000], Loss: 0.5768\n",
      "tensor(2.5081, grad_fn=<MeanBackward0>)\n",
      "46\n",
      "Epoch [47/1000], Loss: 0.6948\n",
      "Epoch [47/1000], Loss: 0.6677\n",
      "Epoch [47/1000], Loss: 0.6074\n",
      "Epoch [47/1000], Loss: 0.5670\n",
      "Epoch [47/1000], Loss: 0.6120\n",
      "Epoch [47/1000], Loss: 0.6260\n",
      "Epoch [47/1000], Loss: 0.7130\n",
      "Epoch [47/1000], Loss: 0.5473\n",
      "Epoch [47/1000], Loss: 0.5059\n",
      "Epoch [47/1000], Loss: 0.7768\n",
      "Epoch [47/1000], Loss: 0.5981\n",
      "tensor(2.4878, grad_fn=<MeanBackward0>)\n",
      "47\n",
      "Epoch [48/1000], Loss: 0.6858\n",
      "Epoch [48/1000], Loss: 0.6570\n",
      "Epoch [48/1000], Loss: 0.6393\n",
      "Epoch [48/1000], Loss: 0.5536\n",
      "Epoch [48/1000], Loss: 0.6455\n",
      "Epoch [48/1000], Loss: 0.6032\n",
      "Epoch [48/1000], Loss: 0.7360\n",
      "Epoch [48/1000], Loss: 0.5334\n",
      "Epoch [48/1000], Loss: 0.5021\n",
      "Epoch [48/1000], Loss: 0.7346\n",
      "Epoch [48/1000], Loss: 0.5712\n",
      "tensor(2.4798, grad_fn=<MeanBackward0>)\n",
      "48\n",
      "Epoch [49/1000], Loss: 0.6719\n",
      "Epoch [49/1000], Loss: 0.6559\n",
      "Epoch [49/1000], Loss: 0.6250\n",
      "Epoch [49/1000], Loss: 0.5553\n",
      "Epoch [49/1000], Loss: 0.6957\n",
      "Epoch [49/1000], Loss: 0.6230\n",
      "Epoch [49/1000], Loss: 0.7933\n",
      "Epoch [49/1000], Loss: 0.7401\n",
      "Epoch [49/1000], Loss: 0.4693\n",
      "Epoch [49/1000], Loss: 1.1485\n",
      "Epoch [49/1000], Loss: 0.6808\n",
      "tensor(2.0790, grad_fn=<MeanBackward0>)\n",
      "49\n",
      "Epoch [50/1000], Loss: 0.8475\n",
      "Epoch [50/1000], Loss: 1.0579\n",
      "Epoch [50/1000], Loss: 1.0003\n",
      "Epoch [50/1000], Loss: 0.5720\n",
      "Epoch [50/1000], Loss: 1.0004\n",
      "Epoch [50/1000], Loss: 0.9602\n",
      "Epoch [50/1000], Loss: 0.7558\n",
      "Epoch [50/1000], Loss: 0.9316\n",
      "Epoch [50/1000], Loss: 1.1290\n",
      "Epoch [50/1000], Loss: 0.9607\n",
      "Epoch [50/1000], Loss: 0.5190\n",
      "tensor(2.9459, grad_fn=<MeanBackward0>)\n",
      "50\n",
      "Epoch [51/1000], Loss: 1.1063\n",
      "Epoch [51/1000], Loss: 1.1594\n",
      "Epoch [51/1000], Loss: 0.7214\n",
      "Epoch [51/1000], Loss: 0.7588\n",
      "Epoch [51/1000], Loss: 0.9163\n",
      "Epoch [51/1000], Loss: 0.8315\n",
      "Epoch [51/1000], Loss: 0.7829\n",
      "Epoch [51/1000], Loss: 0.6506\n",
      "Epoch [51/1000], Loss: 0.5742\n",
      "Epoch [51/1000], Loss: 0.7723\n",
      "Epoch [51/1000], Loss: 0.5280\n",
      "tensor(2.5378, grad_fn=<MeanBackward0>)\n",
      "51\n",
      "Epoch [52/1000], Loss: 0.7157\n",
      "Epoch [52/1000], Loss: 0.7104\n",
      "Epoch [52/1000], Loss: 0.6971\n",
      "Epoch [52/1000], Loss: 0.5930\n",
      "Epoch [52/1000], Loss: 0.6308\n",
      "Epoch [52/1000], Loss: 0.6738\n",
      "Epoch [52/1000], Loss: 0.8006\n",
      "Epoch [52/1000], Loss: 0.5927\n",
      "Epoch [52/1000], Loss: 0.6184\n",
      "Epoch [52/1000], Loss: 0.7300\n",
      "Epoch [52/1000], Loss: 0.5867\n",
      "tensor(2.4761, grad_fn=<MeanBackward0>)\n",
      "52\n",
      "Epoch [53/1000], Loss: 0.6596\n",
      "Epoch [53/1000], Loss: 0.7028\n",
      "Epoch [53/1000], Loss: 0.6603\n",
      "Epoch [53/1000], Loss: 0.5261\n",
      "Epoch [53/1000], Loss: 0.6122\n",
      "Epoch [53/1000], Loss: 0.6091\n",
      "Epoch [53/1000], Loss: 0.6954\n",
      "Epoch [53/1000], Loss: 0.6188\n",
      "Epoch [53/1000], Loss: 0.5458\n",
      "Epoch [53/1000], Loss: 0.8146\n",
      "Epoch [53/1000], Loss: 0.6482\n",
      "tensor(2.4143, grad_fn=<MeanBackward0>)\n",
      "53\n",
      "Epoch [54/1000], Loss: 0.6698\n",
      "Epoch [54/1000], Loss: 0.7360\n",
      "Epoch [54/1000], Loss: 0.6587\n",
      "Epoch [54/1000], Loss: 0.5596\n",
      "Epoch [54/1000], Loss: 0.6227\n",
      "Epoch [54/1000], Loss: 0.6037\n",
      "Epoch [54/1000], Loss: 0.6647\n",
      "Epoch [54/1000], Loss: 0.5105\n",
      "Epoch [54/1000], Loss: 0.4696\n",
      "Epoch [54/1000], Loss: 0.7305\n",
      "Epoch [54/1000], Loss: 0.5224\n",
      "tensor(2.3389, grad_fn=<MeanBackward0>)\n",
      "54\n",
      "Epoch [55/1000], Loss: 0.6702\n",
      "Epoch [55/1000], Loss: 0.7007\n",
      "Epoch [55/1000], Loss: 0.5935\n",
      "Epoch [55/1000], Loss: 0.5655\n",
      "Epoch [55/1000], Loss: 0.5545\n",
      "Epoch [55/1000], Loss: 0.6047\n",
      "Epoch [55/1000], Loss: 0.6538\n",
      "Epoch [55/1000], Loss: 0.5039\n",
      "Epoch [55/1000], Loss: 0.5223\n",
      "Epoch [55/1000], Loss: 0.6820\n",
      "Epoch [55/1000], Loss: 0.5852\n",
      "tensor(2.6015, grad_fn=<MeanBackward0>)\n",
      "55\n",
      "Epoch [56/1000], Loss: 0.6453\n",
      "Epoch [56/1000], Loss: 0.6791\n",
      "Epoch [56/1000], Loss: 0.7313\n",
      "Epoch [56/1000], Loss: 0.5627\n",
      "Epoch [56/1000], Loss: 0.7483\n",
      "Epoch [56/1000], Loss: 0.7756\n",
      "Epoch [56/1000], Loss: 0.6721\n",
      "Epoch [56/1000], Loss: 0.7203\n",
      "Epoch [56/1000], Loss: 0.6116\n",
      "Epoch [56/1000], Loss: 0.6653\n",
      "Epoch [56/1000], Loss: 0.8180\n",
      "tensor(2.7383, grad_fn=<MeanBackward0>)\n",
      "56\n",
      "Epoch [57/1000], Loss: 0.8842\n",
      "Epoch [57/1000], Loss: 0.6541\n",
      "Epoch [57/1000], Loss: 0.8988\n",
      "Epoch [57/1000], Loss: 0.8657\n",
      "Epoch [57/1000], Loss: 0.6906\n",
      "Epoch [57/1000], Loss: 0.6612\n",
      "Epoch [57/1000], Loss: 1.1099\n",
      "Epoch [57/1000], Loss: 0.7539\n",
      "Epoch [57/1000], Loss: 0.5829\n",
      "Epoch [57/1000], Loss: 0.8654\n",
      "Epoch [57/1000], Loss: 0.8167\n",
      "tensor(2.2960, grad_fn=<MeanBackward0>)\n",
      "57\n",
      "Epoch [58/1000], Loss: 0.7152\n",
      "Epoch [58/1000], Loss: 0.6871\n",
      "Epoch [58/1000], Loss: 0.7259\n",
      "Epoch [58/1000], Loss: 0.5219\n",
      "Epoch [58/1000], Loss: 0.6543\n",
      "Epoch [58/1000], Loss: 0.6764\n",
      "Epoch [58/1000], Loss: 0.6963\n",
      "Epoch [58/1000], Loss: 0.5772\n",
      "Epoch [58/1000], Loss: 0.4636\n",
      "Epoch [58/1000], Loss: 0.6845\n",
      "Epoch [58/1000], Loss: 0.4945\n",
      "tensor(2.5930, grad_fn=<MeanBackward0>)\n",
      "58\n",
      "Epoch [59/1000], Loss: 0.6420\n",
      "Epoch [59/1000], Loss: 0.6444\n",
      "Epoch [59/1000], Loss: 0.5853\n",
      "Epoch [59/1000], Loss: 0.6081\n",
      "Epoch [59/1000], Loss: 0.6004\n",
      "Epoch [59/1000], Loss: 0.5884\n",
      "Epoch [59/1000], Loss: 0.7946\n",
      "Epoch [59/1000], Loss: 0.4904\n",
      "Epoch [59/1000], Loss: 0.5626\n",
      "Epoch [59/1000], Loss: 0.6995\n",
      "Epoch [59/1000], Loss: 0.4498\n",
      "tensor(2.6876, grad_fn=<MeanBackward0>)\n",
      "59\n",
      "Epoch [60/1000], Loss: 0.7480\n",
      "Epoch [60/1000], Loss: 0.5827\n",
      "Epoch [60/1000], Loss: 0.6916\n",
      "Epoch [60/1000], Loss: 0.6915\n",
      "Epoch [60/1000], Loss: 0.5913\n",
      "Epoch [60/1000], Loss: 0.6482\n",
      "Epoch [60/1000], Loss: 0.9004\n",
      "Epoch [60/1000], Loss: 0.5180\n",
      "Epoch [60/1000], Loss: 0.6273\n",
      "Epoch [60/1000], Loss: 0.8317\n",
      "Epoch [60/1000], Loss: 0.6125\n",
      "tensor(2.6192, grad_fn=<MeanBackward0>)\n",
      "60\n",
      "Epoch [61/1000], Loss: 0.6362\n",
      "Epoch [61/1000], Loss: 0.8364\n",
      "Epoch [61/1000], Loss: 0.6045\n",
      "Epoch [61/1000], Loss: 0.5797\n",
      "Epoch [61/1000], Loss: 0.7389\n",
      "Epoch [61/1000], Loss: 0.7119\n",
      "Epoch [61/1000], Loss: 0.6630\n",
      "Epoch [61/1000], Loss: 0.5875\n",
      "Epoch [61/1000], Loss: 0.4390\n",
      "Epoch [61/1000], Loss: 0.6881\n",
      "Epoch [61/1000], Loss: 0.4980\n",
      "tensor(2.5280, grad_fn=<MeanBackward0>)\n",
      "61\n",
      "Epoch [62/1000], Loss: 0.6117\n",
      "Epoch [62/1000], Loss: 0.6521\n",
      "Epoch [62/1000], Loss: 0.5408\n",
      "Epoch [62/1000], Loss: 0.5325\n",
      "Epoch [62/1000], Loss: 0.5966\n",
      "Epoch [62/1000], Loss: 0.5514\n",
      "Epoch [62/1000], Loss: 0.7813\n",
      "Epoch [62/1000], Loss: 0.5458\n",
      "Epoch [62/1000], Loss: 0.4948\n",
      "Epoch [62/1000], Loss: 0.7197\n",
      "Epoch [62/1000], Loss: 0.4391\n",
      "tensor(2.6525, grad_fn=<MeanBackward0>)\n",
      "62\n",
      "Epoch [63/1000], Loss: 0.6920\n",
      "Epoch [63/1000], Loss: 0.7279\n",
      "Epoch [63/1000], Loss: 0.5593\n",
      "Epoch [63/1000], Loss: 0.6171\n",
      "Epoch [63/1000], Loss: 0.6279\n",
      "Epoch [63/1000], Loss: 0.5351\n",
      "Epoch [63/1000], Loss: 0.8243\n",
      "Epoch [63/1000], Loss: 0.6236\n",
      "Epoch [63/1000], Loss: 0.4414\n",
      "Epoch [63/1000], Loss: 0.7266\n",
      "Epoch [63/1000], Loss: 0.5433\n",
      "tensor(2.5300, grad_fn=<MeanBackward0>)\n",
      "63\n",
      "Epoch [64/1000], Loss: 0.5892\n",
      "Epoch [64/1000], Loss: 0.7688\n",
      "Epoch [64/1000], Loss: 0.6117\n",
      "Epoch [64/1000], Loss: 0.4738\n",
      "Epoch [64/1000], Loss: 0.6511\n",
      "Epoch [64/1000], Loss: 0.6421\n",
      "Epoch [64/1000], Loss: 0.6159\n",
      "Epoch [64/1000], Loss: 0.5974\n",
      "Epoch [64/1000], Loss: 0.4470\n",
      "Epoch [64/1000], Loss: 0.6947\n",
      "Epoch [64/1000], Loss: 0.6405\n",
      "tensor(2.2924, grad_fn=<MeanBackward0>)\n",
      "64\n",
      "Epoch [65/1000], Loss: 0.6572\n",
      "Epoch [65/1000], Loss: 0.5832\n",
      "Epoch [65/1000], Loss: 0.6704\n",
      "Epoch [65/1000], Loss: 0.5152\n",
      "Epoch [65/1000], Loss: 0.5529\n",
      "Epoch [65/1000], Loss: 0.7003\n",
      "Epoch [65/1000], Loss: 0.6876\n",
      "Epoch [65/1000], Loss: 0.4614\n",
      "Epoch [65/1000], Loss: 0.5352\n",
      "Epoch [65/1000], Loss: 0.6158\n",
      "Epoch [65/1000], Loss: 0.4216\n",
      "tensor(2.3678, grad_fn=<MeanBackward0>)\n",
      "65\n",
      "Epoch [66/1000], Loss: 0.5962\n",
      "Epoch [66/1000], Loss: 0.5689\n",
      "Epoch [66/1000], Loss: 0.5242\n",
      "Epoch [66/1000], Loss: 0.4530\n",
      "Epoch [66/1000], Loss: 0.5070\n",
      "Epoch [66/1000], Loss: 0.5797\n",
      "Epoch [66/1000], Loss: 0.5846\n",
      "Epoch [66/1000], Loss: 0.5445\n",
      "Epoch [66/1000], Loss: 0.3864\n",
      "Epoch [66/1000], Loss: 0.6467\n",
      "Epoch [66/1000], Loss: 0.4928\n",
      "tensor(2.4868, grad_fn=<MeanBackward0>)\n",
      "66\n",
      "Epoch [67/1000], Loss: 0.5402\n",
      "Epoch [67/1000], Loss: 0.6444\n",
      "Epoch [67/1000], Loss: 0.4940\n",
      "Epoch [67/1000], Loss: 0.4454\n",
      "Epoch [67/1000], Loss: 0.5531\n",
      "Epoch [67/1000], Loss: 0.4940\n",
      "Epoch [67/1000], Loss: 0.6166\n",
      "Epoch [67/1000], Loss: 0.4687\n",
      "Epoch [67/1000], Loss: 0.3823\n",
      "Epoch [67/1000], Loss: 0.6032\n",
      "Epoch [67/1000], Loss: 0.3764\n",
      "tensor(2.4947, grad_fn=<MeanBackward0>)\n",
      "67\n",
      "Epoch [68/1000], Loss: 0.5518\n",
      "Epoch [68/1000], Loss: 0.5486\n",
      "Epoch [68/1000], Loss: 0.4847\n",
      "Epoch [68/1000], Loss: 0.4430\n",
      "Epoch [68/1000], Loss: 0.4696\n",
      "Epoch [68/1000], Loss: 0.4896\n",
      "Epoch [68/1000], Loss: 0.5515\n",
      "Epoch [68/1000], Loss: 0.4435\n",
      "Epoch [68/1000], Loss: 0.3685\n",
      "Epoch [68/1000], Loss: 0.5845\n",
      "Epoch [68/1000], Loss: 0.3585\n",
      "tensor(2.4342, grad_fn=<MeanBackward0>)\n",
      "68\n",
      "Epoch [69/1000], Loss: 0.5101\n",
      "Epoch [69/1000], Loss: 0.5034\n",
      "Epoch [69/1000], Loss: 0.4476\n",
      "Epoch [69/1000], Loss: 0.3968\n",
      "Epoch [69/1000], Loss: 0.4513\n",
      "Epoch [69/1000], Loss: 0.4466\n",
      "Epoch [69/1000], Loss: 0.5292\n",
      "Epoch [69/1000], Loss: 0.4315\n",
      "Epoch [69/1000], Loss: 0.3351\n",
      "Epoch [69/1000], Loss: 0.5572\n",
      "Epoch [69/1000], Loss: 0.3202\n",
      "tensor(2.3444, grad_fn=<MeanBackward0>)\n",
      "69\n",
      "Epoch [70/1000], Loss: 0.5204\n",
      "Epoch [70/1000], Loss: 0.5050\n",
      "Epoch [70/1000], Loss: 0.4576\n",
      "Epoch [70/1000], Loss: 0.3905\n",
      "Epoch [70/1000], Loss: 0.4580\n",
      "Epoch [70/1000], Loss: 0.4617\n",
      "Epoch [70/1000], Loss: 0.5063\n",
      "Epoch [70/1000], Loss: 0.4429\n",
      "Epoch [70/1000], Loss: 0.3738\n",
      "Epoch [70/1000], Loss: 0.5547\n",
      "Epoch [70/1000], Loss: 0.3887\n",
      "tensor(2.3401, grad_fn=<MeanBackward0>)\n",
      "70\n",
      "Epoch [71/1000], Loss: 0.4950\n",
      "Epoch [71/1000], Loss: 0.5615\n",
      "Epoch [71/1000], Loss: 0.4937\n",
      "Epoch [71/1000], Loss: 0.4963\n",
      "Epoch [71/1000], Loss: 0.4772\n",
      "Epoch [71/1000], Loss: 0.5243\n",
      "Epoch [71/1000], Loss: 0.6133\n",
      "Epoch [71/1000], Loss: 0.4366\n",
      "Epoch [71/1000], Loss: 0.5389\n",
      "Epoch [71/1000], Loss: 0.5649\n",
      "Epoch [71/1000], Loss: 0.4967\n",
      "tensor(2.2510, grad_fn=<MeanBackward0>)\n",
      "71\n",
      "Epoch [72/1000], Loss: 0.5869\n",
      "Epoch [72/1000], Loss: 0.4720\n",
      "Epoch [72/1000], Loss: 0.5288\n",
      "Epoch [72/1000], Loss: 0.3527\n",
      "Epoch [72/1000], Loss: 0.4342\n",
      "Epoch [72/1000], Loss: 0.4378\n",
      "Epoch [72/1000], Loss: 0.4602\n",
      "Epoch [72/1000], Loss: 0.4104\n",
      "Epoch [72/1000], Loss: 0.3195\n",
      "Epoch [72/1000], Loss: 0.5399\n",
      "Epoch [72/1000], Loss: 0.3163\n",
      "tensor(2.4544, grad_fn=<MeanBackward0>)\n",
      "72\n",
      "Epoch [73/1000], Loss: 0.4627\n",
      "Epoch [73/1000], Loss: 0.4442\n",
      "Epoch [73/1000], Loss: 0.4489\n",
      "Epoch [73/1000], Loss: 0.3254\n",
      "Epoch [73/1000], Loss: 0.4092\n",
      "Epoch [73/1000], Loss: 0.4041\n",
      "Epoch [73/1000], Loss: 0.4752\n",
      "Epoch [73/1000], Loss: 0.3978\n",
      "Epoch [73/1000], Loss: 0.3564\n",
      "Epoch [73/1000], Loss: 0.5251\n",
      "Epoch [73/1000], Loss: 0.3042\n",
      "tensor(2.4977, grad_fn=<MeanBackward0>)\n",
      "73\n",
      "Epoch [74/1000], Loss: 0.5065\n",
      "Epoch [74/1000], Loss: 0.4362\n",
      "Epoch [74/1000], Loss: 0.5465\n",
      "Epoch [74/1000], Loss: 0.3745\n",
      "Epoch [74/1000], Loss: 0.5764\n",
      "Epoch [74/1000], Loss: 0.5652\n",
      "Epoch [74/1000], Loss: 0.5405\n",
      "Epoch [74/1000], Loss: 0.6252\n",
      "Epoch [74/1000], Loss: 0.3423\n",
      "Epoch [74/1000], Loss: 0.8263\n",
      "Epoch [74/1000], Loss: 0.6716\n",
      "tensor(2.3007, grad_fn=<MeanBackward0>)\n",
      "74\n",
      "Epoch [75/1000], Loss: 0.4621\n",
      "Epoch [75/1000], Loss: 0.6434\n",
      "Epoch [75/1000], Loss: 0.6667\n",
      "Epoch [75/1000], Loss: 0.3479\n",
      "Epoch [75/1000], Loss: 0.5905\n",
      "Epoch [75/1000], Loss: 0.5658\n",
      "Epoch [75/1000], Loss: 0.4253\n",
      "Epoch [75/1000], Loss: 0.6031\n",
      "Epoch [75/1000], Loss: 0.4865\n",
      "Epoch [75/1000], Loss: 0.5482\n",
      "Epoch [75/1000], Loss: 0.5770\n",
      "tensor(2.5814, grad_fn=<MeanBackward0>)\n",
      "75\n",
      "Epoch [76/1000], Loss: 0.5246\n",
      "Epoch [76/1000], Loss: 0.4944\n",
      "Epoch [76/1000], Loss: 0.6626\n",
      "Epoch [76/1000], Loss: 0.4852\n",
      "Epoch [76/1000], Loss: 0.4363\n",
      "Epoch [76/1000], Loss: 0.5840\n",
      "Epoch [76/1000], Loss: 0.5890\n",
      "Epoch [76/1000], Loss: 0.4738\n",
      "Epoch [76/1000], Loss: 0.5082\n",
      "Epoch [76/1000], Loss: 0.5520\n",
      "Epoch [76/1000], Loss: 0.3420\n",
      "tensor(2.5057, grad_fn=<MeanBackward0>)\n",
      "76\n",
      "Epoch [77/1000], Loss: 0.4818\n",
      "Epoch [77/1000], Loss: 0.4366\n",
      "Epoch [77/1000], Loss: 0.5322\n",
      "Epoch [77/1000], Loss: 0.3953\n",
      "Epoch [77/1000], Loss: 0.4719\n",
      "Epoch [77/1000], Loss: 0.4887\n",
      "Epoch [77/1000], Loss: 0.4087\n",
      "Epoch [77/1000], Loss: 0.5495\n",
      "Epoch [77/1000], Loss: 0.5038\n",
      "Epoch [77/1000], Loss: 0.4784\n",
      "Epoch [77/1000], Loss: 0.4752\n",
      "tensor(2.5837, grad_fn=<MeanBackward0>)\n",
      "77\n",
      "Epoch [78/1000], Loss: 0.5466\n",
      "Epoch [78/1000], Loss: 0.4588\n",
      "Epoch [78/1000], Loss: 0.6489\n",
      "Epoch [78/1000], Loss: 0.4602\n",
      "Epoch [78/1000], Loss: 0.3946\n",
      "Epoch [78/1000], Loss: 0.5714\n",
      "Epoch [78/1000], Loss: 0.5361\n",
      "Epoch [78/1000], Loss: 0.4585\n",
      "Epoch [78/1000], Loss: 0.5300\n",
      "Epoch [78/1000], Loss: 0.5477\n",
      "Epoch [78/1000], Loss: 0.3157\n",
      "tensor(2.6117, grad_fn=<MeanBackward0>)\n",
      "78\n",
      "Epoch [79/1000], Loss: 0.5688\n",
      "Epoch [79/1000], Loss: 0.4460\n",
      "Epoch [79/1000], Loss: 0.5711\n",
      "Epoch [79/1000], Loss: 0.6140\n",
      "Epoch [79/1000], Loss: 0.5742\n",
      "Epoch [79/1000], Loss: 0.3978\n",
      "Epoch [79/1000], Loss: 0.7368\n",
      "Epoch [79/1000], Loss: 0.5048\n",
      "Epoch [79/1000], Loss: 0.3202\n",
      "Epoch [79/1000], Loss: 0.5752\n",
      "Epoch [79/1000], Loss: 0.4052\n",
      "tensor(2.5010, grad_fn=<MeanBackward0>)\n",
      "79\n",
      "Epoch [80/1000], Loss: 0.4520\n",
      "Epoch [80/1000], Loss: 0.7539\n",
      "Epoch [80/1000], Loss: 0.5586\n",
      "Epoch [80/1000], Loss: 0.3304\n",
      "Epoch [80/1000], Loss: 0.5757\n",
      "Epoch [80/1000], Loss: 0.4881\n",
      "Epoch [80/1000], Loss: 0.3887\n",
      "Epoch [80/1000], Loss: 0.4577\n",
      "Epoch [80/1000], Loss: 0.3552\n",
      "Epoch [80/1000], Loss: 0.5317\n",
      "Epoch [80/1000], Loss: 0.5032\n",
      "tensor(2.2604, grad_fn=<MeanBackward0>)\n",
      "80\n",
      "Epoch [81/1000], Loss: 0.5191\n",
      "Epoch [81/1000], Loss: 0.4523\n",
      "Epoch [81/1000], Loss: 0.4525\n",
      "Epoch [81/1000], Loss: 0.2727\n",
      "Epoch [81/1000], Loss: 0.4373\n",
      "Epoch [81/1000], Loss: 0.3502\n",
      "Epoch [81/1000], Loss: 0.4069\n",
      "Epoch [81/1000], Loss: 0.3433\n",
      "Epoch [81/1000], Loss: 0.2782\n",
      "Epoch [81/1000], Loss: 0.4688\n",
      "Epoch [81/1000], Loss: 0.2499\n",
      "tensor(2.4843, grad_fn=<MeanBackward0>)\n",
      "81\n",
      "Epoch [82/1000], Loss: 0.4099\n",
      "Epoch [82/1000], Loss: 0.3760\n",
      "Epoch [82/1000], Loss: 0.4553\n",
      "Epoch [82/1000], Loss: 0.3063\n",
      "Epoch [82/1000], Loss: 0.4014\n",
      "Epoch [82/1000], Loss: 0.4428\n",
      "Epoch [82/1000], Loss: 0.3490\n",
      "Epoch [82/1000], Loss: 0.3481\n",
      "Epoch [82/1000], Loss: 0.2327\n",
      "Epoch [82/1000], Loss: 0.4855\n",
      "Epoch [82/1000], Loss: 0.2301\n",
      "tensor(2.3301, grad_fn=<MeanBackward0>)\n",
      "82\n",
      "Epoch [83/1000], Loss: 0.4175\n",
      "Epoch [83/1000], Loss: 0.3844\n",
      "Epoch [83/1000], Loss: 0.3779\n",
      "Epoch [83/1000], Loss: 0.2556\n",
      "Epoch [83/1000], Loss: 0.3579\n",
      "Epoch [83/1000], Loss: 0.3234\n",
      "Epoch [83/1000], Loss: 0.3362\n",
      "Epoch [83/1000], Loss: 0.3125\n",
      "Epoch [83/1000], Loss: 0.2233\n",
      "Epoch [83/1000], Loss: 0.4347\n",
      "Epoch [83/1000], Loss: 0.2047\n",
      "tensor(2.4943, grad_fn=<MeanBackward0>)\n",
      "83\n",
      "Epoch [84/1000], Loss: 0.3859\n",
      "Epoch [84/1000], Loss: 0.3537\n",
      "Epoch [84/1000], Loss: 0.4306\n",
      "Epoch [84/1000], Loss: 0.2727\n",
      "Epoch [84/1000], Loss: 0.3707\n",
      "Epoch [84/1000], Loss: 0.3652\n",
      "Epoch [84/1000], Loss: 0.3608\n",
      "Epoch [84/1000], Loss: 0.3163\n",
      "Epoch [84/1000], Loss: 0.2434\n",
      "Epoch [84/1000], Loss: 0.4110\n",
      "Epoch [84/1000], Loss: 0.2497\n",
      "tensor(2.4451, grad_fn=<MeanBackward0>)\n",
      "84\n",
      "Epoch [85/1000], Loss: 0.3472\n",
      "Epoch [85/1000], Loss: 0.4518\n",
      "Epoch [85/1000], Loss: 0.3690\n",
      "Epoch [85/1000], Loss: 0.2990\n",
      "Epoch [85/1000], Loss: 0.3089\n",
      "Epoch [85/1000], Loss: 0.3397\n",
      "Epoch [85/1000], Loss: 0.3012\n",
      "Epoch [85/1000], Loss: 0.3272\n",
      "Epoch [85/1000], Loss: 0.2055\n",
      "Epoch [85/1000], Loss: 0.5539\n",
      "Epoch [85/1000], Loss: 0.2610\n",
      "tensor(2.1316, grad_fn=<MeanBackward0>)\n",
      "85\n",
      "Epoch [86/1000], Loss: 0.5579\n",
      "Epoch [86/1000], Loss: 0.6329\n",
      "Epoch [86/1000], Loss: 0.4401\n",
      "Epoch [86/1000], Loss: 0.3997\n",
      "Epoch [86/1000], Loss: 0.3916\n",
      "Epoch [86/1000], Loss: 0.2948\n",
      "Epoch [86/1000], Loss: 0.3601\n",
      "Epoch [86/1000], Loss: 0.2954\n",
      "Epoch [86/1000], Loss: 0.3056\n",
      "Epoch [86/1000], Loss: 0.4731\n",
      "Epoch [86/1000], Loss: 0.2643\n",
      "tensor(2.2975, grad_fn=<MeanBackward0>)\n",
      "86\n",
      "Epoch [87/1000], Loss: 0.4214\n",
      "Epoch [87/1000], Loss: 0.3437\n",
      "Epoch [87/1000], Loss: 0.4250\n",
      "Epoch [87/1000], Loss: 0.2253\n",
      "Epoch [87/1000], Loss: 0.4116\n",
      "Epoch [87/1000], Loss: 0.3261\n",
      "Epoch [87/1000], Loss: 0.3536\n",
      "Epoch [87/1000], Loss: 0.2879\n",
      "Epoch [87/1000], Loss: 0.2546\n",
      "Epoch [87/1000], Loss: 0.4090\n",
      "Epoch [87/1000], Loss: 0.2858\n",
      "tensor(2.4167, grad_fn=<MeanBackward0>)\n",
      "87\n",
      "Epoch [88/1000], Loss: 0.3206\n",
      "Epoch [88/1000], Loss: 0.3907\n",
      "Epoch [88/1000], Loss: 0.3328\n",
      "Epoch [88/1000], Loss: 0.2426\n",
      "Epoch [88/1000], Loss: 0.3132\n",
      "Epoch [88/1000], Loss: 0.2716\n",
      "Epoch [88/1000], Loss: 0.2803\n",
      "Epoch [88/1000], Loss: 0.2941\n",
      "Epoch [88/1000], Loss: 0.2340\n",
      "Epoch [88/1000], Loss: 0.3948\n",
      "Epoch [88/1000], Loss: 0.2326\n",
      "tensor(2.3539, grad_fn=<MeanBackward0>)\n",
      "88\n",
      "Epoch [89/1000], Loss: 0.3109\n",
      "Epoch [89/1000], Loss: 0.3453\n",
      "Epoch [89/1000], Loss: 0.3481\n",
      "Epoch [89/1000], Loss: 0.2344\n",
      "Epoch [89/1000], Loss: 0.2969\n",
      "Epoch [89/1000], Loss: 0.2469\n",
      "Epoch [89/1000], Loss: 0.2952\n",
      "Epoch [89/1000], Loss: 0.2682\n",
      "Epoch [89/1000], Loss: 0.2191\n",
      "Epoch [89/1000], Loss: 0.3983\n",
      "Epoch [89/1000], Loss: 0.1632\n",
      "tensor(2.4870, grad_fn=<MeanBackward0>)\n",
      "89\n",
      "Epoch [90/1000], Loss: 0.3494\n",
      "Epoch [90/1000], Loss: 0.3281\n",
      "Epoch [90/1000], Loss: 0.3470\n",
      "Epoch [90/1000], Loss: 0.1924\n",
      "Epoch [90/1000], Loss: 0.2838\n",
      "Epoch [90/1000], Loss: 0.2340\n",
      "Epoch [90/1000], Loss: 0.2945\n",
      "Epoch [90/1000], Loss: 0.2362\n",
      "Epoch [90/1000], Loss: 0.2679\n",
      "Epoch [90/1000], Loss: 0.4124\n",
      "Epoch [90/1000], Loss: 0.2863\n",
      "tensor(2.5306, grad_fn=<MeanBackward0>)\n",
      "90\n",
      "Epoch [91/1000], Loss: 0.3619\n",
      "Epoch [91/1000], Loss: 0.4450\n",
      "Epoch [91/1000], Loss: 0.3232\n",
      "Epoch [91/1000], Loss: 0.3546\n",
      "Epoch [91/1000], Loss: 0.2809\n",
      "Epoch [91/1000], Loss: 0.3863\n",
      "Epoch [91/1000], Loss: 0.3050\n",
      "Epoch [91/1000], Loss: 0.3802\n",
      "Epoch [91/1000], Loss: 0.2528\n",
      "Epoch [91/1000], Loss: 0.5663\n",
      "Epoch [91/1000], Loss: 0.3063\n",
      "tensor(2.2318, grad_fn=<MeanBackward0>)\n",
      "91\n",
      "Epoch [92/1000], Loss: 0.4367\n",
      "Epoch [92/1000], Loss: 0.4929\n",
      "Epoch [92/1000], Loss: 0.3337\n",
      "Epoch [92/1000], Loss: 0.2632\n",
      "Epoch [92/1000], Loss: 0.3474\n",
      "Epoch [92/1000], Loss: 0.2612\n",
      "Epoch [92/1000], Loss: 0.3444\n",
      "Epoch [92/1000], Loss: 0.2867\n",
      "Epoch [92/1000], Loss: 0.2844\n",
      "Epoch [92/1000], Loss: 0.3603\n",
      "Epoch [92/1000], Loss: 0.3313\n",
      "tensor(2.4391, grad_fn=<MeanBackward0>)\n",
      "92\n",
      "Epoch [93/1000], Loss: 0.3803\n",
      "Epoch [93/1000], Loss: 0.4225\n",
      "Epoch [93/1000], Loss: 0.3670\n",
      "Epoch [93/1000], Loss: 0.2800\n",
      "Epoch [93/1000], Loss: 0.3274\n",
      "Epoch [93/1000], Loss: 0.2561\n",
      "Epoch [93/1000], Loss: 0.3005\n",
      "Epoch [93/1000], Loss: 0.2759\n",
      "Epoch [93/1000], Loss: 0.2406\n",
      "Epoch [93/1000], Loss: 0.3770\n",
      "Epoch [93/1000], Loss: 0.1521\n",
      "tensor(2.3661, grad_fn=<MeanBackward0>)\n",
      "93\n",
      "Epoch [94/1000], Loss: 0.3204\n",
      "Epoch [94/1000], Loss: 0.3145\n",
      "Epoch [94/1000], Loss: 0.2944\n",
      "Epoch [94/1000], Loss: 0.2278\n",
      "Epoch [94/1000], Loss: 0.2714\n",
      "Epoch [94/1000], Loss: 0.2332\n",
      "Epoch [94/1000], Loss: 0.2360\n",
      "Epoch [94/1000], Loss: 0.2390\n",
      "Epoch [94/1000], Loss: 0.1877\n",
      "Epoch [94/1000], Loss: 0.4058\n",
      "Epoch [94/1000], Loss: 0.1786\n",
      "tensor(2.4138, grad_fn=<MeanBackward0>)\n",
      "94\n",
      "Epoch [95/1000], Loss: 0.2720\n",
      "Epoch [95/1000], Loss: 0.3261\n",
      "Epoch [95/1000], Loss: 0.2693\n",
      "Epoch [95/1000], Loss: 0.2688\n",
      "Epoch [95/1000], Loss: 0.2535\n",
      "Epoch [95/1000], Loss: 0.2428\n",
      "Epoch [95/1000], Loss: 0.2708\n",
      "Epoch [95/1000], Loss: 0.1944\n",
      "Epoch [95/1000], Loss: 0.1889\n",
      "Epoch [95/1000], Loss: 0.3472\n",
      "Epoch [95/1000], Loss: 0.1658\n",
      "tensor(2.4177, grad_fn=<MeanBackward0>)\n",
      "95\n",
      "Epoch [96/1000], Loss: 0.2550\n",
      "Epoch [96/1000], Loss: 0.2744\n",
      "Epoch [96/1000], Loss: 0.2522\n",
      "Epoch [96/1000], Loss: 0.1782\n",
      "Epoch [96/1000], Loss: 0.2362\n",
      "Epoch [96/1000], Loss: 0.2082\n",
      "Epoch [96/1000], Loss: 0.2139\n",
      "Epoch [96/1000], Loss: 0.1969\n",
      "Epoch [96/1000], Loss: 0.1802\n",
      "Epoch [96/1000], Loss: 0.3368\n",
      "Epoch [96/1000], Loss: 0.1938\n",
      "tensor(2.4329, grad_fn=<MeanBackward0>)\n",
      "96\n",
      "Epoch [97/1000], Loss: 0.2380\n",
      "Epoch [97/1000], Loss: 0.2677\n",
      "Epoch [97/1000], Loss: 0.2334\n",
      "Epoch [97/1000], Loss: 0.1776\n",
      "Epoch [97/1000], Loss: 0.2272\n",
      "Epoch [97/1000], Loss: 0.2395\n",
      "Epoch [97/1000], Loss: 0.2169\n",
      "Epoch [97/1000], Loss: 0.3171\n",
      "Epoch [97/1000], Loss: 0.2076\n",
      "Epoch [97/1000], Loss: 0.6301\n",
      "Epoch [97/1000], Loss: 0.4452\n",
      "tensor(2.2394, grad_fn=<MeanBackward0>)\n",
      "97\n",
      "Epoch [98/1000], Loss: 0.3375\n",
      "Epoch [98/1000], Loss: 0.5520\n",
      "Epoch [98/1000], Loss: 0.4063\n",
      "Epoch [98/1000], Loss: 0.1674\n",
      "Epoch [98/1000], Loss: 0.2593\n",
      "Epoch [98/1000], Loss: 0.2187\n",
      "Epoch [98/1000], Loss: 0.2310\n",
      "Epoch [98/1000], Loss: 0.2060\n",
      "Epoch [98/1000], Loss: 0.1894\n",
      "Epoch [98/1000], Loss: 0.4533\n",
      "Epoch [98/1000], Loss: 0.2327\n",
      "tensor(2.2014, grad_fn=<MeanBackward0>)\n",
      "98\n",
      "Epoch [99/1000], Loss: 0.3818\n",
      "Epoch [99/1000], Loss: 0.4834\n",
      "Epoch [99/1000], Loss: 0.2673\n",
      "Epoch [99/1000], Loss: 0.3456\n",
      "Epoch [99/1000], Loss: 0.3294\n",
      "Epoch [99/1000], Loss: 0.2982\n",
      "Epoch [99/1000], Loss: 0.3358\n",
      "Epoch [99/1000], Loss: 0.1960\n",
      "Epoch [99/1000], Loss: 0.2054\n",
      "Epoch [99/1000], Loss: 0.3250\n",
      "Epoch [99/1000], Loss: 0.1381\n",
      "tensor(2.4331, grad_fn=<MeanBackward0>)\n",
      "99\n",
      "Epoch [100/1000], Loss: 0.2430\n",
      "Epoch [100/1000], Loss: 0.2929\n",
      "Epoch [100/1000], Loss: 0.2396\n",
      "Epoch [100/1000], Loss: 0.1492\n",
      "Epoch [100/1000], Loss: 0.2117\n",
      "Epoch [100/1000], Loss: 0.2238\n",
      "Epoch [100/1000], Loss: 0.2155\n",
      "Epoch [100/1000], Loss: 0.1976\n",
      "Epoch [100/1000], Loss: 0.1567\n",
      "Epoch [100/1000], Loss: 0.3532\n",
      "Epoch [100/1000], Loss: 0.1215\n",
      "tensor(2.3060, grad_fn=<MeanBackward0>)\n",
      "100\n",
      "Epoch [101/1000], Loss: 0.2989\n",
      "Epoch [101/1000], Loss: 0.2558\n",
      "Epoch [101/1000], Loss: 0.2582\n",
      "Epoch [101/1000], Loss: 0.2102\n",
      "Epoch [101/1000], Loss: 0.2100\n",
      "Epoch [101/1000], Loss: 0.1941\n",
      "Epoch [101/1000], Loss: 0.2231\n",
      "Epoch [101/1000], Loss: 0.1884\n",
      "Epoch [101/1000], Loss: 0.1827\n",
      "Epoch [101/1000], Loss: 0.3580\n",
      "Epoch [101/1000], Loss: 0.1791\n",
      "tensor(2.2043, grad_fn=<MeanBackward0>)\n",
      "101\n",
      "Epoch [102/1000], Loss: 0.3668\n",
      "Epoch [102/1000], Loss: 0.4538\n",
      "Epoch [102/1000], Loss: 0.2547\n",
      "Epoch [102/1000], Loss: 0.2582\n",
      "Epoch [102/1000], Loss: 0.2288\n",
      "Epoch [102/1000], Loss: 0.2352\n",
      "Epoch [102/1000], Loss: 0.2187\n",
      "Epoch [102/1000], Loss: 0.2011\n",
      "Epoch [102/1000], Loss: 0.1577\n",
      "Epoch [102/1000], Loss: 0.3003\n",
      "Epoch [102/1000], Loss: 0.2180\n",
      "tensor(2.3854, grad_fn=<MeanBackward0>)\n",
      "102\n",
      "Epoch [103/1000], Loss: 0.2276\n",
      "Epoch [103/1000], Loss: 0.3624\n",
      "Epoch [103/1000], Loss: 0.2956\n",
      "Epoch [103/1000], Loss: 0.1740\n",
      "Epoch [103/1000], Loss: 0.2560\n",
      "Epoch [103/1000], Loss: 0.1692\n",
      "Epoch [103/1000], Loss: 0.1993\n",
      "Epoch [103/1000], Loss: 0.1855\n",
      "Epoch [103/1000], Loss: 0.1614\n",
      "Epoch [103/1000], Loss: 0.3148\n",
      "Epoch [103/1000], Loss: 0.1327\n",
      "tensor(2.3484, grad_fn=<MeanBackward0>)\n",
      "103\n",
      "Epoch [104/1000], Loss: 0.2096\n",
      "Epoch [104/1000], Loss: 0.2959\n",
      "Epoch [104/1000], Loss: 0.2153\n",
      "Epoch [104/1000], Loss: 0.1428\n",
      "Epoch [104/1000], Loss: 0.2103\n",
      "Epoch [104/1000], Loss: 0.1913\n",
      "Epoch [104/1000], Loss: 0.2356\n",
      "Epoch [104/1000], Loss: 0.2210\n",
      "Epoch [104/1000], Loss: 0.2563\n",
      "Epoch [104/1000], Loss: 0.2854\n",
      "Epoch [104/1000], Loss: 0.1114\n",
      "tensor(2.4410, grad_fn=<MeanBackward0>)\n",
      "104\n",
      "Epoch [105/1000], Loss: 0.2368\n",
      "Epoch [105/1000], Loss: 0.3955\n",
      "Epoch [105/1000], Loss: 0.4179\n",
      "Epoch [105/1000], Loss: 0.2194\n",
      "Epoch [105/1000], Loss: 0.4147\n",
      "Epoch [105/1000], Loss: 0.5254\n",
      "Epoch [105/1000], Loss: 0.3551\n",
      "Epoch [105/1000], Loss: 0.4308\n",
      "Epoch [105/1000], Loss: 0.6175\n",
      "Epoch [105/1000], Loss: 0.5108\n",
      "Epoch [105/1000], Loss: 0.2176\n",
      "tensor(2.6747, grad_fn=<MeanBackward0>)\n",
      "105\n",
      "Epoch [106/1000], Loss: 0.7001\n",
      "Epoch [106/1000], Loss: 0.6689\n",
      "Epoch [106/1000], Loss: 0.3502\n",
      "Epoch [106/1000], Loss: 0.6019\n",
      "Epoch [106/1000], Loss: 0.7296\n",
      "Epoch [106/1000], Loss: 0.4863\n",
      "Epoch [106/1000], Loss: 0.3594\n",
      "Epoch [106/1000], Loss: 0.5394\n",
      "Epoch [106/1000], Loss: 0.4509\n",
      "Epoch [106/1000], Loss: 0.3797\n",
      "Epoch [106/1000], Loss: 0.4075\n",
      "tensor(2.2171, grad_fn=<MeanBackward0>)\n",
      "106\n",
      "Epoch [107/1000], Loss: 0.4024\n",
      "Epoch [107/1000], Loss: 0.3118\n",
      "Epoch [107/1000], Loss: 0.2814\n",
      "Epoch [107/1000], Loss: 0.1531\n",
      "Epoch [107/1000], Loss: 0.2804\n",
      "Epoch [107/1000], Loss: 0.1887\n",
      "Epoch [107/1000], Loss: 0.3141\n",
      "Epoch [107/1000], Loss: 0.2073\n",
      "Epoch [107/1000], Loss: 0.2745\n",
      "Epoch [107/1000], Loss: 0.3897\n",
      "Epoch [107/1000], Loss: 0.1966\n",
      "tensor(2.5505, grad_fn=<MeanBackward0>)\n",
      "107\n",
      "Epoch [108/1000], Loss: 0.4996\n",
      "Epoch [108/1000], Loss: 0.3283\n",
      "Epoch [108/1000], Loss: 0.4837\n",
      "Epoch [108/1000], Loss: 0.7186\n",
      "Epoch [108/1000], Loss: 0.5665\n",
      "Epoch [108/1000], Loss: 0.2912\n",
      "Epoch [108/1000], Loss: 0.6622\n",
      "Epoch [108/1000], Loss: 0.5793\n",
      "Epoch [108/1000], Loss: 0.2912\n",
      "Epoch [108/1000], Loss: 0.4584\n",
      "Epoch [108/1000], Loss: 0.5323\n",
      "tensor(2.1516, grad_fn=<MeanBackward0>)\n",
      "108\n",
      "Epoch [109/1000], Loss: 0.4678\n",
      "Epoch [109/1000], Loss: 0.3649\n",
      "Epoch [109/1000], Loss: 0.4795\n",
      "Epoch [109/1000], Loss: 0.2929\n",
      "Epoch [109/1000], Loss: 0.3695\n",
      "Epoch [109/1000], Loss: 0.3295\n",
      "Epoch [109/1000], Loss: 0.3138\n",
      "Epoch [109/1000], Loss: 0.2536\n",
      "Epoch [109/1000], Loss: 0.2086\n",
      "Epoch [109/1000], Loss: 0.3416\n",
      "Epoch [109/1000], Loss: 0.2410\n",
      "tensor(2.4436, grad_fn=<MeanBackward0>)\n",
      "109\n",
      "Epoch [110/1000], Loss: 0.3096\n",
      "Epoch [110/1000], Loss: 0.3230\n",
      "Epoch [110/1000], Loss: 0.3621\n",
      "Epoch [110/1000], Loss: 0.4339\n",
      "Epoch [110/1000], Loss: 0.2933\n",
      "Epoch [110/1000], Loss: 0.3577\n",
      "Epoch [110/1000], Loss: 0.4418\n",
      "Epoch [110/1000], Loss: 0.1936\n",
      "Epoch [110/1000], Loss: 0.3375\n",
      "Epoch [110/1000], Loss: 0.3326\n",
      "Epoch [110/1000], Loss: 0.2083\n",
      "tensor(2.4379, grad_fn=<MeanBackward0>)\n",
      "110\n",
      "Epoch [111/1000], Loss: 0.3116\n",
      "Epoch [111/1000], Loss: 0.2854\n",
      "Epoch [111/1000], Loss: 0.2983\n",
      "Epoch [111/1000], Loss: 0.1256\n",
      "Epoch [111/1000], Loss: 0.2663\n",
      "Epoch [111/1000], Loss: 0.2092\n",
      "Epoch [111/1000], Loss: 0.2503\n",
      "Epoch [111/1000], Loss: 0.2210\n",
      "Epoch [111/1000], Loss: 0.1928\n",
      "Epoch [111/1000], Loss: 0.3563\n",
      "Epoch [111/1000], Loss: 0.2028\n",
      "tensor(2.3822, grad_fn=<MeanBackward0>)\n",
      "111\n",
      "Epoch [112/1000], Loss: 0.2403\n",
      "Epoch [112/1000], Loss: 0.2866\n",
      "Epoch [112/1000], Loss: 0.2791\n",
      "Epoch [112/1000], Loss: 0.2850\n",
      "Epoch [112/1000], Loss: 0.1980\n",
      "Epoch [112/1000], Loss: 0.3531\n",
      "Epoch [112/1000], Loss: 0.4123\n",
      "Epoch [112/1000], Loss: 0.1637\n",
      "Epoch [112/1000], Loss: 0.4593\n",
      "Epoch [112/1000], Loss: 0.5080\n",
      "Epoch [112/1000], Loss: 0.2092\n",
      "tensor(2.6269, grad_fn=<MeanBackward0>)\n",
      "112\n",
      "Epoch [113/1000], Loss: 0.5329\n",
      "Epoch [113/1000], Loss: 0.7273\n",
      "Epoch [113/1000], Loss: 0.4047\n",
      "Epoch [113/1000], Loss: 0.4728\n",
      "Epoch [113/1000], Loss: 0.7643\n",
      "Epoch [113/1000], Loss: 0.6820\n",
      "Epoch [113/1000], Loss: 0.3209\n",
      "Epoch [113/1000], Loss: 0.4055\n",
      "Epoch [113/1000], Loss: 0.6371\n",
      "Epoch [113/1000], Loss: 0.6535\n",
      "Epoch [113/1000], Loss: 0.2374\n",
      "tensor(2.2292, grad_fn=<MeanBackward0>)\n",
      "113\n",
      "Epoch [114/1000], Loss: 0.4674\n",
      "Epoch [114/1000], Loss: 0.5336\n",
      "Epoch [114/1000], Loss: 0.4491\n",
      "Epoch [114/1000], Loss: 0.2485\n",
      "Epoch [114/1000], Loss: 0.2870\n",
      "Epoch [114/1000], Loss: 0.3417\n",
      "Epoch [114/1000], Loss: 0.4278\n",
      "Epoch [114/1000], Loss: 0.1893\n",
      "Epoch [114/1000], Loss: 0.2606\n",
      "Epoch [114/1000], Loss: 0.3776\n",
      "Epoch [114/1000], Loss: 0.1628\n",
      "tensor(2.4831, grad_fn=<MeanBackward0>)\n",
      "114\n",
      "Epoch [115/1000], Loss: 0.3049\n",
      "Epoch [115/1000], Loss: 0.3275\n",
      "Epoch [115/1000], Loss: 0.2799\n",
      "Epoch [115/1000], Loss: 0.3882\n",
      "Epoch [115/1000], Loss: 0.3260\n",
      "Epoch [115/1000], Loss: 0.2628\n",
      "Epoch [115/1000], Loss: 0.6035\n",
      "Epoch [115/1000], Loss: 0.4773\n",
      "Epoch [115/1000], Loss: 0.1863\n",
      "Epoch [115/1000], Loss: 0.4574\n",
      "Epoch [115/1000], Loss: 0.5307\n",
      "tensor(2.1545, grad_fn=<MeanBackward0>)\n",
      "115\n",
      "Epoch [116/1000], Loss: 0.4139\n",
      "Epoch [116/1000], Loss: 0.3500\n",
      "Epoch [116/1000], Loss: 0.4434\n",
      "Epoch [116/1000], Loss: 0.2187\n",
      "Epoch [116/1000], Loss: 0.3813\n",
      "Epoch [116/1000], Loss: 0.4728\n",
      "Epoch [116/1000], Loss: 0.3037\n",
      "Epoch [116/1000], Loss: 0.2362\n",
      "Epoch [116/1000], Loss: 0.4692\n",
      "Epoch [116/1000], Loss: 0.5435\n",
      "Epoch [116/1000], Loss: 0.2031\n",
      "tensor(2.1760, grad_fn=<MeanBackward0>)\n",
      "116\n",
      "Epoch [117/1000], Loss: 0.4500\n",
      "Epoch [117/1000], Loss: 0.5196\n",
      "Epoch [117/1000], Loss: 0.3669\n",
      "Epoch [117/1000], Loss: 0.1444\n",
      "Epoch [117/1000], Loss: 0.2496\n",
      "Epoch [117/1000], Loss: 0.1795\n",
      "Epoch [117/1000], Loss: 0.1853\n",
      "Epoch [117/1000], Loss: 0.2108\n",
      "Epoch [117/1000], Loss: 0.1602\n",
      "Epoch [117/1000], Loss: 0.3339\n",
      "Epoch [117/1000], Loss: 0.1138\n",
      "tensor(2.2926, grad_fn=<MeanBackward0>)\n",
      "117\n",
      "Epoch [118/1000], Loss: 0.2530\n",
      "Epoch [118/1000], Loss: 0.2443\n",
      "Epoch [118/1000], Loss: 0.2992\n",
      "Epoch [118/1000], Loss: 0.1630\n",
      "Epoch [118/1000], Loss: 0.2654\n",
      "Epoch [118/1000], Loss: 0.2408\n",
      "Epoch [118/1000], Loss: 0.2751\n",
      "Epoch [118/1000], Loss: 0.2884\n",
      "Epoch [118/1000], Loss: 0.1362\n",
      "Epoch [118/1000], Loss: 0.3201\n",
      "Epoch [118/1000], Loss: 0.1682\n",
      "tensor(2.4444, grad_fn=<MeanBackward0>)\n",
      "118\n",
      "Epoch [119/1000], Loss: 0.2473\n",
      "Epoch [119/1000], Loss: 0.2679\n",
      "Epoch [119/1000], Loss: 0.2868\n",
      "Epoch [119/1000], Loss: 0.3259\n",
      "Epoch [119/1000], Loss: 0.2048\n",
      "Epoch [119/1000], Loss: 0.2577\n",
      "Epoch [119/1000], Loss: 0.4114\n",
      "Epoch [119/1000], Loss: 0.2152\n",
      "Epoch [119/1000], Loss: 0.2004\n",
      "Epoch [119/1000], Loss: 0.3007\n",
      "Epoch [119/1000], Loss: 0.1083\n",
      "tensor(2.3915, grad_fn=<MeanBackward0>)\n",
      "119\n",
      "Epoch [120/1000], Loss: 0.2253\n",
      "Epoch [120/1000], Loss: 0.2329\n",
      "Epoch [120/1000], Loss: 0.2010\n",
      "Epoch [120/1000], Loss: 0.2552\n",
      "Epoch [120/1000], Loss: 0.1857\n",
      "Epoch [120/1000], Loss: 0.1627\n",
      "Epoch [120/1000], Loss: 0.3295\n",
      "Epoch [120/1000], Loss: 0.1743\n",
      "Epoch [120/1000], Loss: 0.1986\n",
      "Epoch [120/1000], Loss: 0.2842\n",
      "Epoch [120/1000], Loss: 0.1095\n",
      "tensor(2.3805, grad_fn=<MeanBackward0>)\n",
      "120\n",
      "Epoch [121/1000], Loss: 0.2027\n",
      "Epoch [121/1000], Loss: 0.2191\n",
      "Epoch [121/1000], Loss: 0.2219\n",
      "Epoch [121/1000], Loss: 0.1739\n",
      "Epoch [121/1000], Loss: 0.2030\n",
      "Epoch [121/1000], Loss: 0.1470\n",
      "Epoch [121/1000], Loss: 0.3645\n",
      "Epoch [121/1000], Loss: 0.3154\n",
      "Epoch [121/1000], Loss: 0.1690\n",
      "Epoch [121/1000], Loss: 0.3802\n",
      "Epoch [121/1000], Loss: 0.4537\n",
      "tensor(2.1431, grad_fn=<MeanBackward0>)\n",
      "121\n",
      "Epoch [122/1000], Loss: 0.3802\n",
      "Epoch [122/1000], Loss: 0.2480\n",
      "Epoch [122/1000], Loss: 0.3961\n",
      "Epoch [122/1000], Loss: 0.2402\n",
      "Epoch [122/1000], Loss: 0.2434\n",
      "Epoch [122/1000], Loss: 0.4194\n",
      "Epoch [122/1000], Loss: 0.2866\n",
      "Epoch [122/1000], Loss: 0.2121\n",
      "Epoch [122/1000], Loss: 0.4855\n",
      "Epoch [122/1000], Loss: 0.5283\n",
      "Epoch [122/1000], Loss: 0.1953\n",
      "tensor(2.1738, grad_fn=<MeanBackward0>)\n",
      "122\n",
      "Epoch [123/1000], Loss: 0.4211\n",
      "Epoch [123/1000], Loss: 0.4952\n",
      "Epoch [123/1000], Loss: 0.3369\n",
      "Epoch [123/1000], Loss: 0.1175\n",
      "Epoch [123/1000], Loss: 0.2668\n",
      "Epoch [123/1000], Loss: 0.1633\n",
      "Epoch [123/1000], Loss: 0.1860\n",
      "Epoch [123/1000], Loss: 0.1627\n",
      "Epoch [123/1000], Loss: 0.2126\n",
      "Epoch [123/1000], Loss: 0.2971\n",
      "Epoch [123/1000], Loss: 0.1882\n",
      "tensor(2.3023, grad_fn=<MeanBackward0>)\n",
      "123\n",
      "Epoch [124/1000], Loss: 0.2459\n",
      "Epoch [124/1000], Loss: 0.2611\n",
      "Epoch [124/1000], Loss: 0.2571\n",
      "Epoch [124/1000], Loss: 0.1762\n",
      "Epoch [124/1000], Loss: 0.2209\n",
      "Epoch [124/1000], Loss: 0.1701\n",
      "Epoch [124/1000], Loss: 0.2939\n",
      "Epoch [124/1000], Loss: 0.3454\n",
      "Epoch [124/1000], Loss: 0.2977\n",
      "Epoch [124/1000], Loss: 0.3248\n",
      "Epoch [124/1000], Loss: 0.3956\n",
      "tensor(2.1141, grad_fn=<MeanBackward0>)\n",
      "124\n",
      "Epoch [125/1000], Loss: 0.3736\n",
      "Epoch [125/1000], Loss: 0.2607\n",
      "Epoch [125/1000], Loss: 0.3845\n",
      "Epoch [125/1000], Loss: 0.3033\n",
      "Epoch [125/1000], Loss: 0.1719\n",
      "Epoch [125/1000], Loss: 0.3798\n",
      "Epoch [125/1000], Loss: 0.3823\n",
      "Epoch [125/1000], Loss: 0.1830\n",
      "Epoch [125/1000], Loss: 0.5047\n",
      "Epoch [125/1000], Loss: 0.8791\n",
      "Epoch [125/1000], Loss: 0.5870\n",
      "tensor(2.4341, grad_fn=<MeanBackward0>)\n",
      "125\n",
      "Epoch [126/1000], Loss: 0.2505\n",
      "Epoch [126/1000], Loss: 0.5280\n",
      "Epoch [126/1000], Loss: 0.8277\n",
      "Epoch [126/1000], Loss: 0.9267\n",
      "Epoch [126/1000], Loss: 0.7883\n",
      "Epoch [126/1000], Loss: 0.6001\n",
      "Epoch [126/1000], Loss: 0.2578\n",
      "Epoch [126/1000], Loss: 0.4074\n",
      "Epoch [126/1000], Loss: 0.7882\n",
      "Epoch [126/1000], Loss: 0.9427\n",
      "Epoch [126/1000], Loss: 0.5564\n",
      "tensor(2.3986, grad_fn=<MeanBackward0>)\n",
      "126\n",
      "Epoch [127/1000], Loss: 0.3419\n",
      "Epoch [127/1000], Loss: 0.5071\n",
      "Epoch [127/1000], Loss: 0.7398\n",
      "Epoch [127/1000], Loss: 0.8504\n",
      "Epoch [127/1000], Loss: 0.8045\n",
      "Epoch [127/1000], Loss: 0.6915\n",
      "Epoch [127/1000], Loss: 0.3859\n",
      "Epoch [127/1000], Loss: 0.2677\n",
      "Epoch [127/1000], Loss: 0.5845\n",
      "Epoch [127/1000], Loss: 1.0370\n",
      "Epoch [127/1000], Loss: 0.8174\n",
      "tensor(2.7203, grad_fn=<MeanBackward0>)\n",
      "127\n",
      "Epoch [128/1000], Loss: 0.6173\n",
      "Epoch [128/1000], Loss: 0.3481\n",
      "Epoch [128/1000], Loss: 0.5148\n",
      "Epoch [128/1000], Loss: 0.7286\n",
      "Epoch [128/1000], Loss: 0.7833\n",
      "Epoch [128/1000], Loss: 0.7958\n",
      "Epoch [128/1000], Loss: 0.5258\n",
      "Epoch [128/1000], Loss: 0.3505\n",
      "Epoch [128/1000], Loss: 0.3812\n",
      "Epoch [128/1000], Loss: 0.8838\n",
      "Epoch [128/1000], Loss: 0.7765\n",
      "tensor(2.7511, grad_fn=<MeanBackward0>)\n",
      "128\n",
      "Epoch [129/1000], Loss: 0.6876\n",
      "Epoch [129/1000], Loss: 0.4266\n",
      "Epoch [129/1000], Loss: 0.4036\n",
      "Epoch [129/1000], Loss: 0.6437\n",
      "Epoch [129/1000], Loss: 0.7645\n",
      "Epoch [129/1000], Loss: 0.8417\n",
      "Epoch [129/1000], Loss: 0.6132\n",
      "Epoch [129/1000], Loss: 0.5254\n",
      "Epoch [129/1000], Loss: 0.2806\n",
      "Epoch [129/1000], Loss: 0.7611\n",
      "Epoch [129/1000], Loss: 0.8395\n",
      "tensor(2.9258, grad_fn=<MeanBackward0>)\n",
      "129\n",
      "Epoch [130/1000], Loss: 0.9555\n",
      "Epoch [130/1000], Loss: 0.8172\n",
      "Epoch [130/1000], Loss: 0.4312\n",
      "Epoch [130/1000], Loss: 0.3997\n",
      "Epoch [130/1000], Loss: 0.6264\n",
      "Epoch [130/1000], Loss: 0.8466\n",
      "Epoch [130/1000], Loss: 0.7580\n",
      "Epoch [130/1000], Loss: 0.7969\n",
      "Epoch [130/1000], Loss: 0.5852\n",
      "Epoch [130/1000], Loss: 0.4669\n",
      "Epoch [130/1000], Loss: 0.4193\n",
      "tensor(2.7576, grad_fn=<MeanBackward0>)\n",
      "130\n",
      "Epoch [131/1000], Loss: 0.7078\n",
      "Epoch [131/1000], Loss: 0.7495\n",
      "Epoch [131/1000], Loss: 0.5132\n",
      "Epoch [131/1000], Loss: 0.2749\n",
      "Epoch [131/1000], Loss: 0.4372\n",
      "Epoch [131/1000], Loss: 0.6561\n",
      "Epoch [131/1000], Loss: 0.6221\n",
      "Epoch [131/1000], Loss: 0.7078\n",
      "Epoch [131/1000], Loss: 0.4807\n",
      "Epoch [131/1000], Loss: 0.4347\n",
      "Epoch [131/1000], Loss: 0.3823\n",
      "tensor(2.7374, grad_fn=<MeanBackward0>)\n",
      "131\n",
      "Epoch [132/1000], Loss: 0.6572\n",
      "Epoch [132/1000], Loss: 0.6638\n",
      "Epoch [132/1000], Loss: 0.3822\n",
      "Epoch [132/1000], Loss: 0.2592\n",
      "Epoch [132/1000], Loss: 0.4402\n",
      "Epoch [132/1000], Loss: 0.6138\n",
      "Epoch [132/1000], Loss: 0.5281\n",
      "Epoch [132/1000], Loss: 0.4752\n",
      "Epoch [132/1000], Loss: 0.2412\n",
      "Epoch [132/1000], Loss: 0.6032\n",
      "Epoch [132/1000], Loss: 0.6189\n",
      "tensor(2.8069, grad_fn=<MeanBackward0>)\n",
      "132\n",
      "Epoch [133/1000], Loss: 0.7218\n",
      "Epoch [133/1000], Loss: 0.5540\n",
      "Epoch [133/1000], Loss: 0.2971\n",
      "Epoch [133/1000], Loss: 0.4038\n",
      "Epoch [133/1000], Loss: 0.5827\n",
      "Epoch [133/1000], Loss: 0.7315\n",
      "Epoch [133/1000], Loss: 0.6089\n",
      "Epoch [133/1000], Loss: 0.5503\n",
      "Epoch [133/1000], Loss: 0.2947\n",
      "Epoch [133/1000], Loss: 0.5647\n",
      "Epoch [133/1000], Loss: 0.6345\n",
      "tensor(2.8914, grad_fn=<MeanBackward0>)\n",
      "133\n",
      "Epoch [134/1000], Loss: 0.8120\n",
      "Epoch [134/1000], Loss: 0.7182\n",
      "Epoch [134/1000], Loss: 0.4043\n",
      "Epoch [134/1000], Loss: 0.2954\n",
      "Epoch [134/1000], Loss: 0.5129\n",
      "Epoch [134/1000], Loss: 0.7311\n",
      "Epoch [134/1000], Loss: 0.6714\n",
      "Epoch [134/1000], Loss: 0.7285\n",
      "Epoch [134/1000], Loss: 0.4905\n",
      "Epoch [134/1000], Loss: 0.4273\n",
      "Epoch [134/1000], Loss: 0.3568\n",
      "tensor(2.7596, grad_fn=<MeanBackward0>)\n",
      "134\n",
      "Epoch [135/1000], Loss: 0.6955\n",
      "Epoch [135/1000], Loss: 0.7904\n",
      "Epoch [135/1000], Loss: 0.5584\n",
      "Epoch [135/1000], Loss: 0.2731\n",
      "Epoch [135/1000], Loss: 0.3478\n",
      "Epoch [135/1000], Loss: 0.5900\n",
      "Epoch [135/1000], Loss: 0.6187\n",
      "Epoch [135/1000], Loss: 0.7509\n",
      "Epoch [135/1000], Loss: 0.5885\n",
      "Epoch [135/1000], Loss: 0.4482\n",
      "Epoch [135/1000], Loss: 0.2577\n",
      "tensor(2.7535, grad_fn=<MeanBackward0>)\n",
      "135\n",
      "Epoch [136/1000], Loss: 0.5976\n",
      "Epoch [136/1000], Loss: 0.7940\n",
      "Epoch [136/1000], Loss: 0.6583\n",
      "Epoch [136/1000], Loss: 0.3459\n",
      "Epoch [136/1000], Loss: 0.2890\n",
      "Epoch [136/1000], Loss: 0.5349\n",
      "Epoch [136/1000], Loss: 0.6208\n",
      "Epoch [136/1000], Loss: 0.7826\n",
      "Epoch [136/1000], Loss: 0.6847\n",
      "Epoch [136/1000], Loss: 0.5163\n",
      "Epoch [136/1000], Loss: 0.2481\n",
      "tensor(2.7094, grad_fn=<MeanBackward0>)\n",
      "136\n",
      "Epoch [137/1000], Loss: 0.5132\n",
      "Epoch [137/1000], Loss: 0.8032\n",
      "Epoch [137/1000], Loss: 0.7723\n",
      "Epoch [137/1000], Loss: 0.5199\n",
      "Epoch [137/1000], Loss: 0.3066\n",
      "Epoch [137/1000], Loss: 0.3835\n",
      "Epoch [137/1000], Loss: 0.4858\n",
      "Epoch [137/1000], Loss: 0.7093\n",
      "Epoch [137/1000], Loss: 0.6509\n",
      "Epoch [137/1000], Loss: 0.5329\n",
      "Epoch [137/1000], Loss: 0.2911\n",
      "tensor(2.6096, grad_fn=<MeanBackward0>)\n",
      "137\n",
      "Epoch [138/1000], Loss: 0.4053\n",
      "Epoch [138/1000], Loss: 0.6916\n",
      "Epoch [138/1000], Loss: 0.6912\n",
      "Epoch [138/1000], Loss: 0.5053\n",
      "Epoch [138/1000], Loss: 0.2943\n",
      "Epoch [138/1000], Loss: 0.3319\n",
      "Epoch [138/1000], Loss: 0.4320\n",
      "Epoch [138/1000], Loss: 0.6200\n",
      "Epoch [138/1000], Loss: 0.5419\n",
      "Epoch [138/1000], Loss: 0.4303\n",
      "Epoch [138/1000], Loss: 0.2028\n",
      "tensor(2.6874, grad_fn=<MeanBackward0>)\n",
      "138\n",
      "Epoch [139/1000], Loss: 0.4559\n",
      "Epoch [139/1000], Loss: 0.6815\n",
      "Epoch [139/1000], Loss: 0.5980\n",
      "Epoch [139/1000], Loss: 0.3419\n",
      "Epoch [139/1000], Loss: 0.2379\n",
      "Epoch [139/1000], Loss: 0.4040\n",
      "Epoch [139/1000], Loss: 0.4801\n",
      "Epoch [139/1000], Loss: 0.6081\n",
      "Epoch [139/1000], Loss: 0.4711\n",
      "Epoch [139/1000], Loss: 0.3713\n",
      "Epoch [139/1000], Loss: 0.2024\n",
      "tensor(2.7130, grad_fn=<MeanBackward0>)\n",
      "139\n",
      "Epoch [140/1000], Loss: 0.4743\n",
      "Epoch [140/1000], Loss: 0.5900\n",
      "Epoch [140/1000], Loss: 0.4287\n",
      "Epoch [140/1000], Loss: 0.1928\n",
      "Epoch [140/1000], Loss: 0.2765\n",
      "Epoch [140/1000], Loss: 0.4367\n",
      "Epoch [140/1000], Loss: 0.4263\n",
      "Epoch [140/1000], Loss: 0.4376\n",
      "Epoch [140/1000], Loss: 0.2457\n",
      "Epoch [140/1000], Loss: 0.3847\n",
      "Epoch [140/1000], Loss: 0.3232\n",
      "tensor(2.6771, grad_fn=<MeanBackward0>)\n",
      "140\n",
      "Epoch [141/1000], Loss: 0.3988\n",
      "Epoch [141/1000], Loss: 0.3545\n",
      "Epoch [141/1000], Loss: 0.2651\n",
      "Epoch [141/1000], Loss: 0.2309\n",
      "Epoch [141/1000], Loss: 0.2388\n",
      "Epoch [141/1000], Loss: 0.2417\n",
      "Epoch [141/1000], Loss: 0.2028\n",
      "Epoch [141/1000], Loss: 0.1892\n",
      "Epoch [141/1000], Loss: 0.2126\n",
      "Epoch [141/1000], Loss: 0.3102\n",
      "Epoch [141/1000], Loss: 0.1340\n",
      "tensor(2.4214, grad_fn=<MeanBackward0>)\n",
      "141\n",
      "Epoch [142/1000], Loss: 0.2601\n",
      "Epoch [142/1000], Loss: 0.2625\n",
      "Epoch [142/1000], Loss: 0.2497\n",
      "Epoch [142/1000], Loss: 0.1446\n",
      "Epoch [142/1000], Loss: 0.1517\n",
      "Epoch [142/1000], Loss: 0.2107\n",
      "Epoch [142/1000], Loss: 0.1850\n",
      "Epoch [142/1000], Loss: 0.1732\n",
      "Epoch [142/1000], Loss: 0.2188\n",
      "Epoch [142/1000], Loss: 0.3097\n",
      "Epoch [142/1000], Loss: 0.1746\n",
      "tensor(2.3964, grad_fn=<MeanBackward0>)\n",
      "142\n",
      "Epoch [143/1000], Loss: 0.2809\n",
      "Epoch [143/1000], Loss: 0.2509\n",
      "Epoch [143/1000], Loss: 0.2765\n",
      "Epoch [143/1000], Loss: 0.1557\n",
      "Epoch [143/1000], Loss: 0.1347\n",
      "Epoch [143/1000], Loss: 0.2643\n",
      "Epoch [143/1000], Loss: 0.2578\n",
      "Epoch [143/1000], Loss: 0.1475\n",
      "Epoch [143/1000], Loss: 0.2787\n",
      "Epoch [143/1000], Loss: 0.4724\n",
      "Epoch [143/1000], Loss: 0.2069\n",
      "tensor(2.4173, grad_fn=<MeanBackward0>)\n",
      "143\n",
      "Epoch [144/1000], Loss: 0.2699\n",
      "Epoch [144/1000], Loss: 0.3675\n",
      "Epoch [144/1000], Loss: 0.2998\n",
      "Epoch [144/1000], Loss: 0.1575\n",
      "Epoch [144/1000], Loss: 0.2228\n",
      "Epoch [144/1000], Loss: 0.1878\n",
      "Epoch [144/1000], Loss: 0.1550\n",
      "Epoch [144/1000], Loss: 0.2284\n",
      "Epoch [144/1000], Loss: 0.1790\n",
      "Epoch [144/1000], Loss: 0.3129\n",
      "Epoch [144/1000], Loss: 0.2083\n",
      "tensor(2.4989, grad_fn=<MeanBackward0>)\n",
      "144\n",
      "Epoch [145/1000], Loss: 0.2157\n",
      "Epoch [145/1000], Loss: 0.2550\n",
      "Epoch [145/1000], Loss: 0.2500\n",
      "Epoch [145/1000], Loss: 0.1614\n",
      "Epoch [145/1000], Loss: 0.1892\n",
      "Epoch [145/1000], Loss: 0.1820\n",
      "Epoch [145/1000], Loss: 0.1536\n",
      "Epoch [145/1000], Loss: 0.2049\n",
      "Epoch [145/1000], Loss: 0.1758\n",
      "Epoch [145/1000], Loss: 0.2872\n",
      "Epoch [145/1000], Loss: 0.2271\n",
      "tensor(2.5142, grad_fn=<MeanBackward0>)\n",
      "145\n",
      "Epoch [146/1000], Loss: 0.2483\n",
      "Epoch [146/1000], Loss: 0.2742\n",
      "Epoch [146/1000], Loss: 0.2829\n",
      "Epoch [146/1000], Loss: 0.2202\n",
      "Epoch [146/1000], Loss: 0.1218\n",
      "Epoch [146/1000], Loss: 0.1508\n",
      "Epoch [146/1000], Loss: 0.1615\n",
      "Epoch [146/1000], Loss: 0.1419\n",
      "Epoch [146/1000], Loss: 0.1737\n",
      "Epoch [146/1000], Loss: 0.2560\n",
      "Epoch [146/1000], Loss: 0.1407\n",
      "tensor(2.5158, grad_fn=<MeanBackward0>)\n",
      "146\n",
      "Epoch [147/1000], Loss: 0.2300\n",
      "Epoch [147/1000], Loss: 0.2486\n",
      "Epoch [147/1000], Loss: 0.2032\n",
      "Epoch [147/1000], Loss: 0.1654\n",
      "Epoch [147/1000], Loss: 0.1030\n",
      "Epoch [147/1000], Loss: 0.1289\n",
      "Epoch [147/1000], Loss: 0.1586\n",
      "Epoch [147/1000], Loss: 0.1417\n",
      "Epoch [147/1000], Loss: 0.1607\n",
      "Epoch [147/1000], Loss: 0.2301\n",
      "Epoch [147/1000], Loss: 0.1247\n",
      "tensor(2.5107, grad_fn=<MeanBackward0>)\n",
      "147\n",
      "Epoch [148/1000], Loss: 0.1818\n",
      "Epoch [148/1000], Loss: 0.2139\n",
      "Epoch [148/1000], Loss: 0.1921\n",
      "Epoch [148/1000], Loss: 0.1242\n",
      "Epoch [148/1000], Loss: 0.1049\n",
      "Epoch [148/1000], Loss: 0.1042\n",
      "Epoch [148/1000], Loss: 0.1191\n",
      "Epoch [148/1000], Loss: 0.1108\n",
      "Epoch [148/1000], Loss: 0.1289\n",
      "Epoch [148/1000], Loss: 0.2155\n",
      "Epoch [148/1000], Loss: 0.1089\n",
      "tensor(2.5369, grad_fn=<MeanBackward0>)\n",
      "148\n",
      "Epoch [149/1000], Loss: 0.1927\n",
      "Epoch [149/1000], Loss: 0.1933\n",
      "Epoch [149/1000], Loss: 0.1812\n",
      "Epoch [149/1000], Loss: 0.1292\n",
      "Epoch [149/1000], Loss: 0.0920\n",
      "Epoch [149/1000], Loss: 0.0984\n",
      "Epoch [149/1000], Loss: 0.1101\n",
      "Epoch [149/1000], Loss: 0.1015\n",
      "Epoch [149/1000], Loss: 0.1306\n",
      "Epoch [149/1000], Loss: 0.1983\n",
      "Epoch [149/1000], Loss: 0.1353\n",
      "tensor(2.4923, grad_fn=<MeanBackward0>)\n",
      "149\n",
      "Epoch [150/1000], Loss: 0.1782\n",
      "Epoch [150/1000], Loss: 0.2006\n",
      "Epoch [150/1000], Loss: 0.1765\n",
      "Epoch [150/1000], Loss: 0.1541\n",
      "Epoch [150/1000], Loss: 0.1105\n",
      "Epoch [150/1000], Loss: 0.1398\n",
      "Epoch [150/1000], Loss: 0.1686\n",
      "Epoch [150/1000], Loss: 0.1114\n",
      "Epoch [150/1000], Loss: 0.1852\n",
      "Epoch [150/1000], Loss: 0.2572\n",
      "Epoch [150/1000], Loss: 0.1212\n",
      "tensor(2.6101, grad_fn=<MeanBackward0>)\n",
      "150\n",
      "Epoch [151/1000], Loss: 0.3048\n",
      "Epoch [151/1000], Loss: 0.3212\n",
      "Epoch [151/1000], Loss: 0.1920\n",
      "Epoch [151/1000], Loss: 0.2642\n",
      "Epoch [151/1000], Loss: 0.2582\n",
      "Epoch [151/1000], Loss: 0.1425\n",
      "Epoch [151/1000], Loss: 0.2676\n",
      "Epoch [151/1000], Loss: 0.3268\n",
      "Epoch [151/1000], Loss: 0.1974\n",
      "Epoch [151/1000], Loss: 0.2887\n",
      "Epoch [151/1000], Loss: 0.3596\n",
      "tensor(2.3931, grad_fn=<MeanBackward0>)\n",
      "151\n",
      "Epoch [152/1000], Loss: 0.2980\n",
      "Epoch [152/1000], Loss: 0.2810\n",
      "Epoch [152/1000], Loss: 0.3885\n",
      "Epoch [152/1000], Loss: 0.2878\n",
      "Epoch [152/1000], Loss: 0.1634\n",
      "Epoch [152/1000], Loss: 0.3418\n",
      "Epoch [152/1000], Loss: 0.3423\n",
      "Epoch [152/1000], Loss: 0.2045\n",
      "Epoch [152/1000], Loss: 0.2298\n",
      "Epoch [152/1000], Loss: 0.4363\n",
      "Epoch [152/1000], Loss: 0.1744\n",
      "tensor(2.4772, grad_fn=<MeanBackward0>)\n",
      "152\n",
      "Epoch [153/1000], Loss: 0.2099\n",
      "Epoch [153/1000], Loss: 0.2615\n",
      "Epoch [153/1000], Loss: 0.1922\n",
      "Epoch [153/1000], Loss: 0.1111\n",
      "Epoch [153/1000], Loss: 0.1432\n",
      "Epoch [153/1000], Loss: 0.1673\n",
      "Epoch [153/1000], Loss: 0.1253\n",
      "Epoch [153/1000], Loss: 0.1705\n",
      "Epoch [153/1000], Loss: 0.1606\n",
      "Epoch [153/1000], Loss: 0.2209\n",
      "Epoch [153/1000], Loss: 0.2368\n",
      "tensor(2.4882, grad_fn=<MeanBackward0>)\n",
      "153\n",
      "Epoch [154/1000], Loss: 0.1785\n",
      "Epoch [154/1000], Loss: 0.3248\n",
      "Epoch [154/1000], Loss: 0.3221\n",
      "Epoch [154/1000], Loss: 0.1465\n",
      "Epoch [154/1000], Loss: 0.2970\n",
      "Epoch [154/1000], Loss: 0.3985\n",
      "Epoch [154/1000], Loss: 0.3046\n",
      "Epoch [154/1000], Loss: 0.1096\n",
      "Epoch [154/1000], Loss: 0.3747\n",
      "Epoch [154/1000], Loss: 0.5373\n",
      "Epoch [154/1000], Loss: 0.2484\n",
      "tensor(2.4464, grad_fn=<MeanBackward0>)\n",
      "154\n",
      "Epoch [155/1000], Loss: 0.2628\n",
      "Epoch [155/1000], Loss: 0.3802\n",
      "Epoch [155/1000], Loss: 0.2916\n",
      "Epoch [155/1000], Loss: 0.1745\n",
      "Epoch [155/1000], Loss: 0.1914\n",
      "Epoch [155/1000], Loss: 0.1391\n",
      "Epoch [155/1000], Loss: 0.1258\n",
      "Epoch [155/1000], Loss: 0.1167\n",
      "Epoch [155/1000], Loss: 0.1303\n",
      "Epoch [155/1000], Loss: 0.2387\n",
      "Epoch [155/1000], Loss: 0.1365\n",
      "tensor(2.4966, grad_fn=<MeanBackward0>)\n",
      "155\n",
      "Epoch [156/1000], Loss: 0.1660\n",
      "Epoch [156/1000], Loss: 0.2209\n",
      "Epoch [156/1000], Loss: 0.1781\n",
      "Epoch [156/1000], Loss: 0.2031\n",
      "Epoch [156/1000], Loss: 0.1612\n",
      "Epoch [156/1000], Loss: 0.1110\n",
      "Epoch [156/1000], Loss: 0.1733\n",
      "Epoch [156/1000], Loss: 0.1271\n",
      "Epoch [156/1000], Loss: 0.1551\n",
      "Epoch [156/1000], Loss: 0.2493\n",
      "Epoch [156/1000], Loss: 0.1102\n",
      "tensor(2.5984, grad_fn=<MeanBackward0>)\n",
      "156\n",
      "Epoch [157/1000], Loss: 0.2617\n",
      "Epoch [157/1000], Loss: 0.2787\n",
      "Epoch [157/1000], Loss: 0.1658\n",
      "Epoch [157/1000], Loss: 0.2253\n",
      "Epoch [157/1000], Loss: 0.2255\n",
      "Epoch [157/1000], Loss: 0.1061\n",
      "Epoch [157/1000], Loss: 0.2159\n",
      "Epoch [157/1000], Loss: 0.2484\n",
      "Epoch [157/1000], Loss: 0.1535\n",
      "Epoch [157/1000], Loss: 0.2419\n",
      "Epoch [157/1000], Loss: 0.2292\n",
      "tensor(2.4516, grad_fn=<MeanBackward0>)\n",
      "157\n",
      "Epoch [158/1000], Loss: 0.1648\n",
      "Epoch [158/1000], Loss: 0.4000\n",
      "Epoch [158/1000], Loss: 0.4542\n",
      "Epoch [158/1000], Loss: 0.2207\n",
      "Epoch [158/1000], Loss: 0.2473\n",
      "Epoch [158/1000], Loss: 0.4674\n",
      "Epoch [158/1000], Loss: 0.4795\n",
      "Epoch [158/1000], Loss: 0.3545\n",
      "Epoch [158/1000], Loss: 0.1342\n",
      "Epoch [158/1000], Loss: 0.5270\n",
      "Epoch [158/1000], Loss: 0.4379\n",
      "tensor(2.6896, grad_fn=<MeanBackward0>)\n",
      "158\n",
      "Epoch [159/1000], Loss: 0.3494\n",
      "Epoch [159/1000], Loss: 0.2475\n",
      "Epoch [159/1000], Loss: 0.2787\n",
      "Epoch [159/1000], Loss: 0.2509\n",
      "Epoch [159/1000], Loss: 0.1946\n",
      "Epoch [159/1000], Loss: 0.1318\n",
      "Epoch [159/1000], Loss: 0.2128\n",
      "Epoch [159/1000], Loss: 0.1645\n",
      "Epoch [159/1000], Loss: 0.1486\n",
      "Epoch [159/1000], Loss: 0.2056\n",
      "Epoch [159/1000], Loss: 0.1798\n",
      "tensor(2.4622, grad_fn=<MeanBackward0>)\n",
      "159\n",
      "Epoch [160/1000], Loss: 0.1653\n",
      "Epoch [160/1000], Loss: 0.3078\n",
      "Epoch [160/1000], Loss: 0.3298\n",
      "Epoch [160/1000], Loss: 0.1916\n",
      "Epoch [160/1000], Loss: 0.2288\n",
      "Epoch [160/1000], Loss: 0.3741\n",
      "Epoch [160/1000], Loss: 0.3361\n",
      "Epoch [160/1000], Loss: 0.1056\n",
      "Epoch [160/1000], Loss: 0.3255\n",
      "Epoch [160/1000], Loss: 0.5226\n",
      "Epoch [160/1000], Loss: 0.2895\n",
      "tensor(2.4796, grad_fn=<MeanBackward0>)\n",
      "160\n",
      "Epoch [161/1000], Loss: 0.2342\n",
      "Epoch [161/1000], Loss: 0.4015\n",
      "Epoch [161/1000], Loss: 0.3170\n",
      "Epoch [161/1000], Loss: 0.1910\n",
      "Epoch [161/1000], Loss: 0.2003\n",
      "Epoch [161/1000], Loss: 0.1386\n",
      "Epoch [161/1000], Loss: 0.1327\n",
      "Epoch [161/1000], Loss: 0.0956\n",
      "Epoch [161/1000], Loss: 0.1416\n",
      "Epoch [161/1000], Loss: 0.2612\n",
      "Epoch [161/1000], Loss: 0.1137\n",
      "tensor(2.4049, grad_fn=<MeanBackward0>)\n",
      "161\n",
      "Epoch [162/1000], Loss: 0.1872\n",
      "Epoch [162/1000], Loss: 0.2030\n",
      "Epoch [162/1000], Loss: 0.2476\n",
      "Epoch [162/1000], Loss: 0.1290\n",
      "Epoch [162/1000], Loss: 0.1529\n",
      "Epoch [162/1000], Loss: 0.2258\n",
      "Epoch [162/1000], Loss: 0.1499\n",
      "Epoch [162/1000], Loss: 0.1582\n",
      "Epoch [162/1000], Loss: 0.2244\n",
      "Epoch [162/1000], Loss: 0.2882\n",
      "Epoch [162/1000], Loss: 0.1437\n",
      "tensor(2.3971, grad_fn=<MeanBackward0>)\n",
      "162\n",
      "Epoch [163/1000], Loss: 0.2294\n",
      "Epoch [163/1000], Loss: 0.2288\n",
      "Epoch [163/1000], Loss: 0.2503\n",
      "Epoch [163/1000], Loss: 0.2284\n",
      "Epoch [163/1000], Loss: 0.1400\n",
      "Epoch [163/1000], Loss: 0.2745\n",
      "Epoch [163/1000], Loss: 0.3672\n",
      "Epoch [163/1000], Loss: 0.2807\n",
      "Epoch [163/1000], Loss: 0.1162\n",
      "Epoch [163/1000], Loss: 0.4508\n",
      "Epoch [163/1000], Loss: 0.4015\n",
      "tensor(2.6575, grad_fn=<MeanBackward0>)\n",
      "163\n",
      "Epoch [164/1000], Loss: 0.3089\n",
      "Epoch [164/1000], Loss: 0.2177\n",
      "Epoch [164/1000], Loss: 0.2639\n",
      "Epoch [164/1000], Loss: 0.1965\n",
      "Epoch [164/1000], Loss: 0.1207\n",
      "Epoch [164/1000], Loss: 0.1142\n",
      "Epoch [164/1000], Loss: 0.1216\n",
      "Epoch [164/1000], Loss: 0.1585\n",
      "Epoch [164/1000], Loss: 0.1015\n",
      "Epoch [164/1000], Loss: 0.3299\n",
      "Epoch [164/1000], Loss: 0.2494\n",
      "tensor(2.5350, grad_fn=<MeanBackward0>)\n",
      "164\n",
      "Epoch [165/1000], Loss: 0.1616\n",
      "Epoch [165/1000], Loss: 0.2781\n",
      "Epoch [165/1000], Loss: 0.2679\n",
      "Epoch [165/1000], Loss: 0.0996\n",
      "Epoch [165/1000], Loss: 0.1685\n",
      "Epoch [165/1000], Loss: 0.1015\n",
      "Epoch [165/1000], Loss: 0.1539\n",
      "Epoch [165/1000], Loss: 0.1793\n",
      "Epoch [165/1000], Loss: 0.1248\n",
      "Epoch [165/1000], Loss: 0.3758\n",
      "Epoch [165/1000], Loss: 0.3877\n",
      "tensor(2.6762, grad_fn=<MeanBackward0>)\n",
      "165\n",
      "Epoch [166/1000], Loss: 0.3201\n",
      "Epoch [166/1000], Loss: 0.1778\n",
      "Epoch [166/1000], Loss: 0.2642\n",
      "Epoch [166/1000], Loss: 0.2826\n",
      "Epoch [166/1000], Loss: 0.1648\n",
      "Epoch [166/1000], Loss: 0.1187\n",
      "Epoch [166/1000], Loss: 0.2185\n",
      "Epoch [166/1000], Loss: 0.1026\n",
      "Epoch [166/1000], Loss: 0.1936\n",
      "Epoch [166/1000], Loss: 0.2690\n",
      "Epoch [166/1000], Loss: 0.0990\n",
      "tensor(2.5709, grad_fn=<MeanBackward0>)\n",
      "166\n",
      "Epoch [167/1000], Loss: 0.2237\n",
      "Epoch [167/1000], Loss: 0.2104\n",
      "Epoch [167/1000], Loss: 0.1982\n",
      "Epoch [167/1000], Loss: 0.2075\n",
      "Epoch [167/1000], Loss: 0.1469\n",
      "Epoch [167/1000], Loss: 0.0854\n",
      "Epoch [167/1000], Loss: 0.1655\n",
      "Epoch [167/1000], Loss: 0.1231\n",
      "Epoch [167/1000], Loss: 0.1300\n",
      "Epoch [167/1000], Loss: 0.2200\n",
      "Epoch [167/1000], Loss: 0.1127\n",
      "tensor(2.5731, grad_fn=<MeanBackward0>)\n",
      "167\n",
      "Epoch [168/1000], Loss: 0.1737\n",
      "Epoch [168/1000], Loss: 0.1901\n",
      "Epoch [168/1000], Loss: 0.2003\n",
      "Epoch [168/1000], Loss: 0.1225\n",
      "Epoch [168/1000], Loss: 0.0991\n",
      "Epoch [168/1000], Loss: 0.0904\n",
      "Epoch [168/1000], Loss: 0.1078\n",
      "Epoch [168/1000], Loss: 0.0989\n",
      "Epoch [168/1000], Loss: 0.1144\n",
      "Epoch [168/1000], Loss: 0.2185\n",
      "Epoch [168/1000], Loss: 0.0997\n",
      "tensor(2.5952, grad_fn=<MeanBackward0>)\n",
      "168\n",
      "Epoch [169/1000], Loss: 0.2284\n",
      "Epoch [169/1000], Loss: 0.3199\n",
      "Epoch [169/1000], Loss: 0.1540\n",
      "Epoch [169/1000], Loss: 0.2622\n",
      "Epoch [169/1000], Loss: 0.3475\n",
      "Epoch [169/1000], Loss: 0.3113\n",
      "Epoch [169/1000], Loss: 0.1312\n",
      "Epoch [169/1000], Loss: 0.2236\n",
      "Epoch [169/1000], Loss: 0.3780\n",
      "Epoch [169/1000], Loss: 0.3608\n",
      "Epoch [169/1000], Loss: 0.1889\n",
      "tensor(2.4541, grad_fn=<MeanBackward0>)\n",
      "169\n",
      "Epoch [170/1000], Loss: 0.2879\n",
      "Epoch [170/1000], Loss: 0.2642\n",
      "Epoch [170/1000], Loss: 0.1803\n",
      "Epoch [170/1000], Loss: 0.1527\n",
      "Epoch [170/1000], Loss: 0.1231\n",
      "Epoch [170/1000], Loss: 0.2136\n",
      "Epoch [170/1000], Loss: 0.2424\n",
      "Epoch [170/1000], Loss: 0.1644\n",
      "Epoch [170/1000], Loss: 0.1924\n",
      "Epoch [170/1000], Loss: 0.3211\n",
      "Epoch [170/1000], Loss: 0.1197\n",
      "tensor(2.4751, grad_fn=<MeanBackward0>)\n",
      "170\n",
      "Epoch [171/1000], Loss: 0.1974\n",
      "Epoch [171/1000], Loss: 0.1810\n",
      "Epoch [171/1000], Loss: 0.1929\n",
      "Epoch [171/1000], Loss: 0.1046\n",
      "Epoch [171/1000], Loss: 0.1469\n",
      "Epoch [171/1000], Loss: 0.1469\n",
      "Epoch [171/1000], Loss: 0.1169\n",
      "Epoch [171/1000], Loss: 0.1196\n",
      "Epoch [171/1000], Loss: 0.1559\n",
      "Epoch [171/1000], Loss: 0.1827\n",
      "Epoch [171/1000], Loss: 0.1138\n",
      "tensor(2.6399, grad_fn=<MeanBackward0>)\n",
      "171\n",
      "Epoch [172/1000], Loss: 0.1975\n",
      "Epoch [172/1000], Loss: 0.2969\n",
      "Epoch [172/1000], Loss: 0.2073\n",
      "Epoch [172/1000], Loss: 0.1511\n",
      "Epoch [172/1000], Loss: 0.2706\n",
      "Epoch [172/1000], Loss: 0.2988\n",
      "Epoch [172/1000], Loss: 0.1845\n",
      "Epoch [172/1000], Loss: 0.1070\n",
      "Epoch [172/1000], Loss: 0.2943\n",
      "Epoch [172/1000], Loss: 0.3112\n",
      "Epoch [172/1000], Loss: 0.1577\n",
      "tensor(2.4516, grad_fn=<MeanBackward0>)\n",
      "172\n",
      "Epoch [173/1000], Loss: 0.1995\n",
      "Epoch [173/1000], Loss: 0.1911\n",
      "Epoch [173/1000], Loss: 0.1850\n",
      "Epoch [173/1000], Loss: 0.1094\n",
      "Epoch [173/1000], Loss: 0.1589\n",
      "Epoch [173/1000], Loss: 0.2253\n",
      "Epoch [173/1000], Loss: 0.1692\n",
      "Epoch [173/1000], Loss: 0.0846\n",
      "Epoch [173/1000], Loss: 0.2164\n",
      "Epoch [173/1000], Loss: 0.3255\n",
      "Epoch [173/1000], Loss: 0.0797\n",
      "tensor(2.4094, grad_fn=<MeanBackward0>)\n",
      "173\n",
      "Epoch [174/1000], Loss: 0.2079\n",
      "Epoch [174/1000], Loss: 0.1992\n",
      "Epoch [174/1000], Loss: 0.1673\n",
      "Epoch [174/1000], Loss: 0.1361\n",
      "Epoch [174/1000], Loss: 0.1089\n",
      "Epoch [174/1000], Loss: 0.2332\n",
      "Epoch [174/1000], Loss: 0.2075\n",
      "Epoch [174/1000], Loss: 0.1238\n",
      "Epoch [174/1000], Loss: 0.2318\n",
      "Epoch [174/1000], Loss: 0.4994\n",
      "Epoch [174/1000], Loss: 0.3867\n",
      "tensor(2.6780, grad_fn=<MeanBackward0>)\n",
      "174\n",
      "Epoch [175/1000], Loss: 0.2333\n",
      "Epoch [175/1000], Loss: 0.2121\n",
      "Epoch [175/1000], Loss: 0.2844\n",
      "Epoch [175/1000], Loss: 0.2351\n",
      "Epoch [175/1000], Loss: 0.1268\n",
      "Epoch [175/1000], Loss: 0.1218\n",
      "Epoch [175/1000], Loss: 0.1145\n",
      "Epoch [175/1000], Loss: 0.2007\n",
      "Epoch [175/1000], Loss: 0.1482\n",
      "Epoch [175/1000], Loss: 0.2490\n",
      "Epoch [175/1000], Loss: 0.2392\n",
      "tensor(2.6169, grad_fn=<MeanBackward0>)\n",
      "175\n",
      "Epoch [176/1000], Loss: 0.2343\n",
      "Epoch [176/1000], Loss: 0.1718\n",
      "Epoch [176/1000], Loss: 0.2613\n",
      "Epoch [176/1000], Loss: 0.2155\n",
      "Epoch [176/1000], Loss: 0.1013\n",
      "Epoch [176/1000], Loss: 0.0874\n",
      "Epoch [176/1000], Loss: 0.0859\n",
      "Epoch [176/1000], Loss: 0.1265\n",
      "Epoch [176/1000], Loss: 0.1026\n",
      "Epoch [176/1000], Loss: 0.2621\n",
      "Epoch [176/1000], Loss: 0.1279\n",
      "tensor(2.4967, grad_fn=<MeanBackward0>)\n",
      "176\n",
      "Epoch [177/1000], Loss: 0.1418\n",
      "Epoch [177/1000], Loss: 0.1664\n",
      "Epoch [177/1000], Loss: 0.1453\n",
      "Epoch [177/1000], Loss: 0.1084\n",
      "Epoch [177/1000], Loss: 0.0791\n",
      "Epoch [177/1000], Loss: 0.1710\n",
      "Epoch [177/1000], Loss: 0.1529\n",
      "Epoch [177/1000], Loss: 0.0752\n",
      "Epoch [177/1000], Loss: 0.1805\n",
      "Epoch [177/1000], Loss: 0.3074\n",
      "Epoch [177/1000], Loss: 0.1016\n",
      "tensor(2.4474, grad_fn=<MeanBackward0>)\n",
      "177\n",
      "Epoch [178/1000], Loss: 0.1731\n",
      "Epoch [178/1000], Loss: 0.1637\n",
      "Epoch [178/1000], Loss: 0.1298\n",
      "Epoch [178/1000], Loss: 0.0865\n",
      "Epoch [178/1000], Loss: 0.1119\n",
      "Epoch [178/1000], Loss: 0.1542\n",
      "Epoch [178/1000], Loss: 0.1120\n",
      "Epoch [178/1000], Loss: 0.0711\n",
      "Epoch [178/1000], Loss: 0.0876\n",
      "Epoch [178/1000], Loss: 0.2357\n",
      "Epoch [178/1000], Loss: 0.1276\n",
      "tensor(2.5112, grad_fn=<MeanBackward0>)\n",
      "178\n",
      "Epoch [179/1000], Loss: 0.1422\n",
      "Epoch [179/1000], Loss: 0.1730\n",
      "Epoch [179/1000], Loss: 0.1790\n",
      "Epoch [179/1000], Loss: 0.0890\n",
      "Epoch [179/1000], Loss: 0.1196\n",
      "Epoch [179/1000], Loss: 0.0756\n",
      "Epoch [179/1000], Loss: 0.1442\n",
      "Epoch [179/1000], Loss: 0.1828\n",
      "Epoch [179/1000], Loss: 0.0951\n",
      "Epoch [179/1000], Loss: 0.3871\n",
      "Epoch [179/1000], Loss: 0.4252\n",
      "tensor(2.8085, grad_fn=<MeanBackward0>)\n",
      "179\n",
      "Epoch [180/1000], Loss: 0.5053\n",
      "Epoch [180/1000], Loss: 0.3716\n",
      "Epoch [180/1000], Loss: 0.2110\n",
      "Epoch [180/1000], Loss: 0.3671\n",
      "Epoch [180/1000], Loss: 0.3779\n",
      "Epoch [180/1000], Loss: 0.2938\n",
      "Epoch [180/1000], Loss: 0.1713\n",
      "Epoch [180/1000], Loss: 0.1656\n",
      "Epoch [180/1000], Loss: 0.1513\n",
      "Epoch [180/1000], Loss: 0.3026\n",
      "Epoch [180/1000], Loss: 0.2381\n",
      "tensor(2.6511, grad_fn=<MeanBackward0>)\n",
      "180\n",
      "Epoch [181/1000], Loss: 0.2905\n",
      "Epoch [181/1000], Loss: 0.2332\n",
      "Epoch [181/1000], Loss: 0.3260\n",
      "Epoch [181/1000], Loss: 0.3464\n",
      "Epoch [181/1000], Loss: 0.2811\n",
      "Epoch [181/1000], Loss: 0.1969\n",
      "Epoch [181/1000], Loss: 0.1527\n",
      "Epoch [181/1000], Loss: 0.1186\n",
      "Epoch [181/1000], Loss: 0.1037\n",
      "Epoch [181/1000], Loss: 0.2207\n",
      "Epoch [181/1000], Loss: 0.1974\n",
      "tensor(2.6999, grad_fn=<MeanBackward0>)\n",
      "181\n",
      "Epoch [182/1000], Loss: 0.2771\n",
      "Epoch [182/1000], Loss: 0.2135\n",
      "Epoch [182/1000], Loss: 0.2466\n",
      "Epoch [182/1000], Loss: 0.3129\n",
      "Epoch [182/1000], Loss: 0.2776\n",
      "Epoch [182/1000], Loss: 0.1707\n",
      "Epoch [182/1000], Loss: 0.1082\n",
      "Epoch [182/1000], Loss: 0.0870\n",
      "Epoch [182/1000], Loss: 0.0780\n",
      "Epoch [182/1000], Loss: 0.2037\n",
      "Epoch [182/1000], Loss: 0.1047\n",
      "tensor(2.7189, grad_fn=<MeanBackward0>)\n",
      "182\n",
      "Epoch [183/1000], Loss: 0.2559\n",
      "Epoch [183/1000], Loss: 0.2651\n",
      "Epoch [183/1000], Loss: 0.1834\n",
      "Epoch [183/1000], Loss: 0.2461\n",
      "Epoch [183/1000], Loss: 0.2277\n",
      "Epoch [183/1000], Loss: 0.1510\n",
      "Epoch [183/1000], Loss: 0.0912\n",
      "Epoch [183/1000], Loss: 0.0904\n",
      "Epoch [183/1000], Loss: 0.1217\n",
      "Epoch [183/1000], Loss: 0.2221\n",
      "Epoch [183/1000], Loss: 0.1769\n",
      "tensor(2.6984, grad_fn=<MeanBackward0>)\n",
      "183\n",
      "Epoch [184/1000], Loss: 0.2351\n",
      "Epoch [184/1000], Loss: 0.1876\n",
      "Epoch [184/1000], Loss: 0.1938\n",
      "Epoch [184/1000], Loss: 0.2598\n",
      "Epoch [184/1000], Loss: 0.1818\n",
      "Epoch [184/1000], Loss: 0.0747\n",
      "Epoch [184/1000], Loss: 0.0953\n",
      "Epoch [184/1000], Loss: 0.0936\n",
      "Epoch [184/1000], Loss: 0.1257\n",
      "Epoch [184/1000], Loss: 0.2021\n",
      "Epoch [184/1000], Loss: 0.2379\n",
      "tensor(2.7881, grad_fn=<MeanBackward0>)\n",
      "184\n",
      "Epoch [185/1000], Loss: 0.3685\n",
      "Epoch [185/1000], Loss: 0.3145\n",
      "Epoch [185/1000], Loss: 0.1331\n",
      "Epoch [185/1000], Loss: 0.3017\n",
      "Epoch [185/1000], Loss: 0.3446\n",
      "Epoch [185/1000], Loss: 0.3377\n",
      "Epoch [185/1000], Loss: 0.2202\n",
      "Epoch [185/1000], Loss: 0.1210\n",
      "Epoch [185/1000], Loss: 0.1879\n",
      "Epoch [185/1000], Loss: 0.2933\n",
      "Epoch [185/1000], Loss: 0.1412\n",
      "tensor(2.6556, grad_fn=<MeanBackward0>)\n",
      "185\n",
      "Epoch [186/1000], Loss: 0.1867\n",
      "Epoch [186/1000], Loss: 0.2217\n",
      "Epoch [186/1000], Loss: 0.1689\n",
      "Epoch [186/1000], Loss: 0.1670\n",
      "Epoch [186/1000], Loss: 0.1806\n",
      "Epoch [186/1000], Loss: 0.1743\n",
      "Epoch [186/1000], Loss: 0.0835\n",
      "Epoch [186/1000], Loss: 0.1104\n",
      "Epoch [186/1000], Loss: 0.1193\n",
      "Epoch [186/1000], Loss: 0.1933\n",
      "Epoch [186/1000], Loss: 0.1267\n",
      "tensor(2.8014, grad_fn=<MeanBackward0>)\n",
      "186\n",
      "Epoch [187/1000], Loss: 0.3475\n",
      "Epoch [187/1000], Loss: 0.5235\n",
      "Epoch [187/1000], Loss: 0.4326\n",
      "Epoch [187/1000], Loss: 0.1196\n",
      "Epoch [187/1000], Loss: 0.3516\n",
      "Epoch [187/1000], Loss: 0.5690\n",
      "Epoch [187/1000], Loss: 0.6132\n",
      "Epoch [187/1000], Loss: 0.6809\n",
      "Epoch [187/1000], Loss: 0.5440\n",
      "Epoch [187/1000], Loss: 0.2984\n",
      "Epoch [187/1000], Loss: 0.2825\n",
      "tensor(3.0184, grad_fn=<MeanBackward0>)\n",
      "187\n",
      "Epoch [188/1000], Loss: 0.7276\n",
      "Epoch [188/1000], Loss: 0.9919\n",
      "Epoch [188/1000], Loss: 0.9512\n",
      "Epoch [188/1000], Loss: 0.6452\n",
      "Epoch [188/1000], Loss: 0.3155\n",
      "Epoch [188/1000], Loss: 0.4456\n",
      "Epoch [188/1000], Loss: 0.6775\n",
      "Epoch [188/1000], Loss: 1.0153\n",
      "Epoch [188/1000], Loss: 1.2086\n",
      "Epoch [188/1000], Loss: 1.1061\n",
      "Epoch [188/1000], Loss: 1.0738\n",
      "tensor(2.1097, grad_fn=<MeanBackward0>)\n",
      "188\n",
      "Epoch [189/1000], Loss: 0.7340\n",
      "Epoch [189/1000], Loss: 0.4323\n",
      "Epoch [189/1000], Loss: 0.4676\n",
      "Epoch [189/1000], Loss: 0.7273\n",
      "Epoch [189/1000], Loss: 0.8700\n",
      "Epoch [189/1000], Loss: 0.7360\n",
      "Epoch [189/1000], Loss: 0.5621\n",
      "Epoch [189/1000], Loss: 0.3367\n",
      "Epoch [189/1000], Loss: 0.5219\n",
      "Epoch [189/1000], Loss: 0.6334\n",
      "Epoch [189/1000], Loss: 0.7529\n",
      "tensor(2.2220, grad_fn=<MeanBackward0>)\n",
      "189\n",
      "Epoch [190/1000], Loss: 0.6432\n",
      "Epoch [190/1000], Loss: 0.4801\n",
      "Epoch [190/1000], Loss: 0.4258\n",
      "Epoch [190/1000], Loss: 0.3746\n",
      "Epoch [190/1000], Loss: 0.5274\n",
      "Epoch [190/1000], Loss: 0.4630\n",
      "Epoch [190/1000], Loss: 0.4195\n",
      "Epoch [190/1000], Loss: 0.3059\n",
      "Epoch [190/1000], Loss: 0.3897\n",
      "Epoch [190/1000], Loss: 0.4465\n",
      "Epoch [190/1000], Loss: 0.4341\n",
      "tensor(2.5308, grad_fn=<MeanBackward0>)\n",
      "190\n",
      "Epoch [191/1000], Loss: 0.2972\n",
      "Epoch [191/1000], Loss: 0.4317\n",
      "Epoch [191/1000], Loss: 0.5820\n",
      "Epoch [191/1000], Loss: 0.4883\n",
      "Epoch [191/1000], Loss: 0.3517\n",
      "Epoch [191/1000], Loss: 0.1909\n",
      "Epoch [191/1000], Loss: 0.3719\n",
      "Epoch [191/1000], Loss: 0.6492\n",
      "Epoch [191/1000], Loss: 0.7610\n",
      "Epoch [191/1000], Loss: 0.6114\n",
      "Epoch [191/1000], Loss: 0.4590\n",
      "tensor(2.6476, grad_fn=<MeanBackward0>)\n",
      "191\n",
      "Epoch [192/1000], Loss: 0.2415\n",
      "Epoch [192/1000], Loss: 0.6417\n",
      "Epoch [192/1000], Loss: 0.9129\n",
      "Epoch [192/1000], Loss: 0.8331\n",
      "Epoch [192/1000], Loss: 0.6885\n",
      "Epoch [192/1000], Loss: 0.3114\n",
      "Epoch [192/1000], Loss: 0.3069\n",
      "Epoch [192/1000], Loss: 0.6353\n",
      "Epoch [192/1000], Loss: 0.9025\n",
      "Epoch [192/1000], Loss: 0.8680\n",
      "Epoch [192/1000], Loss: 0.9316\n",
      "tensor(2.1107, grad_fn=<MeanBackward0>)\n",
      "192\n",
      "Epoch [193/1000], Loss: 0.6849\n",
      "Epoch [193/1000], Loss: 0.4466\n",
      "Epoch [193/1000], Loss: 0.3678\n",
      "Epoch [193/1000], Loss: 0.4529\n",
      "Epoch [193/1000], Loss: 0.6381\n",
      "Epoch [193/1000], Loss: 0.5594\n",
      "Epoch [193/1000], Loss: 0.4509\n",
      "Epoch [193/1000], Loss: 0.2596\n",
      "Epoch [193/1000], Loss: 0.4115\n",
      "Epoch [193/1000], Loss: 0.5331\n",
      "Epoch [193/1000], Loss: 0.5944\n",
      "tensor(2.3045, grad_fn=<MeanBackward0>)\n",
      "193\n",
      "Epoch [194/1000], Loss: 0.4446\n",
      "Epoch [194/1000], Loss: 0.2992\n",
      "Epoch [194/1000], Loss: 0.3378\n",
      "Epoch [194/1000], Loss: 0.4182\n",
      "Epoch [194/1000], Loss: 0.4440\n",
      "Epoch [194/1000], Loss: 0.2990\n",
      "Epoch [194/1000], Loss: 0.2005\n",
      "Epoch [194/1000], Loss: 0.3918\n",
      "Epoch [194/1000], Loss: 0.5751\n",
      "Epoch [194/1000], Loss: 0.5413\n",
      "Epoch [194/1000], Loss: 0.4584\n",
      "tensor(2.5083, grad_fn=<MeanBackward0>)\n",
      "194\n",
      "Epoch [195/1000], Loss: 0.2294\n",
      "Epoch [195/1000], Loss: 0.4595\n",
      "Epoch [195/1000], Loss: 0.6973\n",
      "Epoch [195/1000], Loss: 0.6053\n",
      "Epoch [195/1000], Loss: 0.4547\n",
      "Epoch [195/1000], Loss: 0.1787\n",
      "Epoch [195/1000], Loss: 0.2352\n",
      "Epoch [195/1000], Loss: 0.5840\n",
      "Epoch [195/1000], Loss: 0.7503\n",
      "Epoch [195/1000], Loss: 0.6761\n",
      "Epoch [195/1000], Loss: 0.6301\n",
      "tensor(2.3559, grad_fn=<MeanBackward0>)\n",
      "195\n",
      "Epoch [196/1000], Loss: 0.3442\n",
      "Epoch [196/1000], Loss: 0.3230\n",
      "Epoch [196/1000], Loss: 0.5625\n",
      "Epoch [196/1000], Loss: 0.6065\n",
      "Epoch [196/1000], Loss: 0.5855\n",
      "Epoch [196/1000], Loss: 0.3738\n",
      "Epoch [196/1000], Loss: 0.2544\n",
      "Epoch [196/1000], Loss: 0.3853\n",
      "Epoch [196/1000], Loss: 0.6211\n",
      "Epoch [196/1000], Loss: 0.6617\n",
      "Epoch [196/1000], Loss: 0.7301\n",
      "tensor(2.2024, grad_fn=<MeanBackward0>)\n",
      "196\n",
      "Epoch [197/1000], Loss: 0.5256\n",
      "Epoch [197/1000], Loss: 0.3276\n",
      "Epoch [197/1000], Loss: 0.3134\n",
      "Epoch [197/1000], Loss: 0.4728\n",
      "Epoch [197/1000], Loss: 0.5679\n",
      "Epoch [197/1000], Loss: 0.4669\n",
      "Epoch [197/1000], Loss: 0.3211\n",
      "Epoch [197/1000], Loss: 0.2245\n",
      "Epoch [197/1000], Loss: 0.4615\n",
      "Epoch [197/1000], Loss: 0.5633\n",
      "Epoch [197/1000], Loss: 0.6373\n",
      "tensor(2.2444, grad_fn=<MeanBackward0>)\n",
      "197\n",
      "Epoch [198/1000], Loss: 0.4373\n",
      "Epoch [198/1000], Loss: 0.2701\n",
      "Epoch [198/1000], Loss: 0.3178\n",
      "Epoch [198/1000], Loss: 0.4246\n",
      "Epoch [198/1000], Loss: 0.4574\n",
      "Epoch [198/1000], Loss: 0.3089\n",
      "Epoch [198/1000], Loss: 0.1871\n",
      "Epoch [198/1000], Loss: 0.3011\n",
      "Epoch [198/1000], Loss: 0.5255\n",
      "Epoch [198/1000], Loss: 0.5403\n",
      "Epoch [198/1000], Loss: 0.5468\n",
      "tensor(2.3264, grad_fn=<MeanBackward0>)\n",
      "198\n",
      "Epoch [199/1000], Loss: 0.3348\n",
      "Epoch [199/1000], Loss: 0.2406\n",
      "Epoch [199/1000], Loss: 0.3972\n",
      "Epoch [199/1000], Loss: 0.4368\n",
      "Epoch [199/1000], Loss: 0.4195\n",
      "Epoch [199/1000], Loss: 0.2314\n",
      "Epoch [199/1000], Loss: 0.1508\n",
      "Epoch [199/1000], Loss: 0.3424\n",
      "Epoch [199/1000], Loss: 0.5089\n",
      "Epoch [199/1000], Loss: 0.4971\n",
      "Epoch [199/1000], Loss: 0.4468\n",
      "tensor(2.4053, grad_fn=<MeanBackward0>)\n",
      "199\n",
      "Epoch [200/1000], Loss: 0.2375\n",
      "Epoch [200/1000], Loss: 0.3160\n",
      "Epoch [200/1000], Loss: 0.4411\n",
      "Epoch [200/1000], Loss: 0.4038\n",
      "Epoch [200/1000], Loss: 0.3268\n",
      "Epoch [200/1000], Loss: 0.1470\n",
      "Epoch [200/1000], Loss: 0.1608\n",
      "Epoch [200/1000], Loss: 0.3453\n",
      "Epoch [200/1000], Loss: 0.4394\n",
      "Epoch [200/1000], Loss: 0.3933\n",
      "Epoch [200/1000], Loss: 0.2747\n",
      "tensor(2.5687, grad_fn=<MeanBackward0>)\n",
      "200\n",
      "Epoch [201/1000], Loss: 0.2083\n",
      "Epoch [201/1000], Loss: 0.4033\n",
      "Epoch [201/1000], Loss: 0.4660\n",
      "Epoch [201/1000], Loss: 0.3346\n",
      "Epoch [201/1000], Loss: 0.2155\n",
      "Epoch [201/1000], Loss: 0.1618\n",
      "Epoch [201/1000], Loss: 0.2364\n",
      "Epoch [201/1000], Loss: 0.3300\n",
      "Epoch [201/1000], Loss: 0.3364\n",
      "Epoch [201/1000], Loss: 0.2603\n",
      "Epoch [201/1000], Loss: 0.1560\n",
      "tensor(2.6779, grad_fn=<MeanBackward0>)\n",
      "201\n",
      "Epoch [202/1000], Loss: 0.3028\n",
      "Epoch [202/1000], Loss: 0.3857\n",
      "Epoch [202/1000], Loss: 0.3059\n",
      "Epoch [202/1000], Loss: 0.1272\n",
      "Epoch [202/1000], Loss: 0.1717\n",
      "Epoch [202/1000], Loss: 0.2606\n",
      "Epoch [202/1000], Loss: 0.2283\n",
      "Epoch [202/1000], Loss: 0.2194\n",
      "Epoch [202/1000], Loss: 0.1657\n",
      "Epoch [202/1000], Loss: 0.2217\n",
      "Epoch [202/1000], Loss: 0.1762\n",
      "tensor(2.6519, grad_fn=<MeanBackward0>)\n",
      "202\n",
      "Epoch [203/1000], Loss: 0.2239\n",
      "Epoch [203/1000], Loss: 0.1795\n",
      "Epoch [203/1000], Loss: 0.1970\n",
      "Epoch [203/1000], Loss: 0.1511\n",
      "Epoch [203/1000], Loss: 0.1126\n",
      "Epoch [203/1000], Loss: 0.0934\n",
      "Epoch [203/1000], Loss: 0.0999\n",
      "Epoch [203/1000], Loss: 0.1111\n",
      "Epoch [203/1000], Loss: 0.1302\n",
      "Epoch [203/1000], Loss: 0.1812\n",
      "Epoch [203/1000], Loss: 0.0890\n",
      "tensor(2.6428, grad_fn=<MeanBackward0>)\n",
      "203\n",
      "Epoch [204/1000], Loss: 0.2088\n",
      "Epoch [204/1000], Loss: 0.2358\n",
      "Epoch [204/1000], Loss: 0.1815\n",
      "Epoch [204/1000], Loss: 0.1162\n",
      "Epoch [204/1000], Loss: 0.1321\n",
      "Epoch [204/1000], Loss: 0.1305\n",
      "Epoch [204/1000], Loss: 0.0827\n",
      "Epoch [204/1000], Loss: 0.1056\n",
      "Epoch [204/1000], Loss: 0.1131\n",
      "Epoch [204/1000], Loss: 0.1840\n",
      "Epoch [204/1000], Loss: 0.0934\n",
      "tensor(2.6457, grad_fn=<MeanBackward0>)\n",
      "204\n",
      "Epoch [205/1000], Loss: 0.1851\n",
      "Epoch [205/1000], Loss: 0.2695\n",
      "Epoch [205/1000], Loss: 0.2110\n",
      "Epoch [205/1000], Loss: 0.1033\n",
      "Epoch [205/1000], Loss: 0.1620\n",
      "Epoch [205/1000], Loss: 0.2287\n",
      "Epoch [205/1000], Loss: 0.1599\n",
      "Epoch [205/1000], Loss: 0.0921\n",
      "Epoch [205/1000], Loss: 0.1298\n",
      "Epoch [205/1000], Loss: 0.1849\n",
      "Epoch [205/1000], Loss: 0.1123\n",
      "tensor(2.5348, grad_fn=<MeanBackward0>)\n",
      "205\n",
      "Epoch [206/1000], Loss: 0.1607\n",
      "Epoch [206/1000], Loss: 0.2220\n",
      "Epoch [206/1000], Loss: 0.2962\n",
      "Epoch [206/1000], Loss: 0.1754\n",
      "Epoch [206/1000], Loss: 0.1011\n",
      "Epoch [206/1000], Loss: 0.2679\n",
      "Epoch [206/1000], Loss: 0.3296\n",
      "Epoch [206/1000], Loss: 0.3198\n",
      "Epoch [206/1000], Loss: 0.2371\n",
      "Epoch [206/1000], Loss: 0.1645\n",
      "Epoch [206/1000], Loss: 0.2435\n",
      "tensor(2.7569, grad_fn=<MeanBackward0>)\n",
      "206\n",
      "Epoch [207/1000], Loss: 0.3046\n",
      "Epoch [207/1000], Loss: 0.2387\n",
      "Epoch [207/1000], Loss: 0.1468\n",
      "Epoch [207/1000], Loss: 0.1304\n",
      "Epoch [207/1000], Loss: 0.0968\n",
      "Epoch [207/1000], Loss: 0.0892\n",
      "Epoch [207/1000], Loss: 0.0688\n",
      "Epoch [207/1000], Loss: 0.1055\n",
      "Epoch [207/1000], Loss: 0.1038\n",
      "Epoch [207/1000], Loss: 0.1460\n",
      "Epoch [207/1000], Loss: 0.1006\n",
      "tensor(2.6406, grad_fn=<MeanBackward0>)\n",
      "207\n",
      "Epoch [208/1000], Loss: 0.1465\n",
      "Epoch [208/1000], Loss: 0.1535\n",
      "Epoch [208/1000], Loss: 0.1407\n",
      "Epoch [208/1000], Loss: 0.0758\n",
      "Epoch [208/1000], Loss: 0.0684\n",
      "Epoch [208/1000], Loss: 0.1297\n",
      "Epoch [208/1000], Loss: 0.1158\n",
      "Epoch [208/1000], Loss: 0.0845\n",
      "Epoch [208/1000], Loss: 0.0949\n",
      "Epoch [208/1000], Loss: 0.1357\n",
      "Epoch [208/1000], Loss: 0.0878\n",
      "tensor(2.6178, grad_fn=<MeanBackward0>)\n",
      "208\n",
      "Epoch [209/1000], Loss: 0.1280\n",
      "Epoch [209/1000], Loss: 0.2424\n",
      "Epoch [209/1000], Loss: 0.2482\n",
      "Epoch [209/1000], Loss: 0.1075\n",
      "Epoch [209/1000], Loss: 0.1088\n",
      "Epoch [209/1000], Loss: 0.2177\n",
      "Epoch [209/1000], Loss: 0.2097\n",
      "Epoch [209/1000], Loss: 0.1320\n",
      "Epoch [209/1000], Loss: 0.0776\n",
      "Epoch [209/1000], Loss: 0.1891\n",
      "Epoch [209/1000], Loss: 0.1017\n",
      "tensor(2.5664, grad_fn=<MeanBackward0>)\n",
      "209\n",
      "Epoch [210/1000], Loss: 0.1248\n",
      "Epoch [210/1000], Loss: 0.1663\n",
      "Epoch [210/1000], Loss: 0.1624\n",
      "Epoch [210/1000], Loss: 0.1291\n",
      "Epoch [210/1000], Loss: 0.0767\n",
      "Epoch [210/1000], Loss: 0.2128\n",
      "Epoch [210/1000], Loss: 0.2928\n",
      "Epoch [210/1000], Loss: 0.3247\n",
      "Epoch [210/1000], Loss: 0.3012\n",
      "Epoch [210/1000], Loss: 0.1460\n",
      "Epoch [210/1000], Loss: 0.2484\n",
      "tensor(2.8577, grad_fn=<MeanBackward0>)\n",
      "210\n",
      "Epoch [211/1000], Loss: 0.4423\n",
      "Epoch [211/1000], Loss: 0.5019\n",
      "Epoch [211/1000], Loss: 0.3908\n",
      "Epoch [211/1000], Loss: 0.1546\n",
      "Epoch [211/1000], Loss: 0.1381\n",
      "Epoch [211/1000], Loss: 0.2815\n",
      "Epoch [211/1000], Loss: 0.3559\n",
      "Epoch [211/1000], Loss: 0.4527\n",
      "Epoch [211/1000], Loss: 0.4704\n",
      "Epoch [211/1000], Loss: 0.3544\n",
      "Epoch [211/1000], Loss: 0.1473\n",
      "tensor(2.7908, grad_fn=<MeanBackward0>)\n",
      "211\n",
      "Epoch [212/1000], Loss: 0.3814\n",
      "Epoch [212/1000], Loss: 0.6542\n",
      "Epoch [212/1000], Loss: 0.7205\n",
      "Epoch [212/1000], Loss: 0.6374\n",
      "Epoch [212/1000], Loss: 0.5345\n",
      "Epoch [212/1000], Loss: 0.2392\n",
      "Epoch [212/1000], Loss: 0.2661\n",
      "Epoch [212/1000], Loss: 0.6034\n",
      "Epoch [212/1000], Loss: 0.9378\n",
      "Epoch [212/1000], Loss: 0.9848\n",
      "Epoch [212/1000], Loss: 1.1195\n",
      "tensor(1.9771, grad_fn=<MeanBackward0>)\n",
      "212\n",
      "Epoch [213/1000], Loss: 0.9067\n",
      "Epoch [213/1000], Loss: 0.6621\n",
      "Epoch [213/1000], Loss: 0.3925\n",
      "Epoch [213/1000], Loss: 0.2832\n",
      "Epoch [213/1000], Loss: 0.6805\n",
      "Epoch [213/1000], Loss: 0.8459\n",
      "Epoch [213/1000], Loss: 0.8952\n",
      "Epoch [213/1000], Loss: 0.5737\n",
      "Epoch [213/1000], Loss: 0.3418\n",
      "Epoch [213/1000], Loss: 0.4058\n",
      "Epoch [213/1000], Loss: 0.5894\n",
      "tensor(2.1013, grad_fn=<MeanBackward0>)\n",
      "213\n",
      "Epoch [214/1000], Loss: 0.7004\n",
      "Epoch [214/1000], Loss: 0.7199\n",
      "Epoch [214/1000], Loss: 0.7141\n",
      "Epoch [214/1000], Loss: 0.5013\n",
      "Epoch [214/1000], Loss: 0.3558\n",
      "Epoch [214/1000], Loss: 0.3254\n",
      "Epoch [214/1000], Loss: 0.5147\n",
      "Epoch [214/1000], Loss: 0.4503\n",
      "Epoch [214/1000], Loss: 0.3613\n",
      "Epoch [214/1000], Loss: 0.3101\n",
      "Epoch [214/1000], Loss: 0.3038\n",
      "tensor(2.3205, grad_fn=<MeanBackward0>)\n",
      "214\n",
      "Epoch [215/1000], Loss: 0.3804\n",
      "Epoch [215/1000], Loss: 0.3554\n",
      "Epoch [215/1000], Loss: 0.2547\n",
      "Epoch [215/1000], Loss: 0.1829\n",
      "Epoch [215/1000], Loss: 0.3793\n",
      "Epoch [215/1000], Loss: 0.3849\n",
      "Epoch [215/1000], Loss: 0.2620\n",
      "Epoch [215/1000], Loss: 0.1808\n",
      "Epoch [215/1000], Loss: 0.4548\n",
      "Epoch [215/1000], Loss: 0.5760\n",
      "Epoch [215/1000], Loss: 0.6180\n",
      "tensor(2.2478, grad_fn=<MeanBackward0>)\n",
      "215\n",
      "Epoch [216/1000], Loss: 0.4523\n",
      "Epoch [216/1000], Loss: 0.2285\n",
      "Epoch [216/1000], Loss: 0.3276\n",
      "Epoch [216/1000], Loss: 0.4741\n",
      "Epoch [216/1000], Loss: 0.6619\n",
      "Epoch [216/1000], Loss: 0.5668\n",
      "Epoch [216/1000], Loss: 0.4163\n",
      "Epoch [216/1000], Loss: 0.2178\n",
      "Epoch [216/1000], Loss: 0.4380\n",
      "Epoch [216/1000], Loss: 0.6011\n",
      "Epoch [216/1000], Loss: 0.7526\n",
      "tensor(2.0659, grad_fn=<MeanBackward0>)\n",
      "216\n",
      "Epoch [217/1000], Loss: 0.6820\n",
      "Epoch [217/1000], Loss: 0.5172\n",
      "Epoch [217/1000], Loss: 0.3416\n",
      "Epoch [217/1000], Loss: 0.1970\n",
      "Epoch [217/1000], Loss: 0.4936\n",
      "Epoch [217/1000], Loss: 0.6117\n",
      "Epoch [217/1000], Loss: 0.6066\n",
      "Epoch [217/1000], Loss: 0.3314\n",
      "Epoch [217/1000], Loss: 0.2265\n",
      "Epoch [217/1000], Loss: 0.3924\n",
      "Epoch [217/1000], Loss: 0.5973\n",
      "tensor(2.0442, grad_fn=<MeanBackward0>)\n",
      "217\n",
      "Epoch [218/1000], Loss: 0.6403\n",
      "Epoch [218/1000], Loss: 0.5641\n",
      "Epoch [218/1000], Loss: 0.4779\n",
      "Epoch [218/1000], Loss: 0.2079\n",
      "Epoch [218/1000], Loss: 0.2995\n",
      "Epoch [218/1000], Loss: 0.5029\n",
      "Epoch [218/1000], Loss: 0.6285\n",
      "Epoch [218/1000], Loss: 0.5044\n",
      "Epoch [218/1000], Loss: 0.2898\n",
      "Epoch [218/1000], Loss: 0.2742\n",
      "Epoch [218/1000], Loss: 0.4136\n",
      "tensor(2.1764, grad_fn=<MeanBackward0>)\n",
      "218\n",
      "Epoch [219/1000], Loss: 0.5218\n",
      "Epoch [219/1000], Loss: 0.4814\n",
      "Epoch [219/1000], Loss: 0.3970\n",
      "Epoch [219/1000], Loss: 0.1831\n",
      "Epoch [219/1000], Loss: 0.2897\n",
      "Epoch [219/1000], Loss: 0.4432\n",
      "Epoch [219/1000], Loss: 0.5137\n",
      "Epoch [219/1000], Loss: 0.3355\n",
      "Epoch [219/1000], Loss: 0.1843\n",
      "Epoch [219/1000], Loss: 0.2934\n",
      "Epoch [219/1000], Loss: 0.4720\n",
      "tensor(2.0980, grad_fn=<MeanBackward0>)\n",
      "219\n",
      "Epoch [220/1000], Loss: 0.5200\n",
      "Epoch [220/1000], Loss: 0.4181\n",
      "Epoch [220/1000], Loss: 0.3316\n",
      "Epoch [220/1000], Loss: 0.1275\n",
      "Epoch [220/1000], Loss: 0.3275\n",
      "Epoch [220/1000], Loss: 0.4115\n",
      "Epoch [220/1000], Loss: 0.4206\n",
      "Epoch [220/1000], Loss: 0.2544\n",
      "Epoch [220/1000], Loss: 0.1908\n",
      "Epoch [220/1000], Loss: 0.3301\n",
      "Epoch [220/1000], Loss: 0.4171\n",
      "tensor(2.1889, grad_fn=<MeanBackward0>)\n",
      "220\n",
      "Epoch [221/1000], Loss: 0.4244\n",
      "Epoch [221/1000], Loss: 0.2902\n",
      "Epoch [221/1000], Loss: 0.1952\n",
      "Epoch [221/1000], Loss: 0.1922\n",
      "Epoch [221/1000], Loss: 0.3460\n",
      "Epoch [221/1000], Loss: 0.3222\n",
      "Epoch [221/1000], Loss: 0.1981\n",
      "Epoch [221/1000], Loss: 0.1832\n",
      "Epoch [221/1000], Loss: 0.3269\n",
      "Epoch [221/1000], Loss: 0.3457\n",
      "Epoch [221/1000], Loss: 0.3038\n",
      "tensor(2.3820, grad_fn=<MeanBackward0>)\n",
      "221\n",
      "Epoch [222/1000], Loss: 0.1960\n",
      "Epoch [222/1000], Loss: 0.2462\n",
      "Epoch [222/1000], Loss: 0.3319\n",
      "Epoch [222/1000], Loss: 0.2284\n",
      "Epoch [222/1000], Loss: 0.1794\n",
      "Epoch [222/1000], Loss: 0.1320\n",
      "Epoch [222/1000], Loss: 0.1424\n",
      "Epoch [222/1000], Loss: 0.2207\n",
      "Epoch [222/1000], Loss: 0.2134\n",
      "Epoch [222/1000], Loss: 0.1888\n",
      "Epoch [222/1000], Loss: 0.1328\n",
      "tensor(2.5006, grad_fn=<MeanBackward0>)\n",
      "222\n",
      "Epoch [223/1000], Loss: 0.1884\n",
      "Epoch [223/1000], Loss: 0.2336\n",
      "Epoch [223/1000], Loss: 0.1908\n",
      "Epoch [223/1000], Loss: 0.1186\n",
      "Epoch [223/1000], Loss: 0.1091\n",
      "Epoch [223/1000], Loss: 0.0953\n",
      "Epoch [223/1000], Loss: 0.0827\n",
      "Epoch [223/1000], Loss: 0.1069\n",
      "Epoch [223/1000], Loss: 0.1141\n",
      "Epoch [223/1000], Loss: 0.1213\n",
      "Epoch [223/1000], Loss: 0.0813\n",
      "tensor(2.4890, grad_fn=<MeanBackward0>)\n",
      "223\n",
      "Epoch [224/1000], Loss: 0.1137\n",
      "Epoch [224/1000], Loss: 0.1338\n",
      "Epoch [224/1000], Loss: 0.1393\n",
      "Epoch [224/1000], Loss: 0.0832\n",
      "Epoch [224/1000], Loss: 0.0979\n",
      "Epoch [224/1000], Loss: 0.0789\n",
      "Epoch [224/1000], Loss: 0.0968\n",
      "Epoch [224/1000], Loss: 0.1299\n",
      "Epoch [224/1000], Loss: 0.1225\n",
      "Epoch [224/1000], Loss: 0.1335\n",
      "Epoch [224/1000], Loss: 0.1026\n",
      "tensor(2.4903, grad_fn=<MeanBackward0>)\n",
      "224\n",
      "Epoch [225/1000], Loss: 0.1092\n",
      "Epoch [225/1000], Loss: 0.1367\n",
      "Epoch [225/1000], Loss: 0.1480\n",
      "Epoch [225/1000], Loss: 0.0828\n",
      "Epoch [225/1000], Loss: 0.0936\n",
      "Epoch [225/1000], Loss: 0.0719\n",
      "Epoch [225/1000], Loss: 0.0905\n",
      "Epoch [225/1000], Loss: 0.1166\n",
      "Epoch [225/1000], Loss: 0.1389\n",
      "Epoch [225/1000], Loss: 0.1243\n",
      "Epoch [225/1000], Loss: 0.0956\n",
      "tensor(2.5230, grad_fn=<MeanBackward0>)\n",
      "225\n",
      "Epoch [226/1000], Loss: 0.0997\n",
      "Epoch [226/1000], Loss: 0.1288\n",
      "Epoch [226/1000], Loss: 0.1596\n",
      "Epoch [226/1000], Loss: 0.1142\n",
      "Epoch [226/1000], Loss: 0.1185\n",
      "Epoch [226/1000], Loss: 0.0869\n",
      "Epoch [226/1000], Loss: 0.0578\n",
      "Epoch [226/1000], Loss: 0.1313\n",
      "Epoch [226/1000], Loss: 0.1753\n",
      "Epoch [226/1000], Loss: 0.1384\n",
      "Epoch [226/1000], Loss: 0.1523\n",
      "tensor(2.5747, grad_fn=<MeanBackward0>)\n",
      "226\n",
      "Epoch [227/1000], Loss: 0.1437\n",
      "Epoch [227/1000], Loss: 0.1354\n",
      "Epoch [227/1000], Loss: 0.1052\n",
      "Epoch [227/1000], Loss: 0.0754\n",
      "Epoch [227/1000], Loss: 0.1234\n",
      "Epoch [227/1000], Loss: 0.1417\n",
      "Epoch [227/1000], Loss: 0.1337\n",
      "Epoch [227/1000], Loss: 0.1343\n",
      "Epoch [227/1000], Loss: 0.1985\n",
      "Epoch [227/1000], Loss: 0.2763\n",
      "Epoch [227/1000], Loss: 0.2164\n",
      "tensor(2.5607, grad_fn=<MeanBackward0>)\n",
      "227\n",
      "Epoch [228/1000], Loss: 0.1231\n",
      "Epoch [228/1000], Loss: 0.3055\n",
      "Epoch [228/1000], Loss: 0.4294\n",
      "Epoch [228/1000], Loss: 0.3338\n",
      "Epoch [228/1000], Loss: 0.2387\n",
      "Epoch [228/1000], Loss: 0.1231\n",
      "Epoch [228/1000], Loss: 0.1469\n",
      "Epoch [228/1000], Loss: 0.2364\n",
      "Epoch [228/1000], Loss: 0.2148\n",
      "Epoch [228/1000], Loss: 0.2227\n",
      "Epoch [228/1000], Loss: 0.1310\n",
      "tensor(2.5789, grad_fn=<MeanBackward0>)\n",
      "228\n",
      "Epoch [229/1000], Loss: 0.2180\n",
      "Epoch [229/1000], Loss: 0.3357\n",
      "Epoch [229/1000], Loss: 0.3133\n",
      "Epoch [229/1000], Loss: 0.1364\n",
      "Epoch [229/1000], Loss: 0.1111\n",
      "Epoch [229/1000], Loss: 0.1799\n",
      "Epoch [229/1000], Loss: 0.1011\n",
      "Epoch [229/1000], Loss: 0.1018\n",
      "Epoch [229/1000], Loss: 0.1016\n",
      "Epoch [229/1000], Loss: 0.1243\n",
      "Epoch [229/1000], Loss: 0.0932\n",
      "tensor(2.5265, grad_fn=<MeanBackward0>)\n",
      "229\n",
      "Epoch [230/1000], Loss: 0.0889\n",
      "Epoch [230/1000], Loss: 0.1907\n",
      "Epoch [230/1000], Loss: 0.2269\n",
      "Epoch [230/1000], Loss: 0.0884\n",
      "Epoch [230/1000], Loss: 0.1084\n",
      "Epoch [230/1000], Loss: 0.1553\n",
      "Epoch [230/1000], Loss: 0.0821\n",
      "Epoch [230/1000], Loss: 0.0967\n",
      "Epoch [230/1000], Loss: 0.0938\n",
      "Epoch [230/1000], Loss: 0.1586\n",
      "Epoch [230/1000], Loss: 0.0831\n",
      "tensor(2.5769, grad_fn=<MeanBackward0>)\n",
      "230\n",
      "Epoch [231/1000], Loss: 0.0992\n",
      "Epoch [231/1000], Loss: 0.1906\n",
      "Epoch [231/1000], Loss: 0.2203\n",
      "Epoch [231/1000], Loss: 0.1024\n",
      "Epoch [231/1000], Loss: 0.0807\n",
      "Epoch [231/1000], Loss: 0.1157\n",
      "Epoch [231/1000], Loss: 0.0727\n",
      "Epoch [231/1000], Loss: 0.0765\n",
      "Epoch [231/1000], Loss: 0.0616\n",
      "Epoch [231/1000], Loss: 0.1158\n",
      "Epoch [231/1000], Loss: 0.1072\n",
      "tensor(2.5471, grad_fn=<MeanBackward0>)\n",
      "231\n",
      "Epoch [232/1000], Loss: 0.0929\n",
      "Epoch [232/1000], Loss: 0.1835\n",
      "Epoch [232/1000], Loss: 0.2715\n",
      "Epoch [232/1000], Loss: 0.1990\n",
      "Epoch [232/1000], Loss: 0.1261\n",
      "Epoch [232/1000], Loss: 0.1413\n",
      "Epoch [232/1000], Loss: 0.1999\n",
      "Epoch [232/1000], Loss: 0.2381\n",
      "Epoch [232/1000], Loss: 0.1454\n",
      "Epoch [232/1000], Loss: 0.0929\n",
      "Epoch [232/1000], Loss: 0.0833\n",
      "tensor(2.5133, grad_fn=<MeanBackward0>)\n",
      "232\n",
      "Epoch [233/1000], Loss: 0.0968\n",
      "Epoch [233/1000], Loss: 0.1213\n",
      "Epoch [233/1000], Loss: 0.2170\n",
      "Epoch [233/1000], Loss: 0.1929\n",
      "Epoch [233/1000], Loss: 0.1698\n",
      "Epoch [233/1000], Loss: 0.0886\n",
      "Epoch [233/1000], Loss: 0.1748\n",
      "Epoch [233/1000], Loss: 0.3211\n",
      "Epoch [233/1000], Loss: 0.3698\n",
      "Epoch [233/1000], Loss: 0.3153\n",
      "Epoch [233/1000], Loss: 0.1917\n",
      "tensor(2.5339, grad_fn=<MeanBackward0>)\n",
      "233\n",
      "Epoch [234/1000], Loss: 0.1659\n",
      "Epoch [234/1000], Loss: 0.3167\n",
      "Epoch [234/1000], Loss: 0.4111\n",
      "Epoch [234/1000], Loss: 0.3518\n",
      "Epoch [234/1000], Loss: 0.2698\n",
      "Epoch [234/1000], Loss: 0.1098\n",
      "Epoch [234/1000], Loss: 0.1568\n",
      "Epoch [234/1000], Loss: 0.4045\n",
      "Epoch [234/1000], Loss: 0.5226\n",
      "Epoch [234/1000], Loss: 0.4726\n",
      "Epoch [234/1000], Loss: 0.4618\n",
      "tensor(2.3644, grad_fn=<MeanBackward0>)\n",
      "234\n",
      "Epoch [235/1000], Loss: 0.3099\n",
      "Epoch [235/1000], Loss: 0.1761\n",
      "Epoch [235/1000], Loss: 0.4066\n",
      "Epoch [235/1000], Loss: 0.6091\n",
      "Epoch [235/1000], Loss: 0.7448\n",
      "Epoch [235/1000], Loss: 0.7150\n",
      "Epoch [235/1000], Loss: 0.5874\n",
      "Epoch [235/1000], Loss: 0.2600\n",
      "Epoch [235/1000], Loss: 0.2631\n",
      "Epoch [235/1000], Loss: 0.5122\n",
      "Epoch [235/1000], Loss: 0.7896\n",
      "tensor(1.9238, grad_fn=<MeanBackward0>)\n",
      "235\n",
      "Epoch [236/1000], Loss: 0.8188\n",
      "Epoch [236/1000], Loss: 0.8034\n",
      "Epoch [236/1000], Loss: 0.7366\n",
      "Epoch [236/1000], Loss: 0.4227\n",
      "Epoch [236/1000], Loss: 0.2790\n",
      "Epoch [236/1000], Loss: 0.4400\n",
      "Epoch [236/1000], Loss: 0.7652\n",
      "Epoch [236/1000], Loss: 0.7180\n",
      "Epoch [236/1000], Loss: 0.7030\n",
      "Epoch [236/1000], Loss: 0.5029\n",
      "Epoch [236/1000], Loss: 0.2778\n",
      "tensor(2.1643, grad_fn=<MeanBackward0>)\n",
      "236\n",
      "Epoch [237/1000], Loss: 0.4442\n",
      "Epoch [237/1000], Loss: 0.5953\n",
      "Epoch [237/1000], Loss: 0.6856\n",
      "Epoch [237/1000], Loss: 0.5557\n",
      "Epoch [237/1000], Loss: 0.3195\n",
      "Epoch [237/1000], Loss: 0.1591\n",
      "Epoch [237/1000], Loss: 0.4189\n",
      "Epoch [237/1000], Loss: 0.4448\n",
      "Epoch [237/1000], Loss: 0.4006\n",
      "Epoch [237/1000], Loss: 0.2939\n",
      "Epoch [237/1000], Loss: 0.2278\n",
      "tensor(2.1972, grad_fn=<MeanBackward0>)\n",
      "237\n",
      "Epoch [238/1000], Loss: 0.4153\n",
      "Epoch [238/1000], Loss: 0.5123\n",
      "Epoch [238/1000], Loss: 0.4846\n",
      "Epoch [238/1000], Loss: 0.3055\n",
      "Epoch [238/1000], Loss: 0.1878\n",
      "Epoch [238/1000], Loss: 0.3623\n",
      "Epoch [238/1000], Loss: 0.5222\n",
      "Epoch [238/1000], Loss: 0.4179\n",
      "Epoch [238/1000], Loss: 0.2115\n",
      "Epoch [238/1000], Loss: 0.2297\n",
      "Epoch [238/1000], Loss: 0.4171\n",
      "tensor(2.0515, grad_fn=<MeanBackward0>)\n",
      "238\n",
      "Epoch [239/1000], Loss: 0.5676\n",
      "Epoch [239/1000], Loss: 0.5847\n",
      "Epoch [239/1000], Loss: 0.5505\n",
      "Epoch [239/1000], Loss: 0.3055\n",
      "Epoch [239/1000], Loss: 0.1767\n",
      "Epoch [239/1000], Loss: 0.3931\n",
      "Epoch [239/1000], Loss: 0.6448\n",
      "Epoch [239/1000], Loss: 0.6098\n",
      "Epoch [239/1000], Loss: 0.4916\n",
      "Epoch [239/1000], Loss: 0.3166\n",
      "Epoch [239/1000], Loss: 0.2494\n",
      "tensor(2.0886, grad_fn=<MeanBackward0>)\n",
      "239\n",
      "Epoch [240/1000], Loss: 0.4655\n",
      "Epoch [240/1000], Loss: 0.5864\n",
      "Epoch [240/1000], Loss: 0.6604\n",
      "Epoch [240/1000], Loss: 0.5176\n",
      "Epoch [240/1000], Loss: 0.2717\n",
      "Epoch [240/1000], Loss: 0.1298\n",
      "Epoch [240/1000], Loss: 0.4246\n",
      "Epoch [240/1000], Loss: 0.5112\n",
      "Epoch [240/1000], Loss: 0.4740\n",
      "Epoch [240/1000], Loss: 0.3440\n",
      "Epoch [240/1000], Loss: 0.1547\n",
      "tensor(2.1720, grad_fn=<MeanBackward0>)\n",
      "240\n",
      "Epoch [241/1000], Loss: 0.3740\n",
      "Epoch [241/1000], Loss: 0.5188\n",
      "Epoch [241/1000], Loss: 0.5776\n",
      "Epoch [241/1000], Loss: 0.4718\n",
      "Epoch [241/1000], Loss: 0.2350\n",
      "Epoch [241/1000], Loss: 0.1506\n",
      "Epoch [241/1000], Loss: 0.4070\n",
      "Epoch [241/1000], Loss: 0.4413\n",
      "Epoch [241/1000], Loss: 0.3632\n",
      "Epoch [241/1000], Loss: 0.2496\n",
      "Epoch [241/1000], Loss: 0.1683\n",
      "tensor(2.1693, grad_fn=<MeanBackward0>)\n",
      "241\n",
      "Epoch [242/1000], Loss: 0.3634\n",
      "Epoch [242/1000], Loss: 0.4588\n",
      "Epoch [242/1000], Loss: 0.5089\n",
      "Epoch [242/1000], Loss: 0.3452\n",
      "Epoch [242/1000], Loss: 0.1671\n",
      "Epoch [242/1000], Loss: 0.1886\n",
      "Epoch [242/1000], Loss: 0.4078\n",
      "Epoch [242/1000], Loss: 0.4082\n",
      "Epoch [242/1000], Loss: 0.2928\n",
      "Epoch [242/1000], Loss: 0.2022\n",
      "Epoch [242/1000], Loss: 0.1937\n",
      "tensor(2.1236, grad_fn=<MeanBackward0>)\n",
      "242\n",
      "Epoch [243/1000], Loss: 0.3700\n",
      "Epoch [243/1000], Loss: 0.4247\n",
      "Epoch [243/1000], Loss: 0.3749\n",
      "Epoch [243/1000], Loss: 0.1715\n",
      "Epoch [243/1000], Loss: 0.2073\n",
      "Epoch [243/1000], Loss: 0.3090\n",
      "Epoch [243/1000], Loss: 0.3949\n",
      "Epoch [243/1000], Loss: 0.2707\n",
      "Epoch [243/1000], Loss: 0.1279\n",
      "Epoch [243/1000], Loss: 0.2406\n",
      "Epoch [243/1000], Loss: 0.2733\n",
      "tensor(2.1545, grad_fn=<MeanBackward0>)\n",
      "243\n",
      "Epoch [244/1000], Loss: 0.3438\n",
      "Epoch [244/1000], Loss: 0.3123\n",
      "Epoch [244/1000], Loss: 0.2647\n",
      "Epoch [244/1000], Loss: 0.1142\n",
      "Epoch [244/1000], Loss: 0.1995\n",
      "Epoch [244/1000], Loss: 0.2244\n",
      "Epoch [244/1000], Loss: 0.2581\n",
      "Epoch [244/1000], Loss: 0.1434\n",
      "Epoch [244/1000], Loss: 0.1731\n",
      "Epoch [244/1000], Loss: 0.2031\n",
      "Epoch [244/1000], Loss: 0.1786\n",
      "tensor(2.2487, grad_fn=<MeanBackward0>)\n",
      "244\n",
      "Epoch [245/1000], Loss: 0.1918\n",
      "Epoch [245/1000], Loss: 0.1693\n",
      "Epoch [245/1000], Loss: 0.1772\n",
      "Epoch [245/1000], Loss: 0.1733\n",
      "Epoch [245/1000], Loss: 0.1564\n",
      "Epoch [245/1000], Loss: 0.0780\n",
      "Epoch [245/1000], Loss: 0.1051\n",
      "Epoch [245/1000], Loss: 0.1433\n",
      "Epoch [245/1000], Loss: 0.1491\n",
      "Epoch [245/1000], Loss: 0.1098\n",
      "Epoch [245/1000], Loss: 0.1348\n",
      "tensor(2.3867, grad_fn=<MeanBackward0>)\n",
      "245\n",
      "Epoch [246/1000], Loss: 0.1228\n",
      "Epoch [246/1000], Loss: 0.1387\n",
      "Epoch [246/1000], Loss: 0.1548\n",
      "Epoch [246/1000], Loss: 0.0849\n",
      "Epoch [246/1000], Loss: 0.1179\n",
      "Epoch [246/1000], Loss: 0.1096\n",
      "Epoch [246/1000], Loss: 0.1034\n",
      "Epoch [246/1000], Loss: 0.1176\n",
      "Epoch [246/1000], Loss: 0.1569\n",
      "Epoch [246/1000], Loss: 0.1088\n",
      "Epoch [246/1000], Loss: 0.1047\n",
      "tensor(2.4066, grad_fn=<MeanBackward0>)\n",
      "246\n",
      "Epoch [247/1000], Loss: 0.1037\n",
      "Epoch [247/1000], Loss: 0.1041\n",
      "Epoch [247/1000], Loss: 0.1206\n",
      "Epoch [247/1000], Loss: 0.0660\n",
      "Epoch [247/1000], Loss: 0.0918\n",
      "Epoch [247/1000], Loss: 0.0771\n",
      "Epoch [247/1000], Loss: 0.0639\n",
      "Epoch [247/1000], Loss: 0.1135\n",
      "Epoch [247/1000], Loss: 0.1149\n",
      "Epoch [247/1000], Loss: 0.0867\n",
      "Epoch [247/1000], Loss: 0.0970\n",
      "tensor(2.4096, grad_fn=<MeanBackward0>)\n",
      "247\n",
      "Epoch [248/1000], Loss: 0.0813\n",
      "Epoch [248/1000], Loss: 0.0916\n",
      "Epoch [248/1000], Loss: 0.1096\n",
      "Epoch [248/1000], Loss: 0.0558\n",
      "Epoch [248/1000], Loss: 0.0731\n",
      "Epoch [248/1000], Loss: 0.0646\n",
      "Epoch [248/1000], Loss: 0.0650\n",
      "Epoch [248/1000], Loss: 0.0864\n",
      "Epoch [248/1000], Loss: 0.0906\n",
      "Epoch [248/1000], Loss: 0.0814\n",
      "Epoch [248/1000], Loss: 0.0703\n",
      "tensor(2.4128, grad_fn=<MeanBackward0>)\n",
      "248\n",
      "Epoch [249/1000], Loss: 0.0649\n",
      "Epoch [249/1000], Loss: 0.0778\n",
      "Epoch [249/1000], Loss: 0.0959\n",
      "Epoch [249/1000], Loss: 0.0479\n",
      "Epoch [249/1000], Loss: 0.0666\n",
      "Epoch [249/1000], Loss: 0.0583\n",
      "Epoch [249/1000], Loss: 0.0585\n",
      "Epoch [249/1000], Loss: 0.0931\n",
      "Epoch [249/1000], Loss: 0.0988\n",
      "Epoch [249/1000], Loss: 0.0874\n",
      "Epoch [249/1000], Loss: 0.0552\n",
      "tensor(2.4412, grad_fn=<MeanBackward0>)\n",
      "249\n",
      "Epoch [250/1000], Loss: 0.0756\n",
      "Epoch [250/1000], Loss: 0.0688\n",
      "Epoch [250/1000], Loss: 0.1036\n",
      "Epoch [250/1000], Loss: 0.0623\n",
      "Epoch [250/1000], Loss: 0.0925\n",
      "Epoch [250/1000], Loss: 0.1232\n",
      "Epoch [250/1000], Loss: 0.0931\n",
      "Epoch [250/1000], Loss: 0.0848\n",
      "Epoch [250/1000], Loss: 0.1543\n",
      "Epoch [250/1000], Loss: 0.1879\n",
      "Epoch [250/1000], Loss: 0.1450\n",
      "tensor(2.4537, grad_fn=<MeanBackward0>)\n",
      "250\n",
      "Epoch [251/1000], Loss: 0.0933\n",
      "Epoch [251/1000], Loss: 0.1804\n",
      "Epoch [251/1000], Loss: 0.1663\n",
      "Epoch [251/1000], Loss: 0.0699\n",
      "Epoch [251/1000], Loss: 0.1048\n",
      "Epoch [251/1000], Loss: 0.0804\n",
      "Epoch [251/1000], Loss: 0.1190\n",
      "Epoch [251/1000], Loss: 0.1363\n",
      "Epoch [251/1000], Loss: 0.1158\n",
      "Epoch [251/1000], Loss: 0.2292\n",
      "Epoch [251/1000], Loss: 0.3061\n",
      "tensor(2.2481, grad_fn=<MeanBackward0>)\n",
      "251\n",
      "Epoch [252/1000], Loss: 0.2956\n",
      "Epoch [252/1000], Loss: 0.1339\n",
      "Epoch [252/1000], Loss: 0.2105\n",
      "Epoch [252/1000], Loss: 0.3146\n",
      "Epoch [252/1000], Loss: 0.3224\n",
      "Epoch [252/1000], Loss: 0.2539\n",
      "Epoch [252/1000], Loss: 0.1749\n",
      "Epoch [252/1000], Loss: 0.1506\n",
      "Epoch [252/1000], Loss: 0.2366\n",
      "Epoch [252/1000], Loss: 0.2737\n",
      "Epoch [252/1000], Loss: 0.3623\n",
      "tensor(2.1638, grad_fn=<MeanBackward0>)\n",
      "252\n",
      "Epoch [253/1000], Loss: 0.3085\n",
      "Epoch [253/1000], Loss: 0.1601\n",
      "Epoch [253/1000], Loss: 0.2177\n",
      "Epoch [253/1000], Loss: 0.2632\n",
      "Epoch [253/1000], Loss: 0.3312\n",
      "Epoch [253/1000], Loss: 0.2851\n",
      "Epoch [253/1000], Loss: 0.2050\n",
      "Epoch [253/1000], Loss: 0.1058\n",
      "Epoch [253/1000], Loss: 0.1409\n",
      "Epoch [253/1000], Loss: 0.2506\n",
      "Epoch [253/1000], Loss: 0.3892\n",
      "tensor(2.1451, grad_fn=<MeanBackward0>)\n",
      "253\n",
      "Epoch [254/1000], Loss: 0.3880\n",
      "Epoch [254/1000], Loss: 0.2703\n",
      "Epoch [254/1000], Loss: 0.1773\n",
      "Epoch [254/1000], Loss: 0.2321\n",
      "Epoch [254/1000], Loss: 0.3657\n",
      "Epoch [254/1000], Loss: 0.4226\n",
      "Epoch [254/1000], Loss: 0.3947\n",
      "Epoch [254/1000], Loss: 0.3237\n",
      "Epoch [254/1000], Loss: 0.1601\n",
      "Epoch [254/1000], Loss: 0.2368\n",
      "Epoch [254/1000], Loss: 0.5007\n",
      "tensor(1.9424, grad_fn=<MeanBackward0>)\n",
      "254\n",
      "Epoch [255/1000], Loss: 0.6272\n",
      "Epoch [255/1000], Loss: 0.6235\n",
      "Epoch [255/1000], Loss: 0.6280\n",
      "Epoch [255/1000], Loss: 0.4922\n",
      "Epoch [255/1000], Loss: 0.2172\n",
      "Epoch [255/1000], Loss: 0.1704\n",
      "Epoch [255/1000], Loss: 0.4984\n",
      "Epoch [255/1000], Loss: 0.6355\n",
      "Epoch [255/1000], Loss: 0.7600\n",
      "Epoch [255/1000], Loss: 0.6593\n",
      "Epoch [255/1000], Loss: 0.2723\n",
      "tensor(2.2089, grad_fn=<MeanBackward0>)\n",
      "255\n",
      "Epoch [256/1000], Loss: 0.2924\n",
      "Epoch [256/1000], Loss: 0.4370\n",
      "Epoch [256/1000], Loss: 0.6581\n",
      "Epoch [256/1000], Loss: 0.7506\n",
      "Epoch [256/1000], Loss: 0.6627\n",
      "Epoch [256/1000], Loss: 0.5314\n",
      "Epoch [256/1000], Loss: 0.2050\n",
      "Epoch [256/1000], Loss: 0.1595\n",
      "Epoch [256/1000], Loss: 0.4422\n",
      "Epoch [256/1000], Loss: 0.5986\n",
      "Epoch [256/1000], Loss: 0.4256\n",
      "tensor(2.4783, grad_fn=<MeanBackward0>)\n",
      "256\n",
      "Epoch [257/1000], Loss: 0.2675\n",
      "Epoch [257/1000], Loss: 0.2338\n",
      "Epoch [257/1000], Loss: 0.3679\n",
      "Epoch [257/1000], Loss: 0.4879\n",
      "Epoch [257/1000], Loss: 0.4283\n",
      "Epoch [257/1000], Loss: 0.3617\n",
      "Epoch [257/1000], Loss: 0.1676\n",
      "Epoch [257/1000], Loss: 0.1701\n",
      "Epoch [257/1000], Loss: 0.3226\n",
      "Epoch [257/1000], Loss: 0.3435\n",
      "Epoch [257/1000], Loss: 0.1533\n",
      "tensor(2.1997, grad_fn=<MeanBackward0>)\n",
      "257\n",
      "Epoch [258/1000], Loss: 0.2263\n",
      "Epoch [258/1000], Loss: 0.2941\n",
      "Epoch [258/1000], Loss: 0.3809\n",
      "Epoch [258/1000], Loss: 0.3289\n",
      "Epoch [258/1000], Loss: 0.1454\n",
      "Epoch [258/1000], Loss: 0.1365\n",
      "Epoch [258/1000], Loss: 0.3515\n",
      "Epoch [258/1000], Loss: 0.4078\n",
      "Epoch [258/1000], Loss: 0.4180\n",
      "Epoch [258/1000], Loss: 0.2859\n",
      "Epoch [258/1000], Loss: 0.1691\n",
      "tensor(2.1163, grad_fn=<MeanBackward0>)\n",
      "258\n",
      "Epoch [259/1000], Loss: 0.3538\n",
      "Epoch [259/1000], Loss: 0.4079\n",
      "Epoch [259/1000], Loss: 0.4948\n",
      "Epoch [259/1000], Loss: 0.4188\n",
      "Epoch [259/1000], Loss: 0.1959\n",
      "Epoch [259/1000], Loss: 0.1159\n",
      "Epoch [259/1000], Loss: 0.3552\n",
      "Epoch [259/1000], Loss: 0.5098\n",
      "Epoch [259/1000], Loss: 0.6085\n",
      "Epoch [259/1000], Loss: 0.4965\n",
      "Epoch [259/1000], Loss: 0.1853\n",
      "tensor(2.0731, grad_fn=<MeanBackward0>)\n",
      "259\n",
      "Epoch [260/1000], Loss: 0.2967\n",
      "Epoch [260/1000], Loss: 0.4408\n",
      "Epoch [260/1000], Loss: 0.5860\n",
      "Epoch [260/1000], Loss: 0.5779\n",
      "Epoch [260/1000], Loss: 0.4095\n",
      "Epoch [260/1000], Loss: 0.2473\n",
      "Epoch [260/1000], Loss: 0.1858\n",
      "Epoch [260/1000], Loss: 0.3636\n",
      "Epoch [260/1000], Loss: 0.4879\n",
      "Epoch [260/1000], Loss: 0.4954\n",
      "Epoch [260/1000], Loss: 0.2794\n",
      "tensor(2.3139, grad_fn=<MeanBackward0>)\n",
      "260\n",
      "Epoch [261/1000], Loss: 0.2694\n",
      "Epoch [261/1000], Loss: 0.3271\n",
      "Epoch [261/1000], Loss: 0.4684\n",
      "Epoch [261/1000], Loss: 0.5252\n",
      "Epoch [261/1000], Loss: 0.4140\n",
      "Epoch [261/1000], Loss: 0.3500\n",
      "Epoch [261/1000], Loss: 0.1741\n",
      "Epoch [261/1000], Loss: 0.2418\n",
      "Epoch [261/1000], Loss: 0.4070\n",
      "Epoch [261/1000], Loss: 0.4583\n",
      "Epoch [261/1000], Loss: 0.2494\n",
      "tensor(2.2722, grad_fn=<MeanBackward0>)\n",
      "261\n",
      "Epoch [262/1000], Loss: 0.1942\n",
      "Epoch [262/1000], Loss: 0.2673\n",
      "Epoch [262/1000], Loss: 0.4305\n",
      "Epoch [262/1000], Loss: 0.4798\n",
      "Epoch [262/1000], Loss: 0.3448\n",
      "Epoch [262/1000], Loss: 0.2484\n",
      "Epoch [262/1000], Loss: 0.2034\n",
      "Epoch [262/1000], Loss: 0.2597\n",
      "Epoch [262/1000], Loss: 0.4049\n",
      "Epoch [262/1000], Loss: 0.4232\n",
      "Epoch [262/1000], Loss: 0.1552\n",
      "tensor(2.2717, grad_fn=<MeanBackward0>)\n",
      "262\n",
      "Epoch [263/1000], Loss: 0.1630\n",
      "Epoch [263/1000], Loss: 0.2567\n",
      "Epoch [263/1000], Loss: 0.4298\n",
      "Epoch [263/1000], Loss: 0.4720\n",
      "Epoch [263/1000], Loss: 0.3497\n",
      "Epoch [263/1000], Loss: 0.2519\n",
      "Epoch [263/1000], Loss: 0.1425\n",
      "Epoch [263/1000], Loss: 0.3076\n",
      "Epoch [263/1000], Loss: 0.5125\n",
      "Epoch [263/1000], Loss: 0.5130\n",
      "Epoch [263/1000], Loss: 0.3097\n",
      "tensor(2.3888, grad_fn=<MeanBackward0>)\n",
      "263\n",
      "Epoch [264/1000], Loss: 0.1233\n",
      "Epoch [264/1000], Loss: 0.2165\n",
      "Epoch [264/1000], Loss: 0.4551\n",
      "Epoch [264/1000], Loss: 0.5908\n",
      "Epoch [264/1000], Loss: 0.5239\n",
      "Epoch [264/1000], Loss: 0.4338\n",
      "Epoch [264/1000], Loss: 0.2593\n",
      "Epoch [264/1000], Loss: 0.1383\n",
      "Epoch [264/1000], Loss: 0.3560\n",
      "Epoch [264/1000], Loss: 0.5626\n",
      "Epoch [264/1000], Loss: 0.4845\n",
      "tensor(2.6169, grad_fn=<MeanBackward0>)\n",
      "264\n",
      "Epoch [265/1000], Loss: 0.3911\n",
      "Epoch [265/1000], Loss: 0.2608\n",
      "Epoch [265/1000], Loss: 0.2469\n",
      "Epoch [265/1000], Loss: 0.4302\n",
      "Epoch [265/1000], Loss: 0.4726\n",
      "Epoch [265/1000], Loss: 0.5184\n",
      "Epoch [265/1000], Loss: 0.3601\n",
      "Epoch [265/1000], Loss: 0.3017\n",
      "Epoch [265/1000], Loss: 0.1607\n",
      "Epoch [265/1000], Loss: 0.3330\n",
      "Epoch [265/1000], Loss: 0.3677\n",
      "tensor(2.6046, grad_fn=<MeanBackward0>)\n",
      "265\n",
      "Epoch [266/1000], Loss: 0.4247\n",
      "Epoch [266/1000], Loss: 0.3572\n",
      "Epoch [266/1000], Loss: 0.2194\n",
      "Epoch [266/1000], Loss: 0.2931\n",
      "Epoch [266/1000], Loss: 0.3593\n",
      "Epoch [266/1000], Loss: 0.4520\n",
      "Epoch [266/1000], Loss: 0.3573\n",
      "Epoch [266/1000], Loss: 0.3081\n",
      "Epoch [266/1000], Loss: 0.1351\n",
      "Epoch [266/1000], Loss: 0.2854\n",
      "Epoch [266/1000], Loss: 0.3051\n",
      "tensor(2.6237, grad_fn=<MeanBackward0>)\n",
      "266\n",
      "Epoch [267/1000], Loss: 0.3557\n",
      "Epoch [267/1000], Loss: 0.2813\n",
      "Epoch [267/1000], Loss: 0.1692\n",
      "Epoch [267/1000], Loss: 0.2797\n",
      "Epoch [267/1000], Loss: 0.3142\n",
      "Epoch [267/1000], Loss: 0.3773\n",
      "Epoch [267/1000], Loss: 0.2740\n",
      "Epoch [267/1000], Loss: 0.1895\n",
      "Epoch [267/1000], Loss: 0.1321\n",
      "Epoch [267/1000], Loss: 0.2674\n",
      "Epoch [267/1000], Loss: 0.2060\n",
      "tensor(2.5014, grad_fn=<MeanBackward0>)\n",
      "267\n",
      "Epoch [268/1000], Loss: 0.1845\n",
      "Epoch [268/1000], Loss: 0.1192\n",
      "Epoch [268/1000], Loss: 0.2024\n",
      "Epoch [268/1000], Loss: 0.2919\n",
      "Epoch [268/1000], Loss: 0.2546\n",
      "Epoch [268/1000], Loss: 0.2062\n",
      "Epoch [268/1000], Loss: 0.1101\n",
      "Epoch [268/1000], Loss: 0.1190\n",
      "Epoch [268/1000], Loss: 0.1717\n",
      "Epoch [268/1000], Loss: 0.1646\n",
      "Epoch [268/1000], Loss: 0.1130\n",
      "tensor(2.3692, grad_fn=<MeanBackward0>)\n",
      "268\n",
      "Epoch [269/1000], Loss: 0.1149\n",
      "Epoch [269/1000], Loss: 0.0939\n",
      "Epoch [269/1000], Loss: 0.1126\n",
      "Epoch [269/1000], Loss: 0.0960\n",
      "Epoch [269/1000], Loss: 0.0884\n",
      "Epoch [269/1000], Loss: 0.0870\n",
      "Epoch [269/1000], Loss: 0.0938\n",
      "Epoch [269/1000], Loss: 0.1083\n",
      "Epoch [269/1000], Loss: 0.1205\n",
      "Epoch [269/1000], Loss: 0.1454\n",
      "Epoch [269/1000], Loss: 0.1026\n",
      "tensor(2.3980, grad_fn=<MeanBackward0>)\n",
      "269\n",
      "Epoch [270/1000], Loss: 0.1032\n",
      "Epoch [270/1000], Loss: 0.1062\n",
      "Epoch [270/1000], Loss: 0.1140\n",
      "Epoch [270/1000], Loss: 0.1021\n",
      "Epoch [270/1000], Loss: 0.0739\n",
      "Epoch [270/1000], Loss: 0.0674\n",
      "Epoch [270/1000], Loss: 0.0801\n",
      "Epoch [270/1000], Loss: 0.1069\n",
      "Epoch [270/1000], Loss: 0.0928\n",
      "Epoch [270/1000], Loss: 0.1091\n",
      "Epoch [270/1000], Loss: 0.0730\n",
      "tensor(2.4159, grad_fn=<MeanBackward0>)\n",
      "270\n",
      "Epoch [271/1000], Loss: 0.0697\n",
      "Epoch [271/1000], Loss: 0.0829\n",
      "Epoch [271/1000], Loss: 0.1040\n",
      "Epoch [271/1000], Loss: 0.1000\n",
      "Epoch [271/1000], Loss: 0.0769\n",
      "Epoch [271/1000], Loss: 0.0708\n",
      "Epoch [271/1000], Loss: 0.0767\n",
      "Epoch [271/1000], Loss: 0.1051\n",
      "Epoch [271/1000], Loss: 0.1085\n",
      "Epoch [271/1000], Loss: 0.1081\n",
      "Epoch [271/1000], Loss: 0.0501\n",
      "tensor(2.4338, grad_fn=<MeanBackward0>)\n",
      "271\n",
      "Epoch [272/1000], Loss: 0.0648\n",
      "Epoch [272/1000], Loss: 0.0835\n",
      "Epoch [272/1000], Loss: 0.1092\n",
      "Epoch [272/1000], Loss: 0.1037\n",
      "Epoch [272/1000], Loss: 0.0854\n",
      "Epoch [272/1000], Loss: 0.0595\n",
      "Epoch [272/1000], Loss: 0.0603\n",
      "Epoch [272/1000], Loss: 0.0844\n",
      "Epoch [272/1000], Loss: 0.1332\n",
      "Epoch [272/1000], Loss: 0.1392\n",
      "Epoch [272/1000], Loss: 0.0991\n",
      "tensor(2.3893, grad_fn=<MeanBackward0>)\n",
      "272\n",
      "Epoch [273/1000], Loss: 0.0760\n",
      "Epoch [273/1000], Loss: 0.0753\n",
      "Epoch [273/1000], Loss: 0.1081\n",
      "Epoch [273/1000], Loss: 0.1488\n",
      "Epoch [273/1000], Loss: 0.1449\n",
      "Epoch [273/1000], Loss: 0.1403\n",
      "Epoch [273/1000], Loss: 0.1088\n",
      "Epoch [273/1000], Loss: 0.0655\n",
      "Epoch [273/1000], Loss: 0.0619\n",
      "Epoch [273/1000], Loss: 0.1190\n",
      "Epoch [273/1000], Loss: 0.1071\n",
      "tensor(2.3210, grad_fn=<MeanBackward0>)\n",
      "273\n",
      "Epoch [274/1000], Loss: 0.1379\n",
      "Epoch [274/1000], Loss: 0.1164\n",
      "Epoch [274/1000], Loss: 0.1120\n",
      "Epoch [274/1000], Loss: 0.0692\n",
      "Epoch [274/1000], Loss: 0.0940\n",
      "Epoch [274/1000], Loss: 0.1216\n",
      "Epoch [274/1000], Loss: 0.1144\n",
      "Epoch [274/1000], Loss: 0.1200\n",
      "Epoch [274/1000], Loss: 0.1335\n",
      "Epoch [274/1000], Loss: 0.0859\n",
      "Epoch [274/1000], Loss: 0.0983\n",
      "tensor(2.4070, grad_fn=<MeanBackward0>)\n",
      "274\n",
      "Epoch [275/1000], Loss: 0.0970\n",
      "Epoch [275/1000], Loss: 0.1240\n",
      "Epoch [275/1000], Loss: 0.1788\n",
      "Epoch [275/1000], Loss: 0.1383\n",
      "Epoch [275/1000], Loss: 0.0895\n",
      "Epoch [275/1000], Loss: 0.0555\n",
      "Epoch [275/1000], Loss: 0.0882\n",
      "Epoch [275/1000], Loss: 0.0803\n",
      "Epoch [275/1000], Loss: 0.1316\n",
      "Epoch [275/1000], Loss: 0.0763\n",
      "Epoch [275/1000], Loss: 0.0842\n",
      "tensor(2.4625, grad_fn=<MeanBackward0>)\n",
      "275\n",
      "Epoch [276/1000], Loss: 0.0966\n",
      "Epoch [276/1000], Loss: 0.0762\n",
      "Epoch [276/1000], Loss: 0.1503\n",
      "Epoch [276/1000], Loss: 0.1799\n",
      "Epoch [276/1000], Loss: 0.1351\n",
      "Epoch [276/1000], Loss: 0.0881\n",
      "Epoch [276/1000], Loss: 0.0877\n",
      "Epoch [276/1000], Loss: 0.1019\n",
      "Epoch [276/1000], Loss: 0.0887\n",
      "Epoch [276/1000], Loss: 0.1104\n",
      "Epoch [276/1000], Loss: 0.1231\n",
      "tensor(2.4518, grad_fn=<MeanBackward0>)\n",
      "276\n",
      "Epoch [277/1000], Loss: 0.0828\n",
      "Epoch [277/1000], Loss: 0.1390\n",
      "Epoch [277/1000], Loss: 0.1661\n",
      "Epoch [277/1000], Loss: 0.0645\n",
      "Epoch [277/1000], Loss: 0.1066\n",
      "Epoch [277/1000], Loss: 0.1270\n",
      "Epoch [277/1000], Loss: 0.0896\n",
      "Epoch [277/1000], Loss: 0.1134\n",
      "Epoch [277/1000], Loss: 0.1671\n",
      "Epoch [277/1000], Loss: 0.0777\n",
      "Epoch [277/1000], Loss: 0.1133\n",
      "tensor(2.3506, grad_fn=<MeanBackward0>)\n",
      "277\n",
      "Epoch [278/1000], Loss: 0.1410\n",
      "Epoch [278/1000], Loss: 0.0799\n",
      "Epoch [278/1000], Loss: 0.1699\n",
      "Epoch [278/1000], Loss: 0.1702\n",
      "Epoch [278/1000], Loss: 0.1047\n",
      "Epoch [278/1000], Loss: 0.1284\n",
      "Epoch [278/1000], Loss: 0.1565\n",
      "Epoch [278/1000], Loss: 0.1386\n",
      "Epoch [278/1000], Loss: 0.0661\n",
      "Epoch [278/1000], Loss: 0.1515\n",
      "Epoch [278/1000], Loss: 0.1359\n",
      "tensor(2.4041, grad_fn=<MeanBackward0>)\n",
      "278\n",
      "Epoch [279/1000], Loss: 0.0799\n",
      "Epoch [279/1000], Loss: 0.1067\n",
      "Epoch [279/1000], Loss: 0.0851\n",
      "Epoch [279/1000], Loss: 0.0779\n",
      "Epoch [279/1000], Loss: 0.0764\n",
      "Epoch [279/1000], Loss: 0.0558\n",
      "Epoch [279/1000], Loss: 0.0717\n",
      "Epoch [279/1000], Loss: 0.0756\n",
      "Epoch [279/1000], Loss: 0.0778\n",
      "Epoch [279/1000], Loss: 0.0834\n",
      "Epoch [279/1000], Loss: 0.0566\n",
      "tensor(2.3993, grad_fn=<MeanBackward0>)\n",
      "279\n",
      "Epoch [280/1000], Loss: 0.0659\n",
      "Epoch [280/1000], Loss: 0.0605\n",
      "Epoch [280/1000], Loss: 0.0889\n",
      "Epoch [280/1000], Loss: 0.0560\n",
      "Epoch [280/1000], Loss: 0.0525\n",
      "Epoch [280/1000], Loss: 0.0307\n",
      "Epoch [280/1000], Loss: 0.0386\n",
      "Epoch [280/1000], Loss: 0.0646\n",
      "Epoch [280/1000], Loss: 0.0522\n",
      "Epoch [280/1000], Loss: 0.0912\n",
      "Epoch [280/1000], Loss: 0.0813\n",
      "tensor(2.3700, grad_fn=<MeanBackward0>)\n",
      "280\n",
      "Epoch [281/1000], Loss: 0.0639\n",
      "Epoch [281/1000], Loss: 0.0850\n",
      "Epoch [281/1000], Loss: 0.0615\n",
      "Epoch [281/1000], Loss: 0.0568\n",
      "Epoch [281/1000], Loss: 0.0424\n",
      "Epoch [281/1000], Loss: 0.0462\n",
      "Epoch [281/1000], Loss: 0.0441\n",
      "Epoch [281/1000], Loss: 0.0520\n",
      "Epoch [281/1000], Loss: 0.0448\n",
      "Epoch [281/1000], Loss: 0.0544\n",
      "Epoch [281/1000], Loss: 0.0495\n",
      "tensor(2.3965, grad_fn=<MeanBackward0>)\n",
      "281\n",
      "Epoch [282/1000], Loss: 0.0521\n",
      "Epoch [282/1000], Loss: 0.0993\n",
      "Epoch [282/1000], Loss: 0.0839\n",
      "Epoch [282/1000], Loss: 0.0404\n",
      "Epoch [282/1000], Loss: 0.0733\n",
      "Epoch [282/1000], Loss: 0.0396\n",
      "Epoch [282/1000], Loss: 0.0988\n",
      "Epoch [282/1000], Loss: 0.1404\n",
      "Epoch [282/1000], Loss: 0.0772\n",
      "Epoch [282/1000], Loss: 0.1303\n",
      "Epoch [282/1000], Loss: 0.1994\n",
      "tensor(2.5258, grad_fn=<MeanBackward0>)\n",
      "282\n",
      "Epoch [283/1000], Loss: 0.2302\n",
      "Epoch [283/1000], Loss: 0.0915\n",
      "Epoch [283/1000], Loss: 0.2142\n",
      "Epoch [283/1000], Loss: 0.3300\n",
      "Epoch [283/1000], Loss: 0.2858\n",
      "Epoch [283/1000], Loss: 0.1513\n",
      "Epoch [283/1000], Loss: 0.1163\n",
      "Epoch [283/1000], Loss: 0.1455\n",
      "Epoch [283/1000], Loss: 0.1083\n",
      "Epoch [283/1000], Loss: 0.0941\n",
      "Epoch [283/1000], Loss: 0.0715\n",
      "tensor(2.4825, grad_fn=<MeanBackward0>)\n",
      "283\n",
      "Epoch [284/1000], Loss: 0.1285\n",
      "Epoch [284/1000], Loss: 0.1333\n",
      "Epoch [284/1000], Loss: 0.1051\n",
      "Epoch [284/1000], Loss: 0.1546\n",
      "Epoch [284/1000], Loss: 0.1011\n",
      "Epoch [284/1000], Loss: 0.0959\n",
      "Epoch [284/1000], Loss: 0.1053\n",
      "Epoch [284/1000], Loss: 0.0482\n",
      "Epoch [284/1000], Loss: 0.0925\n",
      "Epoch [284/1000], Loss: 0.1003\n",
      "Epoch [284/1000], Loss: 0.1021\n",
      "tensor(2.5094, grad_fn=<MeanBackward0>)\n",
      "284\n",
      "Epoch [285/1000], Loss: 0.1608\n",
      "Epoch [285/1000], Loss: 0.1198\n",
      "Epoch [285/1000], Loss: 0.1150\n",
      "Epoch [285/1000], Loss: 0.2010\n",
      "Epoch [285/1000], Loss: 0.1361\n",
      "Epoch [285/1000], Loss: 0.0618\n",
      "Epoch [285/1000], Loss: 0.1207\n",
      "Epoch [285/1000], Loss: 0.1111\n",
      "Epoch [285/1000], Loss: 0.0579\n",
      "Epoch [285/1000], Loss: 0.1158\n",
      "Epoch [285/1000], Loss: 0.0875\n",
      "tensor(2.4985, grad_fn=<MeanBackward0>)\n",
      "285\n",
      "Epoch [286/1000], Loss: 0.1483\n",
      "Epoch [286/1000], Loss: 0.1981\n",
      "Epoch [286/1000], Loss: 0.1355\n",
      "Epoch [286/1000], Loss: 0.1145\n",
      "Epoch [286/1000], Loss: 0.1762\n",
      "Epoch [286/1000], Loss: 0.2025\n",
      "Epoch [286/1000], Loss: 0.1245\n",
      "Epoch [286/1000], Loss: 0.0762\n",
      "Epoch [286/1000], Loss: 0.1536\n",
      "Epoch [286/1000], Loss: 0.1370\n",
      "Epoch [286/1000], Loss: 0.0875\n",
      "tensor(2.4209, grad_fn=<MeanBackward0>)\n",
      "286\n",
      "Epoch [287/1000], Loss: 0.0966\n",
      "Epoch [287/1000], Loss: 0.0688\n",
      "Epoch [287/1000], Loss: 0.1300\n",
      "Epoch [287/1000], Loss: 0.1040\n",
      "Epoch [287/1000], Loss: 0.0869\n",
      "Epoch [287/1000], Loss: 0.1471\n",
      "Epoch [287/1000], Loss: 0.1592\n",
      "Epoch [287/1000], Loss: 0.1224\n",
      "Epoch [287/1000], Loss: 0.0683\n",
      "Epoch [287/1000], Loss: 0.1476\n",
      "Epoch [287/1000], Loss: 0.1176\n",
      "tensor(2.4245, grad_fn=<MeanBackward0>)\n",
      "287\n",
      "Epoch [288/1000], Loss: 0.0925\n",
      "Epoch [288/1000], Loss: 0.1246\n",
      "Epoch [288/1000], Loss: 0.0950\n",
      "Epoch [288/1000], Loss: 0.0811\n",
      "Epoch [288/1000], Loss: 0.0694\n",
      "Epoch [288/1000], Loss: 0.0935\n",
      "Epoch [288/1000], Loss: 0.1342\n",
      "Epoch [288/1000], Loss: 0.1417\n",
      "Epoch [288/1000], Loss: 0.0758\n",
      "Epoch [288/1000], Loss: 0.1434\n",
      "Epoch [288/1000], Loss: 0.1486\n",
      "tensor(2.4925, grad_fn=<MeanBackward0>)\n",
      "288\n",
      "Epoch [289/1000], Loss: 0.1053\n",
      "Epoch [289/1000], Loss: 0.0688\n",
      "Epoch [289/1000], Loss: 0.1090\n",
      "Epoch [289/1000], Loss: 0.0973\n",
      "Epoch [289/1000], Loss: 0.1072\n",
      "Epoch [289/1000], Loss: 0.1264\n",
      "Epoch [289/1000], Loss: 0.0482\n",
      "Epoch [289/1000], Loss: 0.1565\n",
      "Epoch [289/1000], Loss: 0.2107\n",
      "Epoch [289/1000], Loss: 0.1082\n",
      "Epoch [289/1000], Loss: 0.1548\n",
      "tensor(2.6317, grad_fn=<MeanBackward0>)\n",
      "289\n",
      "Epoch [290/1000], Loss: 0.3167\n",
      "Epoch [290/1000], Loss: 0.3648\n",
      "Epoch [290/1000], Loss: 0.2094\n",
      "Epoch [290/1000], Loss: 0.0915\n",
      "Epoch [290/1000], Loss: 0.2003\n",
      "Epoch [290/1000], Loss: 0.2367\n",
      "Epoch [290/1000], Loss: 0.1784\n",
      "Epoch [290/1000], Loss: 0.0918\n",
      "Epoch [290/1000], Loss: 0.1081\n",
      "Epoch [290/1000], Loss: 0.1982\n",
      "Epoch [290/1000], Loss: 0.1535\n",
      "tensor(2.5176, grad_fn=<MeanBackward0>)\n",
      "290\n",
      "Epoch [291/1000], Loss: 0.0865\n",
      "Epoch [291/1000], Loss: 0.0957\n",
      "Epoch [291/1000], Loss: 0.1266\n",
      "Epoch [291/1000], Loss: 0.1266\n",
      "Epoch [291/1000], Loss: 0.0572\n",
      "Epoch [291/1000], Loss: 0.0865\n",
      "Epoch [291/1000], Loss: 0.0624\n",
      "Epoch [291/1000], Loss: 0.0975\n",
      "Epoch [291/1000], Loss: 0.1596\n",
      "Epoch [291/1000], Loss: 0.0761\n",
      "Epoch [291/1000], Loss: 0.0999\n",
      "tensor(2.6015, grad_fn=<MeanBackward0>)\n",
      "291\n",
      "Epoch [292/1000], Loss: 0.2318\n",
      "Epoch [292/1000], Loss: 0.2764\n",
      "Epoch [292/1000], Loss: 0.1457\n",
      "Epoch [292/1000], Loss: 0.1193\n",
      "Epoch [292/1000], Loss: 0.2286\n",
      "Epoch [292/1000], Loss: 0.2116\n",
      "Epoch [292/1000], Loss: 0.1142\n",
      "Epoch [292/1000], Loss: 0.0773\n",
      "Epoch [292/1000], Loss: 0.0881\n",
      "Epoch [292/1000], Loss: 0.0717\n",
      "Epoch [292/1000], Loss: 0.0783\n",
      "tensor(2.5534, grad_fn=<MeanBackward0>)\n",
      "292\n",
      "Epoch [293/1000], Loss: 0.0960\n",
      "Epoch [293/1000], Loss: 0.1496\n",
      "Epoch [293/1000], Loss: 0.0770\n",
      "Epoch [293/1000], Loss: 0.1242\n",
      "Epoch [293/1000], Loss: 0.1560\n",
      "Epoch [293/1000], Loss: 0.1147\n",
      "Epoch [293/1000], Loss: 0.0575\n",
      "Epoch [293/1000], Loss: 0.0788\n",
      "Epoch [293/1000], Loss: 0.0630\n",
      "Epoch [293/1000], Loss: 0.0749\n",
      "Epoch [293/1000], Loss: 0.0829\n",
      "tensor(2.5638, grad_fn=<MeanBackward0>)\n",
      "293\n",
      "Epoch [294/1000], Loss: 0.1562\n",
      "Epoch [294/1000], Loss: 0.2820\n",
      "Epoch [294/1000], Loss: 0.2157\n",
      "Epoch [294/1000], Loss: 0.0792\n",
      "Epoch [294/1000], Loss: 0.1885\n",
      "Epoch [294/1000], Loss: 0.2973\n",
      "Epoch [294/1000], Loss: 0.2945\n",
      "Epoch [294/1000], Loss: 0.2678\n",
      "Epoch [294/1000], Loss: 0.1447\n",
      "Epoch [294/1000], Loss: 0.1478\n",
      "Epoch [294/1000], Loss: 0.2197\n",
      "tensor(2.6500, grad_fn=<MeanBackward0>)\n",
      "294\n",
      "Epoch [295/1000], Loss: 0.2120\n",
      "Epoch [295/1000], Loss: 0.1935\n",
      "Epoch [295/1000], Loss: 0.1144\n",
      "Epoch [295/1000], Loss: 0.1427\n",
      "Epoch [295/1000], Loss: 0.1628\n",
      "Epoch [295/1000], Loss: 0.1901\n",
      "Epoch [295/1000], Loss: 0.1380\n",
      "Epoch [295/1000], Loss: 0.0804\n",
      "Epoch [295/1000], Loss: 0.1058\n",
      "Epoch [295/1000], Loss: 0.1161\n",
      "Epoch [295/1000], Loss: 0.0672\n",
      "tensor(2.5152, grad_fn=<MeanBackward0>)\n",
      "295\n",
      "Epoch [296/1000], Loss: 0.0662\n",
      "Epoch [296/1000], Loss: 0.1401\n",
      "Epoch [296/1000], Loss: 0.1498\n",
      "Epoch [296/1000], Loss: 0.0833\n",
      "Epoch [296/1000], Loss: 0.1139\n",
      "Epoch [296/1000], Loss: 0.2296\n",
      "Epoch [296/1000], Loss: 0.2398\n",
      "Epoch [296/1000], Loss: 0.2084\n",
      "Epoch [296/1000], Loss: 0.1360\n",
      "Epoch [296/1000], Loss: 0.1011\n",
      "Epoch [296/1000], Loss: 0.2267\n",
      "tensor(2.6765, grad_fn=<MeanBackward0>)\n",
      "296\n",
      "Epoch [297/1000], Loss: 0.2475\n",
      "Epoch [297/1000], Loss: 0.2220\n",
      "Epoch [297/1000], Loss: 0.1111\n",
      "Epoch [297/1000], Loss: 0.0949\n",
      "Epoch [297/1000], Loss: 0.1452\n",
      "Epoch [297/1000], Loss: 0.1868\n",
      "Epoch [297/1000], Loss: 0.1609\n",
      "Epoch [297/1000], Loss: 0.1045\n",
      "Epoch [297/1000], Loss: 0.0701\n",
      "Epoch [297/1000], Loss: 0.1031\n",
      "Epoch [297/1000], Loss: 0.0765\n",
      "tensor(2.5289, grad_fn=<MeanBackward0>)\n",
      "297\n",
      "Epoch [298/1000], Loss: 0.0736\n",
      "Epoch [298/1000], Loss: 0.0867\n",
      "Epoch [298/1000], Loss: 0.1146\n",
      "Epoch [298/1000], Loss: 0.1040\n",
      "Epoch [298/1000], Loss: 0.0767\n",
      "Epoch [298/1000], Loss: 0.2043\n",
      "Epoch [298/1000], Loss: 0.2626\n",
      "Epoch [298/1000], Loss: 0.2890\n",
      "Epoch [298/1000], Loss: 0.2687\n",
      "Epoch [298/1000], Loss: 0.1169\n",
      "Epoch [298/1000], Loss: 0.2008\n",
      "tensor(2.7854, grad_fn=<MeanBackward0>)\n",
      "298\n",
      "Epoch [299/1000], Loss: 0.4069\n",
      "Epoch [299/1000], Loss: 0.5019\n",
      "Epoch [299/1000], Loss: 0.4885\n",
      "Epoch [299/1000], Loss: 0.3827\n",
      "Epoch [299/1000], Loss: 0.1962\n",
      "Epoch [299/1000], Loss: 0.1952\n",
      "Epoch [299/1000], Loss: 0.3997\n",
      "Epoch [299/1000], Loss: 0.6532\n",
      "Epoch [299/1000], Loss: 0.8271\n",
      "Epoch [299/1000], Loss: 0.8140\n",
      "Epoch [299/1000], Loss: 0.7805\n",
      "tensor(2.1792, grad_fn=<MeanBackward0>)\n",
      "299\n",
      "Epoch [300/1000], Loss: 0.5164\n",
      "Epoch [300/1000], Loss: 0.2029\n",
      "Epoch [300/1000], Loss: 0.3136\n",
      "Epoch [300/1000], Loss: 0.6182\n",
      "Epoch [300/1000], Loss: 0.8103\n",
      "Epoch [300/1000], Loss: 0.7576\n",
      "Epoch [300/1000], Loss: 0.6500\n",
      "Epoch [300/1000], Loss: 0.3108\n",
      "Epoch [300/1000], Loss: 0.2177\n",
      "Epoch [300/1000], Loss: 0.4104\n",
      "Epoch [300/1000], Loss: 0.6256\n",
      "tensor(2.0925, grad_fn=<MeanBackward0>)\n",
      "300\n",
      "Epoch [301/1000], Loss: 0.5850\n",
      "Epoch [301/1000], Loss: 0.5142\n",
      "Epoch [301/1000], Loss: 0.5095\n",
      "Epoch [301/1000], Loss: 0.2976\n",
      "Epoch [301/1000], Loss: 0.2588\n",
      "Epoch [301/1000], Loss: 0.2977\n",
      "Epoch [301/1000], Loss: 0.3682\n",
      "Epoch [301/1000], Loss: 0.2760\n",
      "Epoch [301/1000], Loss: 0.2124\n",
      "Epoch [301/1000], Loss: 0.1966\n",
      "Epoch [301/1000], Loss: 0.2977\n",
      "tensor(2.2946, grad_fn=<MeanBackward0>)\n",
      "301\n",
      "Epoch [302/1000], Loss: 0.2568\n",
      "Epoch [302/1000], Loss: 0.1713\n",
      "Epoch [302/1000], Loss: 0.1987\n",
      "Epoch [302/1000], Loss: 0.2006\n",
      "Epoch [302/1000], Loss: 0.2662\n",
      "Epoch [302/1000], Loss: 0.1761\n",
      "Epoch [302/1000], Loss: 0.1055\n",
      "Epoch [302/1000], Loss: 0.2499\n",
      "Epoch [302/1000], Loss: 0.3324\n",
      "Epoch [302/1000], Loss: 0.2945\n",
      "Epoch [302/1000], Loss: 0.1770\n",
      "tensor(2.5631, grad_fn=<MeanBackward0>)\n",
      "302\n",
      "Epoch [303/1000], Loss: 0.1815\n",
      "Epoch [303/1000], Loss: 0.3757\n",
      "Epoch [303/1000], Loss: 0.3905\n",
      "Epoch [303/1000], Loss: 0.3353\n",
      "Epoch [303/1000], Loss: 0.2230\n",
      "Epoch [303/1000], Loss: 0.1276\n",
      "Epoch [303/1000], Loss: 0.1912\n",
      "Epoch [303/1000], Loss: 0.3478\n",
      "Epoch [303/1000], Loss: 0.4386\n",
      "Epoch [303/1000], Loss: 0.3616\n",
      "Epoch [303/1000], Loss: 0.2968\n",
      "tensor(2.4658, grad_fn=<MeanBackward0>)\n",
      "303\n",
      "Epoch [304/1000], Loss: 0.1040\n",
      "Epoch [304/1000], Loss: 0.3145\n",
      "Epoch [304/1000], Loss: 0.4231\n",
      "Epoch [304/1000], Loss: 0.4647\n",
      "Epoch [304/1000], Loss: 0.3649\n",
      "Epoch [304/1000], Loss: 0.1983\n",
      "Epoch [304/1000], Loss: 0.1523\n",
      "Epoch [304/1000], Loss: 0.3605\n",
      "Epoch [304/1000], Loss: 0.5107\n",
      "Epoch [304/1000], Loss: 0.4888\n",
      "Epoch [304/1000], Loss: 0.4824\n",
      "tensor(2.3115, grad_fn=<MeanBackward0>)\n",
      "304\n",
      "Epoch [305/1000], Loss: 0.2903\n",
      "Epoch [305/1000], Loss: 0.1344\n",
      "Epoch [305/1000], Loss: 0.3100\n",
      "Epoch [305/1000], Loss: 0.4895\n",
      "Epoch [305/1000], Loss: 0.5706\n",
      "Epoch [305/1000], Loss: 0.5041\n",
      "Epoch [305/1000], Loss: 0.4025\n",
      "Epoch [305/1000], Loss: 0.1673\n",
      "Epoch [305/1000], Loss: 0.2841\n",
      "Epoch [305/1000], Loss: 0.3990\n",
      "Epoch [305/1000], Loss: 0.5547\n",
      "tensor(2.1448, grad_fn=<MeanBackward0>)\n",
      "305\n",
      "Epoch [306/1000], Loss: 0.4870\n",
      "Epoch [306/1000], Loss: 0.3692\n",
      "Epoch [306/1000], Loss: 0.2844\n",
      "Epoch [306/1000], Loss: 0.1607\n",
      "Epoch [306/1000], Loss: 0.3442\n",
      "Epoch [306/1000], Loss: 0.4333\n",
      "Epoch [306/1000], Loss: 0.4547\n",
      "Epoch [306/1000], Loss: 0.3124\n",
      "Epoch [306/1000], Loss: 0.1774\n",
      "Epoch [306/1000], Loss: 0.2189\n",
      "Epoch [306/1000], Loss: 0.4004\n",
      "tensor(2.1441, grad_fn=<MeanBackward0>)\n",
      "306\n",
      "Epoch [307/1000], Loss: 0.4018\n",
      "Epoch [307/1000], Loss: 0.3238\n",
      "Epoch [307/1000], Loss: 0.2099\n",
      "Epoch [307/1000], Loss: 0.1248\n",
      "Epoch [307/1000], Loss: 0.2869\n",
      "Epoch [307/1000], Loss: 0.3673\n",
      "Epoch [307/1000], Loss: 0.3415\n",
      "Epoch [307/1000], Loss: 0.1542\n",
      "Epoch [307/1000], Loss: 0.1621\n",
      "Epoch [307/1000], Loss: 0.2675\n",
      "Epoch [307/1000], Loss: 0.3797\n",
      "tensor(2.2065, grad_fn=<MeanBackward0>)\n",
      "307\n",
      "Epoch [308/1000], Loss: 0.3179\n",
      "Epoch [308/1000], Loss: 0.2148\n",
      "Epoch [308/1000], Loss: 0.1953\n",
      "Epoch [308/1000], Loss: 0.1577\n",
      "Epoch [308/1000], Loss: 0.2335\n",
      "Epoch [308/1000], Loss: 0.2511\n",
      "Epoch [308/1000], Loss: 0.2234\n",
      "Epoch [308/1000], Loss: 0.1093\n",
      "Epoch [308/1000], Loss: 0.1676\n",
      "Epoch [308/1000], Loss: 0.2306\n",
      "Epoch [308/1000], Loss: 0.2838\n",
      "tensor(2.2657, grad_fn=<MeanBackward0>)\n",
      "308\n",
      "Epoch [309/1000], Loss: 0.1920\n",
      "Epoch [309/1000], Loss: 0.0936\n",
      "Epoch [309/1000], Loss: 0.1688\n",
      "Epoch [309/1000], Loss: 0.1969\n",
      "Epoch [309/1000], Loss: 0.1890\n",
      "Epoch [309/1000], Loss: 0.1161\n",
      "Epoch [309/1000], Loss: 0.0787\n",
      "Epoch [309/1000], Loss: 0.1731\n",
      "Epoch [309/1000], Loss: 0.1940\n",
      "Epoch [309/1000], Loss: 0.1421\n",
      "Epoch [309/1000], Loss: 0.1135\n",
      "tensor(2.4664, grad_fn=<MeanBackward0>)\n",
      "309\n",
      "Epoch [310/1000], Loss: 0.1346\n",
      "Epoch [310/1000], Loss: 0.1808\n",
      "Epoch [310/1000], Loss: 0.1263\n",
      "Epoch [310/1000], Loss: 0.0742\n",
      "Epoch [310/1000], Loss: 0.0853\n",
      "Epoch [310/1000], Loss: 0.0681\n",
      "Epoch [310/1000], Loss: 0.0772\n",
      "Epoch [310/1000], Loss: 0.0764\n",
      "Epoch [310/1000], Loss: 0.0798\n",
      "Epoch [310/1000], Loss: 0.0957\n",
      "Epoch [310/1000], Loss: 0.0598\n",
      "tensor(2.4338, grad_fn=<MeanBackward0>)\n",
      "310\n",
      "Epoch [311/1000], Loss: 0.0855\n",
      "Epoch [311/1000], Loss: 0.1062\n",
      "Epoch [311/1000], Loss: 0.1006\n",
      "Epoch [311/1000], Loss: 0.0725\n",
      "Epoch [311/1000], Loss: 0.0717\n",
      "Epoch [311/1000], Loss: 0.0674\n",
      "Epoch [311/1000], Loss: 0.0649\n",
      "Epoch [311/1000], Loss: 0.0795\n",
      "Epoch [311/1000], Loss: 0.0715\n",
      "Epoch [311/1000], Loss: 0.0829\n",
      "Epoch [311/1000], Loss: 0.0851\n",
      "tensor(2.4364, grad_fn=<MeanBackward0>)\n",
      "311\n",
      "Epoch [312/1000], Loss: 0.0692\n",
      "Epoch [312/1000], Loss: 0.1044\n",
      "Epoch [312/1000], Loss: 0.1113\n",
      "Epoch [312/1000], Loss: 0.0932\n",
      "Epoch [312/1000], Loss: 0.0713\n",
      "Epoch [312/1000], Loss: 0.0977\n",
      "Epoch [312/1000], Loss: 0.0958\n",
      "Epoch [312/1000], Loss: 0.0725\n",
      "Epoch [312/1000], Loss: 0.0941\n",
      "Epoch [312/1000], Loss: 0.0820\n",
      "Epoch [312/1000], Loss: 0.0732\n",
      "tensor(2.3862, grad_fn=<MeanBackward0>)\n",
      "312\n",
      "Epoch [313/1000], Loss: 0.0892\n",
      "Epoch [313/1000], Loss: 0.0915\n",
      "Epoch [313/1000], Loss: 0.1562\n",
      "Epoch [313/1000], Loss: 0.1757\n",
      "Epoch [313/1000], Loss: 0.1479\n",
      "Epoch [313/1000], Loss: 0.1106\n",
      "Epoch [313/1000], Loss: 0.1155\n",
      "Epoch [313/1000], Loss: 0.1603\n",
      "Epoch [313/1000], Loss: 0.1215\n",
      "Epoch [313/1000], Loss: 0.0771\n",
      "Epoch [313/1000], Loss: 0.1073\n",
      "tensor(2.4798, grad_fn=<MeanBackward0>)\n",
      "313\n",
      "Epoch [314/1000], Loss: 0.0788\n",
      "Epoch [314/1000], Loss: 0.0812\n",
      "Epoch [314/1000], Loss: 0.1012\n",
      "Epoch [314/1000], Loss: 0.1117\n",
      "Epoch [314/1000], Loss: 0.1371\n",
      "Epoch [314/1000], Loss: 0.0889\n",
      "Epoch [314/1000], Loss: 0.0999\n",
      "Epoch [314/1000], Loss: 0.1796\n",
      "Epoch [314/1000], Loss: 0.1518\n",
      "Epoch [314/1000], Loss: 0.0929\n",
      "Epoch [314/1000], Loss: 0.1020\n",
      "tensor(2.5067, grad_fn=<MeanBackward0>)\n",
      "314\n",
      "Epoch [315/1000], Loss: 0.1029\n",
      "Epoch [315/1000], Loss: 0.0629\n",
      "Epoch [315/1000], Loss: 0.0890\n",
      "Epoch [315/1000], Loss: 0.0716\n",
      "Epoch [315/1000], Loss: 0.1280\n",
      "Epoch [315/1000], Loss: 0.1541\n",
      "Epoch [315/1000], Loss: 0.0788\n",
      "Epoch [315/1000], Loss: 0.1542\n",
      "Epoch [315/1000], Loss: 0.2524\n",
      "Epoch [315/1000], Loss: 0.3033\n",
      "Epoch [315/1000], Loss: 0.2512\n",
      "tensor(2.3862, grad_fn=<MeanBackward0>)\n",
      "315\n",
      "Epoch [316/1000], Loss: 0.0902\n",
      "Epoch [316/1000], Loss: 0.1748\n",
      "Epoch [316/1000], Loss: 0.2926\n",
      "Epoch [316/1000], Loss: 0.3206\n",
      "Epoch [316/1000], Loss: 0.2676\n",
      "Epoch [316/1000], Loss: 0.1564\n",
      "Epoch [316/1000], Loss: 0.0857\n",
      "Epoch [316/1000], Loss: 0.2245\n",
      "Epoch [316/1000], Loss: 0.3097\n",
      "Epoch [316/1000], Loss: 0.3034\n",
      "Epoch [316/1000], Loss: 0.2825\n",
      "tensor(2.3461, grad_fn=<MeanBackward0>)\n",
      "316\n",
      "Epoch [317/1000], Loss: 0.1364\n",
      "Epoch [317/1000], Loss: 0.1181\n",
      "Epoch [317/1000], Loss: 0.2520\n",
      "Epoch [317/1000], Loss: 0.3062\n",
      "Epoch [317/1000], Loss: 0.2876\n",
      "Epoch [317/1000], Loss: 0.1827\n",
      "Epoch [317/1000], Loss: 0.0640\n",
      "Epoch [317/1000], Loss: 0.2027\n",
      "Epoch [317/1000], Loss: 0.3206\n",
      "Epoch [317/1000], Loss: 0.3680\n",
      "Epoch [317/1000], Loss: 0.3930\n",
      "tensor(2.2293, grad_fn=<MeanBackward0>)\n",
      "317\n",
      "Epoch [318/1000], Loss: 0.2653\n",
      "Epoch [318/1000], Loss: 0.1291\n",
      "Epoch [318/1000], Loss: 0.2134\n",
      "Epoch [318/1000], Loss: 0.3585\n",
      "Epoch [318/1000], Loss: 0.4672\n",
      "Epoch [318/1000], Loss: 0.4611\n",
      "Epoch [318/1000], Loss: 0.4060\n",
      "Epoch [318/1000], Loss: 0.2438\n",
      "Epoch [318/1000], Loss: 0.1401\n",
      "Epoch [318/1000], Loss: 0.3001\n",
      "Epoch [318/1000], Loss: 0.5247\n",
      "tensor(1.9979, grad_fn=<MeanBackward0>)\n",
      "318\n",
      "Epoch [319/1000], Loss: 0.5675\n",
      "Epoch [319/1000], Loss: 0.5638\n",
      "Epoch [319/1000], Loss: 0.4773\n",
      "Epoch [319/1000], Loss: 0.2604\n",
      "Epoch [319/1000], Loss: 0.1650\n",
      "Epoch [319/1000], Loss: 0.3692\n",
      "Epoch [319/1000], Loss: 0.5866\n",
      "Epoch [319/1000], Loss: 0.6130\n",
      "Epoch [319/1000], Loss: 0.5886\n",
      "Epoch [319/1000], Loss: 0.3903\n",
      "Epoch [319/1000], Loss: 0.1765\n",
      "tensor(2.1764, grad_fn=<MeanBackward0>)\n",
      "319\n",
      "Epoch [320/1000], Loss: 0.3133\n",
      "Epoch [320/1000], Loss: 0.4584\n",
      "Epoch [320/1000], Loss: 0.5905\n",
      "Epoch [320/1000], Loss: 0.5879\n",
      "Epoch [320/1000], Loss: 0.4142\n",
      "Epoch [320/1000], Loss: 0.2131\n",
      "Epoch [320/1000], Loss: 0.1735\n",
      "Epoch [320/1000], Loss: 0.3231\n",
      "Epoch [320/1000], Loss: 0.4185\n",
      "Epoch [320/1000], Loss: 0.3504\n",
      "Epoch [320/1000], Loss: 0.1685\n",
      "tensor(2.2848, grad_fn=<MeanBackward0>)\n",
      "320\n",
      "Epoch [321/1000], Loss: 0.1959\n",
      "Epoch [321/1000], Loss: 0.3046\n",
      "Epoch [321/1000], Loss: 0.3934\n",
      "Epoch [321/1000], Loss: 0.3771\n",
      "Epoch [321/1000], Loss: 0.2228\n",
      "Epoch [321/1000], Loss: 0.1057\n",
      "Epoch [321/1000], Loss: 0.2540\n",
      "Epoch [321/1000], Loss: 0.3525\n",
      "Epoch [321/1000], Loss: 0.3482\n",
      "Epoch [321/1000], Loss: 0.2082\n",
      "Epoch [321/1000], Loss: 0.1251\n",
      "tensor(2.1918, grad_fn=<MeanBackward0>)\n",
      "321\n",
      "Epoch [322/1000], Loss: 0.2439\n",
      "Epoch [322/1000], Loss: 0.3312\n",
      "Epoch [322/1000], Loss: 0.3755\n",
      "Epoch [322/1000], Loss: 0.2728\n",
      "Epoch [322/1000], Loss: 0.1044\n",
      "Epoch [322/1000], Loss: 0.2044\n",
      "Epoch [322/1000], Loss: 0.3554\n",
      "Epoch [322/1000], Loss: 0.4225\n",
      "Epoch [322/1000], Loss: 0.3648\n",
      "Epoch [322/1000], Loss: 0.1954\n",
      "Epoch [322/1000], Loss: 0.1590\n",
      "tensor(2.1043, grad_fn=<MeanBackward0>)\n",
      "322\n",
      "Epoch [323/1000], Loss: 0.2972\n",
      "Epoch [323/1000], Loss: 0.3860\n",
      "Epoch [323/1000], Loss: 0.4122\n",
      "Epoch [323/1000], Loss: 0.3064\n",
      "Epoch [323/1000], Loss: 0.1102\n",
      "Epoch [323/1000], Loss: 0.1701\n",
      "Epoch [323/1000], Loss: 0.3442\n",
      "Epoch [323/1000], Loss: 0.4225\n",
      "Epoch [323/1000], Loss: 0.3696\n",
      "Epoch [323/1000], Loss: 0.2325\n",
      "Epoch [323/1000], Loss: 0.1223\n",
      "tensor(2.1988, grad_fn=<MeanBackward0>)\n",
      "323\n",
      "Epoch [324/1000], Loss: 0.2429\n",
      "Epoch [324/1000], Loss: 0.3345\n",
      "Epoch [324/1000], Loss: 0.4171\n",
      "Epoch [324/1000], Loss: 0.3606\n",
      "Epoch [324/1000], Loss: 0.1933\n",
      "Epoch [324/1000], Loss: 0.0964\n",
      "Epoch [324/1000], Loss: 0.2759\n",
      "Epoch [324/1000], Loss: 0.3853\n",
      "Epoch [324/1000], Loss: 0.4075\n",
      "Epoch [324/1000], Loss: 0.3098\n",
      "Epoch [324/1000], Loss: 0.1148\n",
      "tensor(2.2011, grad_fn=<MeanBackward0>)\n",
      "324\n",
      "Epoch [325/1000], Loss: 0.1927\n",
      "Epoch [325/1000], Loss: 0.3464\n",
      "Epoch [325/1000], Loss: 0.4666\n",
      "Epoch [325/1000], Loss: 0.4736\n",
      "Epoch [325/1000], Loss: 0.3338\n",
      "Epoch [325/1000], Loss: 0.2097\n",
      "Epoch [325/1000], Loss: 0.1399\n",
      "Epoch [325/1000], Loss: 0.3279\n",
      "Epoch [325/1000], Loss: 0.4249\n",
      "Epoch [325/1000], Loss: 0.4227\n",
      "Epoch [325/1000], Loss: 0.2727\n",
      "tensor(2.3612, grad_fn=<MeanBackward0>)\n",
      "325\n",
      "Epoch [326/1000], Loss: 0.1498\n",
      "Epoch [326/1000], Loss: 0.2302\n",
      "Epoch [326/1000], Loss: 0.4027\n",
      "Epoch [326/1000], Loss: 0.4973\n",
      "Epoch [326/1000], Loss: 0.4275\n",
      "Epoch [326/1000], Loss: 0.3599\n",
      "Epoch [326/1000], Loss: 0.1809\n",
      "Epoch [326/1000], Loss: 0.1302\n",
      "Epoch [326/1000], Loss: 0.2746\n",
      "Epoch [326/1000], Loss: 0.3424\n",
      "Epoch [326/1000], Loss: 0.2846\n",
      "tensor(2.4260, grad_fn=<MeanBackward0>)\n",
      "326\n",
      "Epoch [327/1000], Loss: 0.1867\n",
      "Epoch [327/1000], Loss: 0.1574\n",
      "Epoch [327/1000], Loss: 0.2927\n",
      "Epoch [327/1000], Loss: 0.3879\n",
      "Epoch [327/1000], Loss: 0.3401\n",
      "Epoch [327/1000], Loss: 0.2909\n",
      "Epoch [327/1000], Loss: 0.1155\n",
      "Epoch [327/1000], Loss: 0.1312\n",
      "Epoch [327/1000], Loss: 0.2527\n",
      "Epoch [327/1000], Loss: 0.3033\n",
      "Epoch [327/1000], Loss: 0.2211\n",
      "tensor(2.3972, grad_fn=<MeanBackward0>)\n",
      "327\n",
      "Epoch [328/1000], Loss: 0.1220\n",
      "Epoch [328/1000], Loss: 0.1615\n",
      "Epoch [328/1000], Loss: 0.2800\n",
      "Epoch [328/1000], Loss: 0.3503\n",
      "Epoch [328/1000], Loss: 0.2836\n",
      "Epoch [328/1000], Loss: 0.2138\n",
      "Epoch [328/1000], Loss: 0.0980\n",
      "Epoch [328/1000], Loss: 0.1377\n",
      "Epoch [328/1000], Loss: 0.2334\n",
      "Epoch [328/1000], Loss: 0.2194\n",
      "Epoch [328/1000], Loss: 0.1196\n",
      "tensor(2.3396, grad_fn=<MeanBackward0>)\n",
      "328\n",
      "Epoch [329/1000], Loss: 0.0982\n",
      "Epoch [329/1000], Loss: 0.1465\n",
      "Epoch [329/1000], Loss: 0.2246\n",
      "Epoch [329/1000], Loss: 0.2122\n",
      "Epoch [329/1000], Loss: 0.1212\n",
      "Epoch [329/1000], Loss: 0.0863\n",
      "Epoch [329/1000], Loss: 0.1518\n",
      "Epoch [329/1000], Loss: 0.1481\n",
      "Epoch [329/1000], Loss: 0.1131\n",
      "Epoch [329/1000], Loss: 0.1211\n",
      "Epoch [329/1000], Loss: 0.1357\n",
      "tensor(2.3023, grad_fn=<MeanBackward0>)\n",
      "329\n",
      "Epoch [330/1000], Loss: 0.0760\n",
      "Epoch [330/1000], Loss: 0.0791\n",
      "Epoch [330/1000], Loss: 0.0953\n",
      "Epoch [330/1000], Loss: 0.0850\n",
      "Epoch [330/1000], Loss: 0.0697\n",
      "Epoch [330/1000], Loss: 0.0782\n",
      "Epoch [330/1000], Loss: 0.0728\n",
      "Epoch [330/1000], Loss: 0.0869\n",
      "Epoch [330/1000], Loss: 0.0920\n",
      "Epoch [330/1000], Loss: 0.1020\n",
      "Epoch [330/1000], Loss: 0.0617\n",
      "tensor(2.3812, grad_fn=<MeanBackward0>)\n",
      "330\n",
      "Epoch [331/1000], Loss: 0.0709\n",
      "Epoch [331/1000], Loss: 0.0944\n",
      "Epoch [331/1000], Loss: 0.1104\n",
      "Epoch [331/1000], Loss: 0.0817\n",
      "Epoch [331/1000], Loss: 0.0633\n",
      "Epoch [331/1000], Loss: 0.0604\n",
      "Epoch [331/1000], Loss: 0.0582\n",
      "Epoch [331/1000], Loss: 0.0918\n",
      "Epoch [331/1000], Loss: 0.0687\n",
      "Epoch [331/1000], Loss: 0.0713\n",
      "Epoch [331/1000], Loss: 0.0751\n",
      "tensor(2.3691, grad_fn=<MeanBackward0>)\n",
      "331\n",
      "Epoch [332/1000], Loss: 0.0524\n",
      "Epoch [332/1000], Loss: 0.0878\n",
      "Epoch [332/1000], Loss: 0.1206\n",
      "Epoch [332/1000], Loss: 0.0828\n",
      "Epoch [332/1000], Loss: 0.0589\n",
      "Epoch [332/1000], Loss: 0.0772\n",
      "Epoch [332/1000], Loss: 0.0423\n",
      "Epoch [332/1000], Loss: 0.0970\n",
      "Epoch [332/1000], Loss: 0.0907\n",
      "Epoch [332/1000], Loss: 0.0445\n",
      "Epoch [332/1000], Loss: 0.1070\n",
      "tensor(2.4528, grad_fn=<MeanBackward0>)\n",
      "332\n",
      "Epoch [333/1000], Loss: 0.1368\n",
      "Epoch [333/1000], Loss: 0.0530\n",
      "Epoch [333/1000], Loss: 0.1377\n",
      "Epoch [333/1000], Loss: 0.2110\n",
      "Epoch [333/1000], Loss: 0.1693\n",
      "Epoch [333/1000], Loss: 0.1095\n",
      "Epoch [333/1000], Loss: 0.1188\n",
      "Epoch [333/1000], Loss: 0.1796\n",
      "Epoch [333/1000], Loss: 0.1533\n",
      "Epoch [333/1000], Loss: 0.0710\n",
      "Epoch [333/1000], Loss: 0.1095\n",
      "tensor(2.3597, grad_fn=<MeanBackward0>)\n",
      "333\n",
      "Epoch [334/1000], Loss: 0.0637\n",
      "Epoch [334/1000], Loss: 0.0893\n",
      "Epoch [334/1000], Loss: 0.0946\n",
      "Epoch [334/1000], Loss: 0.0723\n",
      "Epoch [334/1000], Loss: 0.0692\n",
      "Epoch [334/1000], Loss: 0.0574\n",
      "Epoch [334/1000], Loss: 0.0633\n",
      "Epoch [334/1000], Loss: 0.0859\n",
      "Epoch [334/1000], Loss: 0.0545\n",
      "Epoch [334/1000], Loss: 0.0771\n",
      "Epoch [334/1000], Loss: 0.0684\n",
      "tensor(2.4031, grad_fn=<MeanBackward0>)\n",
      "334\n",
      "Epoch [335/1000], Loss: 0.0683\n",
      "Epoch [335/1000], Loss: 0.0772\n",
      "Epoch [335/1000], Loss: 0.0783\n",
      "Epoch [335/1000], Loss: 0.0895\n",
      "Epoch [335/1000], Loss: 0.0619\n",
      "Epoch [335/1000], Loss: 0.0540\n",
      "Epoch [335/1000], Loss: 0.0586\n",
      "Epoch [335/1000], Loss: 0.0427\n",
      "Epoch [335/1000], Loss: 0.0557\n",
      "Epoch [335/1000], Loss: 0.0564\n",
      "Epoch [335/1000], Loss: 0.0596\n",
      "tensor(2.3982, grad_fn=<MeanBackward0>)\n",
      "335\n",
      "Epoch [336/1000], Loss: 0.0431\n",
      "Epoch [336/1000], Loss: 0.0408\n",
      "Epoch [336/1000], Loss: 0.0525\n",
      "Epoch [336/1000], Loss: 0.0518\n",
      "Epoch [336/1000], Loss: 0.0475\n",
      "Epoch [336/1000], Loss: 0.0570\n",
      "Epoch [336/1000], Loss: 0.0433\n",
      "Epoch [336/1000], Loss: 0.0784\n",
      "Epoch [336/1000], Loss: 0.0646\n",
      "Epoch [336/1000], Loss: 0.0730\n",
      "Epoch [336/1000], Loss: 0.0960\n",
      "tensor(2.3865, grad_fn=<MeanBackward0>)\n",
      "336\n",
      "Epoch [337/1000], Loss: 0.0533\n",
      "Epoch [337/1000], Loss: 0.1197\n",
      "Epoch [337/1000], Loss: 0.1034\n",
      "Epoch [337/1000], Loss: 0.0518\n",
      "Epoch [337/1000], Loss: 0.0980\n",
      "Epoch [337/1000], Loss: 0.1142\n",
      "Epoch [337/1000], Loss: 0.0721\n",
      "Epoch [337/1000], Loss: 0.1163\n",
      "Epoch [337/1000], Loss: 0.1875\n",
      "Epoch [337/1000], Loss: 0.1747\n",
      "Epoch [337/1000], Loss: 0.0431\n",
      "tensor(2.2698, grad_fn=<MeanBackward0>)\n",
      "337\n",
      "Epoch [338/1000], Loss: 0.1819\n",
      "Epoch [338/1000], Loss: 0.2046\n",
      "Epoch [338/1000], Loss: 0.1354\n",
      "Epoch [338/1000], Loss: 0.0844\n",
      "Epoch [338/1000], Loss: 0.1453\n",
      "Epoch [338/1000], Loss: 0.0869\n",
      "Epoch [338/1000], Loss: 0.0566\n",
      "Epoch [338/1000], Loss: 0.1108\n",
      "Epoch [338/1000], Loss: 0.0767\n",
      "Epoch [338/1000], Loss: 0.1373\n",
      "Epoch [338/1000], Loss: 0.1469\n",
      "tensor(2.4301, grad_fn=<MeanBackward0>)\n",
      "338\n",
      "Epoch [339/1000], Loss: 0.0739\n",
      "Epoch [339/1000], Loss: 0.0865\n",
      "Epoch [339/1000], Loss: 0.1198\n",
      "Epoch [339/1000], Loss: 0.0908\n",
      "Epoch [339/1000], Loss: 0.0630\n",
      "Epoch [339/1000], Loss: 0.0678\n",
      "Epoch [339/1000], Loss: 0.0501\n",
      "Epoch [339/1000], Loss: 0.0546\n",
      "Epoch [339/1000], Loss: 0.0562\n",
      "Epoch [339/1000], Loss: 0.0828\n",
      "Epoch [339/1000], Loss: 0.0703\n",
      "tensor(2.3821, grad_fn=<MeanBackward0>)\n",
      "339\n",
      "Epoch [340/1000], Loss: 0.0550\n",
      "Epoch [340/1000], Loss: 0.0492\n",
      "Epoch [340/1000], Loss: 0.0501\n",
      "Epoch [340/1000], Loss: 0.0472\n",
      "Epoch [340/1000], Loss: 0.0525\n",
      "Epoch [340/1000], Loss: 0.0497\n",
      "Epoch [340/1000], Loss: 0.0606\n",
      "Epoch [340/1000], Loss: 0.0400\n",
      "Epoch [340/1000], Loss: 0.0529\n",
      "Epoch [340/1000], Loss: 0.0456\n",
      "Epoch [340/1000], Loss: 0.0493\n",
      "tensor(2.3914, grad_fn=<MeanBackward0>)\n",
      "340\n",
      "Epoch [341/1000], Loss: 0.0351\n",
      "Epoch [341/1000], Loss: 0.0492\n",
      "Epoch [341/1000], Loss: 0.0570\n",
      "Epoch [341/1000], Loss: 0.0492\n",
      "Epoch [341/1000], Loss: 0.0408\n",
      "Epoch [341/1000], Loss: 0.0544\n",
      "Epoch [341/1000], Loss: 0.0470\n",
      "Epoch [341/1000], Loss: 0.0635\n",
      "Epoch [341/1000], Loss: 0.0564\n",
      "Epoch [341/1000], Loss: 0.0665\n",
      "Epoch [341/1000], Loss: 0.0701\n",
      "tensor(2.4397, grad_fn=<MeanBackward0>)\n",
      "341\n",
      "Epoch [342/1000], Loss: 0.0853\n",
      "Epoch [342/1000], Loss: 0.1069\n",
      "Epoch [342/1000], Loss: 0.0764\n",
      "Epoch [342/1000], Loss: 0.0963\n",
      "Epoch [342/1000], Loss: 0.0867\n",
      "Epoch [342/1000], Loss: 0.0486\n",
      "Epoch [342/1000], Loss: 0.0673\n",
      "Epoch [342/1000], Loss: 0.0827\n",
      "Epoch [342/1000], Loss: 0.0966\n",
      "Epoch [342/1000], Loss: 0.0890\n",
      "Epoch [342/1000], Loss: 0.1063\n",
      "tensor(2.3774, grad_fn=<MeanBackward0>)\n",
      "342\n",
      "Epoch [343/1000], Loss: 0.0711\n",
      "Epoch [343/1000], Loss: 0.0733\n",
      "Epoch [343/1000], Loss: 0.0872\n",
      "Epoch [343/1000], Loss: 0.0840\n",
      "Epoch [343/1000], Loss: 0.0999\n",
      "Epoch [343/1000], Loss: 0.1216\n",
      "Epoch [343/1000], Loss: 0.0953\n",
      "Epoch [343/1000], Loss: 0.0550\n",
      "Epoch [343/1000], Loss: 0.1145\n",
      "Epoch [343/1000], Loss: 0.0925\n",
      "Epoch [343/1000], Loss: 0.0776\n",
      "tensor(2.3098, grad_fn=<MeanBackward0>)\n",
      "343\n",
      "Epoch [344/1000], Loss: 0.0980\n",
      "Epoch [344/1000], Loss: 0.0559\n",
      "Epoch [344/1000], Loss: 0.0764\n",
      "Epoch [344/1000], Loss: 0.0920\n",
      "Epoch [344/1000], Loss: 0.0640\n",
      "Epoch [344/1000], Loss: 0.1150\n",
      "Epoch [344/1000], Loss: 0.1123\n",
      "Epoch [344/1000], Loss: 0.0731\n",
      "Epoch [344/1000], Loss: 0.1112\n",
      "Epoch [344/1000], Loss: 0.1949\n",
      "Epoch [344/1000], Loss: 0.1871\n",
      "tensor(2.4034, grad_fn=<MeanBackward0>)\n",
      "344\n",
      "Epoch [345/1000], Loss: 0.0518\n",
      "Epoch [345/1000], Loss: 0.1199\n",
      "Epoch [345/1000], Loss: 0.1370\n",
      "Epoch [345/1000], Loss: 0.0974\n",
      "Epoch [345/1000], Loss: 0.0574\n",
      "Epoch [345/1000], Loss: 0.0766\n",
      "Epoch [345/1000], Loss: 0.0634\n",
      "Epoch [345/1000], Loss: 0.1065\n",
      "Epoch [345/1000], Loss: 0.0617\n",
      "Epoch [345/1000], Loss: 0.1343\n",
      "Epoch [345/1000], Loss: 0.2021\n",
      "tensor(2.4904, grad_fn=<MeanBackward0>)\n",
      "345\n",
      "Epoch [346/1000], Loss: 0.1631\n",
      "Epoch [346/1000], Loss: 0.0490\n",
      "Epoch [346/1000], Loss: 0.1610\n",
      "Epoch [346/1000], Loss: 0.2335\n",
      "Epoch [346/1000], Loss: 0.1986\n",
      "Epoch [346/1000], Loss: 0.1288\n",
      "Epoch [346/1000], Loss: 0.0652\n",
      "Epoch [346/1000], Loss: 0.0970\n",
      "Epoch [346/1000], Loss: 0.0970\n",
      "Epoch [346/1000], Loss: 0.0734\n",
      "Epoch [346/1000], Loss: 0.0661\n",
      "tensor(2.3771, grad_fn=<MeanBackward0>)\n",
      "346\n",
      "Epoch [347/1000], Loss: 0.0634\n",
      "Epoch [347/1000], Loss: 0.0673\n",
      "Epoch [347/1000], Loss: 0.0623\n",
      "Epoch [347/1000], Loss: 0.0595\n",
      "Epoch [347/1000], Loss: 0.0666\n",
      "Epoch [347/1000], Loss: 0.0498\n",
      "Epoch [347/1000], Loss: 0.0458\n",
      "Epoch [347/1000], Loss: 0.0625\n",
      "Epoch [347/1000], Loss: 0.0772\n",
      "Epoch [347/1000], Loss: 0.0603\n",
      "Epoch [347/1000], Loss: 0.0423\n",
      "tensor(2.3681, grad_fn=<MeanBackward0>)\n",
      "347\n",
      "Epoch [348/1000], Loss: 0.0596\n",
      "Epoch [348/1000], Loss: 0.0490\n",
      "Epoch [348/1000], Loss: 0.0652\n",
      "Epoch [348/1000], Loss: 0.0578\n",
      "Epoch [348/1000], Loss: 0.0798\n",
      "Epoch [348/1000], Loss: 0.0845\n",
      "Epoch [348/1000], Loss: 0.0663\n",
      "Epoch [348/1000], Loss: 0.0501\n",
      "Epoch [348/1000], Loss: 0.0632\n",
      "Epoch [348/1000], Loss: 0.0882\n",
      "Epoch [348/1000], Loss: 0.0577\n",
      "tensor(2.3188, grad_fn=<MeanBackward0>)\n",
      "348\n",
      "Epoch [349/1000], Loss: 0.0754\n",
      "Epoch [349/1000], Loss: 0.0808\n",
      "Epoch [349/1000], Loss: 0.0570\n",
      "Epoch [349/1000], Loss: 0.0747\n",
      "Epoch [349/1000], Loss: 0.0635\n",
      "Epoch [349/1000], Loss: 0.0673\n",
      "Epoch [349/1000], Loss: 0.1324\n",
      "Epoch [349/1000], Loss: 0.1194\n",
      "Epoch [349/1000], Loss: 0.1116\n",
      "Epoch [349/1000], Loss: 0.1449\n",
      "Epoch [349/1000], Loss: 0.2871\n",
      "tensor(2.5927, grad_fn=<MeanBackward0>)\n",
      "349\n",
      "Epoch [350/1000], Loss: 0.2930\n",
      "Epoch [350/1000], Loss: 0.1552\n",
      "Epoch [350/1000], Loss: 0.1329\n",
      "Epoch [350/1000], Loss: 0.2559\n",
      "Epoch [350/1000], Loss: 0.2491\n",
      "Epoch [350/1000], Loss: 0.2273\n",
      "Epoch [350/1000], Loss: 0.1601\n",
      "Epoch [350/1000], Loss: 0.1219\n",
      "Epoch [350/1000], Loss: 0.0787\n",
      "Epoch [350/1000], Loss: 0.1553\n",
      "Epoch [350/1000], Loss: 0.2329\n",
      "tensor(2.5488, grad_fn=<MeanBackward0>)\n",
      "350\n",
      "Epoch [351/1000], Loss: 0.2175\n",
      "Epoch [351/1000], Loss: 0.0655\n",
      "Epoch [351/1000], Loss: 0.1553\n",
      "Epoch [351/1000], Loss: 0.2452\n",
      "Epoch [351/1000], Loss: 0.2132\n",
      "Epoch [351/1000], Loss: 0.1254\n",
      "Epoch [351/1000], Loss: 0.0666\n",
      "Epoch [351/1000], Loss: 0.0925\n",
      "Epoch [351/1000], Loss: 0.0591\n",
      "Epoch [351/1000], Loss: 0.0650\n",
      "Epoch [351/1000], Loss: 0.0799\n",
      "tensor(2.4489, grad_fn=<MeanBackward0>)\n",
      "351\n",
      "Epoch [352/1000], Loss: 0.0906\n",
      "Epoch [352/1000], Loss: 0.0633\n",
      "Epoch [352/1000], Loss: 0.0734\n",
      "Epoch [352/1000], Loss: 0.0867\n",
      "Epoch [352/1000], Loss: 0.0418\n",
      "Epoch [352/1000], Loss: 0.0516\n",
      "Epoch [352/1000], Loss: 0.0570\n",
      "Epoch [352/1000], Loss: 0.0780\n",
      "Epoch [352/1000], Loss: 0.0854\n",
      "Epoch [352/1000], Loss: 0.1080\n",
      "Epoch [352/1000], Loss: 0.2150\n",
      "tensor(2.5592, grad_fn=<MeanBackward0>)\n",
      "352\n",
      "Epoch [353/1000], Loss: 0.2299\n",
      "Epoch [353/1000], Loss: 0.1484\n",
      "Epoch [353/1000], Loss: 0.0941\n",
      "Epoch [353/1000], Loss: 0.2019\n",
      "Epoch [353/1000], Loss: 0.2178\n",
      "Epoch [353/1000], Loss: 0.2330\n",
      "Epoch [353/1000], Loss: 0.1880\n",
      "Epoch [353/1000], Loss: 0.1183\n",
      "Epoch [353/1000], Loss: 0.0527\n",
      "Epoch [353/1000], Loss: 0.1649\n",
      "Epoch [353/1000], Loss: 0.2617\n",
      "tensor(2.6470, grad_fn=<MeanBackward0>)\n",
      "353\n",
      "Epoch [354/1000], Loss: 0.2643\n",
      "Epoch [354/1000], Loss: 0.1607\n",
      "Epoch [354/1000], Loss: 0.1229\n",
      "Epoch [354/1000], Loss: 0.1914\n",
      "Epoch [354/1000], Loss: 0.2432\n",
      "Epoch [354/1000], Loss: 0.2550\n",
      "Epoch [354/1000], Loss: 0.2091\n",
      "Epoch [354/1000], Loss: 0.1707\n",
      "Epoch [354/1000], Loss: 0.1074\n",
      "Epoch [354/1000], Loss: 0.1248\n",
      "Epoch [354/1000], Loss: 0.2991\n",
      "tensor(2.6655, grad_fn=<MeanBackward0>)\n",
      "354\n",
      "Epoch [355/1000], Loss: 0.3763\n",
      "Epoch [355/1000], Loss: 0.3397\n",
      "Epoch [355/1000], Loss: 0.2538\n",
      "Epoch [355/1000], Loss: 0.1304\n",
      "Epoch [355/1000], Loss: 0.2141\n",
      "Epoch [355/1000], Loss: 0.3653\n",
      "Epoch [355/1000], Loss: 0.4252\n",
      "Epoch [355/1000], Loss: 0.4486\n",
      "Epoch [355/1000], Loss: 0.4322\n",
      "Epoch [355/1000], Loss: 0.3107\n",
      "Epoch [355/1000], Loss: 0.1035\n",
      "tensor(2.7231, grad_fn=<MeanBackward0>)\n",
      "355\n",
      "Epoch [356/1000], Loss: 0.3925\n",
      "Epoch [356/1000], Loss: 0.6220\n",
      "Epoch [356/1000], Loss: 0.7426\n",
      "Epoch [356/1000], Loss: 0.7083\n",
      "Epoch [356/1000], Loss: 0.5326\n",
      "Epoch [356/1000], Loss: 0.2072\n",
      "Epoch [356/1000], Loss: 0.2047\n",
      "Epoch [356/1000], Loss: 0.4815\n",
      "Epoch [356/1000], Loss: 0.7378\n",
      "Epoch [356/1000], Loss: 0.7742\n",
      "Epoch [356/1000], Loss: 0.7705\n",
      "tensor(2.0992, grad_fn=<MeanBackward0>)\n",
      "356\n",
      "Epoch [357/1000], Loss: 0.5764\n",
      "Epoch [357/1000], Loss: 0.3219\n",
      "Epoch [357/1000], Loss: 0.2357\n",
      "Epoch [357/1000], Loss: 0.3485\n",
      "Epoch [357/1000], Loss: 0.5082\n",
      "Epoch [357/1000], Loss: 0.4778\n",
      "Epoch [357/1000], Loss: 0.4425\n",
      "Epoch [357/1000], Loss: 0.2279\n",
      "Epoch [357/1000], Loss: 0.1917\n",
      "Epoch [357/1000], Loss: 0.3425\n",
      "Epoch [357/1000], Loss: 0.4486\n",
      "tensor(2.2148, grad_fn=<MeanBackward0>)\n",
      "357\n",
      "Epoch [358/1000], Loss: 0.3564\n",
      "Epoch [358/1000], Loss: 0.2338\n",
      "Epoch [358/1000], Loss: 0.1530\n",
      "Epoch [358/1000], Loss: 0.1745\n",
      "Epoch [358/1000], Loss: 0.2584\n",
      "Epoch [358/1000], Loss: 0.2045\n",
      "Epoch [358/1000], Loss: 0.1284\n",
      "Epoch [358/1000], Loss: 0.1630\n",
      "Epoch [358/1000], Loss: 0.2764\n",
      "Epoch [358/1000], Loss: 0.2962\n",
      "Epoch [358/1000], Loss: 0.2054\n",
      "tensor(2.4847, grad_fn=<MeanBackward0>)\n",
      "358\n",
      "Epoch [359/1000], Loss: 0.0925\n",
      "Epoch [359/1000], Loss: 0.2551\n",
      "Epoch [359/1000], Loss: 0.3519\n",
      "Epoch [359/1000], Loss: 0.2603\n",
      "Epoch [359/1000], Loss: 0.1417\n",
      "Epoch [359/1000], Loss: 0.1599\n",
      "Epoch [359/1000], Loss: 0.2360\n",
      "Epoch [359/1000], Loss: 0.2711\n",
      "Epoch [359/1000], Loss: 0.2593\n",
      "Epoch [359/1000], Loss: 0.1477\n",
      "Epoch [359/1000], Loss: 0.1437\n",
      "tensor(2.6245, grad_fn=<MeanBackward0>)\n",
      "359\n",
      "Epoch [360/1000], Loss: 0.2668\n",
      "Epoch [360/1000], Loss: 0.3359\n",
      "Epoch [360/1000], Loss: 0.2937\n",
      "Epoch [360/1000], Loss: 0.1280\n",
      "Epoch [360/1000], Loss: 0.1054\n",
      "Epoch [360/1000], Loss: 0.2135\n",
      "Epoch [360/1000], Loss: 0.2564\n",
      "Epoch [360/1000], Loss: 0.2567\n",
      "Epoch [360/1000], Loss: 0.2427\n",
      "Epoch [360/1000], Loss: 0.1482\n",
      "Epoch [360/1000], Loss: 0.1108\n",
      "tensor(2.6393, grad_fn=<MeanBackward0>)\n",
      "360\n",
      "Epoch [361/1000], Loss: 0.2844\n",
      "Epoch [361/1000], Loss: 0.3516\n",
      "Epoch [361/1000], Loss: 0.3785\n",
      "Epoch [361/1000], Loss: 0.2186\n",
      "Epoch [361/1000], Loss: 0.1102\n",
      "Epoch [361/1000], Loss: 0.1998\n",
      "Epoch [361/1000], Loss: 0.2881\n",
      "Epoch [361/1000], Loss: 0.3758\n",
      "Epoch [361/1000], Loss: 0.3902\n",
      "Epoch [361/1000], Loss: 0.3313\n",
      "Epoch [361/1000], Loss: 0.1859\n",
      "tensor(2.5950, grad_fn=<MeanBackward0>)\n",
      "361\n",
      "Epoch [362/1000], Loss: 0.1586\n",
      "Epoch [362/1000], Loss: 0.3733\n",
      "Epoch [362/1000], Loss: 0.5183\n",
      "Epoch [362/1000], Loss: 0.4875\n",
      "Epoch [362/1000], Loss: 0.4203\n",
      "Epoch [362/1000], Loss: 0.2113\n",
      "Epoch [362/1000], Loss: 0.2368\n",
      "Epoch [362/1000], Loss: 0.3723\n",
      "Epoch [362/1000], Loss: 0.5613\n",
      "Epoch [362/1000], Loss: 0.6223\n",
      "Epoch [362/1000], Loss: 0.6557\n",
      "tensor(2.1711, grad_fn=<MeanBackward0>)\n",
      "362\n",
      "Epoch [363/1000], Loss: 0.5193\n",
      "Epoch [363/1000], Loss: 0.2653\n",
      "Epoch [363/1000], Loss: 0.1830\n",
      "Epoch [363/1000], Loss: 0.3648\n",
      "Epoch [363/1000], Loss: 0.5304\n",
      "Epoch [363/1000], Loss: 0.5466\n",
      "Epoch [363/1000], Loss: 0.4501\n",
      "Epoch [363/1000], Loss: 0.2584\n",
      "Epoch [363/1000], Loss: 0.2105\n",
      "Epoch [363/1000], Loss: 0.2917\n",
      "Epoch [363/1000], Loss: 0.5326\n",
      "tensor(2.0491, grad_fn=<MeanBackward0>)\n",
      "363\n",
      "Epoch [364/1000], Loss: 0.5541\n",
      "Epoch [364/1000], Loss: 0.4296\n",
      "Epoch [364/1000], Loss: 0.3139\n",
      "Epoch [364/1000], Loss: 0.1461\n",
      "Epoch [364/1000], Loss: 0.2537\n",
      "Epoch [364/1000], Loss: 0.3640\n",
      "Epoch [364/1000], Loss: 0.4259\n",
      "Epoch [364/1000], Loss: 0.3115\n",
      "Epoch [364/1000], Loss: 0.1953\n",
      "Epoch [364/1000], Loss: 0.1978\n",
      "Epoch [364/1000], Loss: 0.3295\n",
      "tensor(2.2179, grad_fn=<MeanBackward0>)\n",
      "364\n",
      "Epoch [365/1000], Loss: 0.3622\n",
      "Epoch [365/1000], Loss: 0.2992\n",
      "Epoch [365/1000], Loss: 0.2026\n",
      "Epoch [365/1000], Loss: 0.1292\n",
      "Epoch [365/1000], Loss: 0.1968\n",
      "Epoch [365/1000], Loss: 0.2850\n",
      "Epoch [365/1000], Loss: 0.2941\n",
      "Epoch [365/1000], Loss: 0.1892\n",
      "Epoch [365/1000], Loss: 0.1104\n",
      "Epoch [365/1000], Loss: 0.2071\n",
      "Epoch [365/1000], Loss: 0.3062\n",
      "tensor(2.2163, grad_fn=<MeanBackward0>)\n",
      "365\n",
      "Epoch [366/1000], Loss: 0.2605\n",
      "Epoch [366/1000], Loss: 0.1478\n",
      "Epoch [366/1000], Loss: 0.1139\n",
      "Epoch [366/1000], Loss: 0.1479\n",
      "Epoch [366/1000], Loss: 0.1635\n",
      "Epoch [366/1000], Loss: 0.1186\n",
      "Epoch [366/1000], Loss: 0.0948\n",
      "Epoch [366/1000], Loss: 0.1503\n",
      "Epoch [366/1000], Loss: 0.1877\n",
      "Epoch [366/1000], Loss: 0.1241\n",
      "Epoch [366/1000], Loss: 0.0961\n",
      "tensor(2.4698, grad_fn=<MeanBackward0>)\n",
      "366\n",
      "Epoch [367/1000], Loss: 0.1103\n",
      "Epoch [367/1000], Loss: 0.1807\n",
      "Epoch [367/1000], Loss: 0.1511\n",
      "Epoch [367/1000], Loss: 0.0763\n",
      "Epoch [367/1000], Loss: 0.0943\n",
      "Epoch [367/1000], Loss: 0.1028\n",
      "Epoch [367/1000], Loss: 0.0714\n",
      "Epoch [367/1000], Loss: 0.0665\n",
      "Epoch [367/1000], Loss: 0.0705\n",
      "Epoch [367/1000], Loss: 0.0698\n",
      "Epoch [367/1000], Loss: 0.0546\n",
      "tensor(2.4206, grad_fn=<MeanBackward0>)\n",
      "367\n",
      "Epoch [368/1000], Loss: 0.0642\n",
      "Epoch [368/1000], Loss: 0.0757\n",
      "Epoch [368/1000], Loss: 0.0757\n",
      "Epoch [368/1000], Loss: 0.0562\n",
      "Epoch [368/1000], Loss: 0.0662\n",
      "Epoch [368/1000], Loss: 0.0619\n",
      "Epoch [368/1000], Loss: 0.0512\n",
      "Epoch [368/1000], Loss: 0.0675\n",
      "Epoch [368/1000], Loss: 0.0925\n",
      "Epoch [368/1000], Loss: 0.0908\n",
      "Epoch [368/1000], Loss: 0.0484\n",
      "tensor(2.4733, grad_fn=<MeanBackward0>)\n",
      "368\n",
      "Epoch [369/1000], Loss: 0.0783\n",
      "Epoch [369/1000], Loss: 0.0902\n",
      "Epoch [369/1000], Loss: 0.0782\n",
      "Epoch [369/1000], Loss: 0.0804\n",
      "Epoch [369/1000], Loss: 0.0872\n",
      "Epoch [369/1000], Loss: 0.0793\n",
      "Epoch [369/1000], Loss: 0.0618\n",
      "Epoch [369/1000], Loss: 0.0785\n",
      "Epoch [369/1000], Loss: 0.1055\n",
      "Epoch [369/1000], Loss: 0.0925\n",
      "Epoch [369/1000], Loss: 0.0815\n",
      "tensor(2.4608, grad_fn=<MeanBackward0>)\n",
      "369\n",
      "Epoch [370/1000], Loss: 0.0661\n",
      "Epoch [370/1000], Loss: 0.1111\n",
      "Epoch [370/1000], Loss: 0.1588\n",
      "Epoch [370/1000], Loss: 0.1105\n",
      "Epoch [370/1000], Loss: 0.0983\n",
      "Epoch [370/1000], Loss: 0.1506\n",
      "Epoch [370/1000], Loss: 0.1639\n",
      "Epoch [370/1000], Loss: 0.1698\n",
      "Epoch [370/1000], Loss: 0.0905\n",
      "Epoch [370/1000], Loss: 0.0870\n",
      "Epoch [370/1000], Loss: 0.1025\n",
      "tensor(2.4892, grad_fn=<MeanBackward0>)\n",
      "370\n",
      "Epoch [371/1000], Loss: 0.0588\n",
      "Epoch [371/1000], Loss: 0.0821\n",
      "Epoch [371/1000], Loss: 0.0888\n",
      "Epoch [371/1000], Loss: 0.0885\n",
      "Epoch [371/1000], Loss: 0.0970\n",
      "Epoch [371/1000], Loss: 0.0806\n",
      "Epoch [371/1000], Loss: 0.0932\n",
      "Epoch [371/1000], Loss: 0.1339\n",
      "Epoch [371/1000], Loss: 0.0961\n",
      "Epoch [371/1000], Loss: 0.0784\n",
      "Epoch [371/1000], Loss: 0.1020\n",
      "tensor(2.4979, grad_fn=<MeanBackward0>)\n",
      "371\n",
      "Epoch [372/1000], Loss: 0.0972\n",
      "Epoch [372/1000], Loss: 0.0703\n",
      "Epoch [372/1000], Loss: 0.0782\n",
      "Epoch [372/1000], Loss: 0.0718\n",
      "Epoch [372/1000], Loss: 0.0738\n",
      "Epoch [372/1000], Loss: 0.0425\n",
      "Epoch [372/1000], Loss: 0.0541\n",
      "Epoch [372/1000], Loss: 0.0854\n",
      "Epoch [372/1000], Loss: 0.0772\n",
      "Epoch [372/1000], Loss: 0.0575\n",
      "Epoch [372/1000], Loss: 0.0512\n",
      "tensor(2.5041, grad_fn=<MeanBackward0>)\n",
      "372\n",
      "Epoch [373/1000], Loss: 0.0590\n",
      "Epoch [373/1000], Loss: 0.0519\n",
      "Epoch [373/1000], Loss: 0.0478\n",
      "Epoch [373/1000], Loss: 0.0416\n",
      "Epoch [373/1000], Loss: 0.0527\n",
      "Epoch [373/1000], Loss: 0.0489\n",
      "Epoch [373/1000], Loss: 0.0505\n",
      "Epoch [373/1000], Loss: 0.0678\n",
      "Epoch [373/1000], Loss: 0.0413\n",
      "Epoch [373/1000], Loss: 0.0472\n",
      "Epoch [373/1000], Loss: 0.0457\n",
      "tensor(2.4963, grad_fn=<MeanBackward0>)\n",
      "373\n",
      "Epoch [374/1000], Loss: 0.0497\n",
      "Epoch [374/1000], Loss: 0.0741\n",
      "Epoch [374/1000], Loss: 0.0598\n",
      "Epoch [374/1000], Loss: 0.0383\n",
      "Epoch [374/1000], Loss: 0.0472\n",
      "Epoch [374/1000], Loss: 0.0442\n",
      "Epoch [374/1000], Loss: 0.0494\n",
      "Epoch [374/1000], Loss: 0.0670\n",
      "Epoch [374/1000], Loss: 0.0555\n",
      "Epoch [374/1000], Loss: 0.0731\n",
      "Epoch [374/1000], Loss: 0.0425\n",
      "tensor(2.5417, grad_fn=<MeanBackward0>)\n",
      "374\n",
      "Epoch [375/1000], Loss: 0.0942\n",
      "Epoch [375/1000], Loss: 0.1321\n",
      "Epoch [375/1000], Loss: 0.1288\n",
      "Epoch [375/1000], Loss: 0.0790\n",
      "Epoch [375/1000], Loss: 0.0801\n",
      "Epoch [375/1000], Loss: 0.1257\n",
      "Epoch [375/1000], Loss: 0.0963\n",
      "Epoch [375/1000], Loss: 0.0586\n",
      "Epoch [375/1000], Loss: 0.0459\n",
      "Epoch [375/1000], Loss: 0.0607\n",
      "Epoch [375/1000], Loss: 0.0746\n",
      "tensor(2.5157, grad_fn=<MeanBackward0>)\n",
      "375\n",
      "Epoch [376/1000], Loss: 0.0702\n",
      "Epoch [376/1000], Loss: 0.1234\n",
      "Epoch [376/1000], Loss: 0.1663\n",
      "Epoch [376/1000], Loss: 0.1203\n",
      "Epoch [376/1000], Loss: 0.0708\n",
      "Epoch [376/1000], Loss: 0.1391\n",
      "Epoch [376/1000], Loss: 0.1712\n",
      "Epoch [376/1000], Loss: 0.1547\n",
      "Epoch [376/1000], Loss: 0.1318\n",
      "Epoch [376/1000], Loss: 0.0927\n",
      "Epoch [376/1000], Loss: 0.0915\n",
      "tensor(2.5248, grad_fn=<MeanBackward0>)\n",
      "376\n",
      "Epoch [377/1000], Loss: 0.0590\n",
      "Epoch [377/1000], Loss: 0.0895\n",
      "Epoch [377/1000], Loss: 0.1438\n",
      "Epoch [377/1000], Loss: 0.1540\n",
      "Epoch [377/1000], Loss: 0.1272\n",
      "Epoch [377/1000], Loss: 0.1013\n",
      "Epoch [377/1000], Loss: 0.1539\n",
      "Epoch [377/1000], Loss: 0.2358\n",
      "Epoch [377/1000], Loss: 0.2734\n",
      "Epoch [377/1000], Loss: 0.2318\n",
      "Epoch [377/1000], Loss: 0.1404\n",
      "tensor(2.5412, grad_fn=<MeanBackward0>)\n",
      "377\n",
      "Epoch [378/1000], Loss: 0.1152\n",
      "Epoch [378/1000], Loss: 0.2616\n",
      "Epoch [378/1000], Loss: 0.3653\n",
      "Epoch [378/1000], Loss: 0.4116\n",
      "Epoch [378/1000], Loss: 0.3422\n",
      "Epoch [378/1000], Loss: 0.1964\n",
      "Epoch [378/1000], Loss: 0.1215\n",
      "Epoch [378/1000], Loss: 0.2999\n",
      "Epoch [378/1000], Loss: 0.5098\n",
      "Epoch [378/1000], Loss: 0.5806\n",
      "Epoch [378/1000], Loss: 0.6129\n",
      "tensor(2.1606, grad_fn=<MeanBackward0>)\n",
      "378\n",
      "Epoch [379/1000], Loss: 0.5529\n",
      "Epoch [379/1000], Loss: 0.3377\n",
      "Epoch [379/1000], Loss: 0.1359\n",
      "Epoch [379/1000], Loss: 0.3833\n",
      "Epoch [379/1000], Loss: 0.6519\n",
      "Epoch [379/1000], Loss: 0.7197\n",
      "Epoch [379/1000], Loss: 0.7073\n",
      "Epoch [379/1000], Loss: 0.5225\n",
      "Epoch [379/1000], Loss: 0.2523\n",
      "Epoch [379/1000], Loss: 0.2409\n",
      "Epoch [379/1000], Loss: 0.4878\n",
      "tensor(2.0684, grad_fn=<MeanBackward0>)\n",
      "379\n",
      "Epoch [380/1000], Loss: 0.6079\n",
      "Epoch [380/1000], Loss: 0.6273\n",
      "Epoch [380/1000], Loss: 0.6481\n",
      "Epoch [380/1000], Loss: 0.5175\n",
      "Epoch [380/1000], Loss: 0.3141\n",
      "Epoch [380/1000], Loss: 0.2081\n",
      "Epoch [380/1000], Loss: 0.3172\n",
      "Epoch [380/1000], Loss: 0.3596\n",
      "Epoch [380/1000], Loss: 0.4399\n",
      "Epoch [380/1000], Loss: 0.3921\n",
      "Epoch [380/1000], Loss: 0.1642\n",
      "tensor(2.3010, grad_fn=<MeanBackward0>)\n",
      "380\n",
      "Epoch [381/1000], Loss: 0.2339\n",
      "Epoch [381/1000], Loss: 0.2894\n",
      "Epoch [381/1000], Loss: 0.2913\n",
      "Epoch [381/1000], Loss: 0.2478\n",
      "Epoch [381/1000], Loss: 0.1392\n",
      "Epoch [381/1000], Loss: 0.1779\n",
      "Epoch [381/1000], Loss: 0.2162\n",
      "Epoch [381/1000], Loss: 0.1647\n",
      "Epoch [381/1000], Loss: 0.1121\n",
      "Epoch [381/1000], Loss: 0.1846\n",
      "Epoch [381/1000], Loss: 0.2367\n",
      "tensor(2.2878, grad_fn=<MeanBackward0>)\n",
      "381\n",
      "Epoch [382/1000], Loss: 0.2414\n",
      "Epoch [382/1000], Loss: 0.1459\n",
      "Epoch [382/1000], Loss: 0.1127\n",
      "Epoch [382/1000], Loss: 0.2097\n",
      "Epoch [382/1000], Loss: 0.2652\n",
      "Epoch [382/1000], Loss: 0.2388\n",
      "Epoch [382/1000], Loss: 0.1597\n",
      "Epoch [382/1000], Loss: 0.1311\n",
      "Epoch [382/1000], Loss: 0.2427\n",
      "Epoch [382/1000], Loss: 0.2645\n",
      "Epoch [382/1000], Loss: 0.3032\n",
      "tensor(2.2333, grad_fn=<MeanBackward0>)\n",
      "382\n",
      "Epoch [383/1000], Loss: 0.2315\n",
      "Epoch [383/1000], Loss: 0.0937\n",
      "Epoch [383/1000], Loss: 0.1754\n",
      "Epoch [383/1000], Loss: 0.2692\n",
      "Epoch [383/1000], Loss: 0.3119\n",
      "Epoch [383/1000], Loss: 0.2446\n",
      "Epoch [383/1000], Loss: 0.1162\n",
      "Epoch [383/1000], Loss: 0.1166\n",
      "Epoch [383/1000], Loss: 0.2621\n",
      "Epoch [383/1000], Loss: 0.2897\n",
      "Epoch [383/1000], Loss: 0.3237\n",
      "tensor(2.2689, grad_fn=<MeanBackward0>)\n",
      "383\n",
      "Epoch [384/1000], Loss: 0.2478\n",
      "Epoch [384/1000], Loss: 0.1189\n",
      "Epoch [384/1000], Loss: 0.1299\n",
      "Epoch [384/1000], Loss: 0.2421\n",
      "Epoch [384/1000], Loss: 0.3163\n",
      "Epoch [384/1000], Loss: 0.2778\n",
      "Epoch [384/1000], Loss: 0.1818\n",
      "Epoch [384/1000], Loss: 0.0969\n",
      "Epoch [384/1000], Loss: 0.2266\n",
      "Epoch [384/1000], Loss: 0.3020\n",
      "Epoch [384/1000], Loss: 0.3756\n",
      "tensor(2.1439, grad_fn=<MeanBackward0>)\n",
      "384\n",
      "Epoch [385/1000], Loss: 0.3488\n",
      "Epoch [385/1000], Loss: 0.2290\n",
      "Epoch [385/1000], Loss: 0.1610\n",
      "Epoch [385/1000], Loss: 0.2171\n",
      "Epoch [385/1000], Loss: 0.4135\n",
      "Epoch [385/1000], Loss: 0.4443\n",
      "Epoch [385/1000], Loss: 0.4330\n",
      "Epoch [385/1000], Loss: 0.3300\n",
      "Epoch [385/1000], Loss: 0.1676\n",
      "Epoch [385/1000], Loss: 0.1879\n",
      "Epoch [385/1000], Loss: 0.4043\n",
      "tensor(2.0318, grad_fn=<MeanBackward0>)\n",
      "385\n",
      "Epoch [386/1000], Loss: 0.5141\n",
      "Epoch [386/1000], Loss: 0.5223\n",
      "Epoch [386/1000], Loss: 0.5423\n",
      "Epoch [386/1000], Loss: 0.4214\n",
      "Epoch [386/1000], Loss: 0.2126\n",
      "Epoch [386/1000], Loss: 0.1871\n",
      "Epoch [386/1000], Loss: 0.3646\n",
      "Epoch [386/1000], Loss: 0.4648\n",
      "Epoch [386/1000], Loss: 0.5173\n",
      "Epoch [386/1000], Loss: 0.4547\n",
      "Epoch [386/1000], Loss: 0.1812\n",
      "tensor(2.2560, grad_fn=<MeanBackward0>)\n",
      "386\n",
      "Epoch [387/1000], Loss: 0.2286\n",
      "Epoch [387/1000], Loss: 0.3213\n",
      "Epoch [387/1000], Loss: 0.4462\n",
      "Epoch [387/1000], Loss: 0.4820\n",
      "Epoch [387/1000], Loss: 0.3233\n",
      "Epoch [387/1000], Loss: 0.1958\n",
      "Epoch [387/1000], Loss: 0.1439\n",
      "Epoch [387/1000], Loss: 0.2383\n",
      "Epoch [387/1000], Loss: 0.3214\n",
      "Epoch [387/1000], Loss: 0.3002\n",
      "Epoch [387/1000], Loss: 0.1668\n",
      "tensor(2.3307, grad_fn=<MeanBackward0>)\n",
      "387\n",
      "Epoch [388/1000], Loss: 0.1573\n",
      "Epoch [388/1000], Loss: 0.2118\n",
      "Epoch [388/1000], Loss: 0.2865\n",
      "Epoch [388/1000], Loss: 0.2967\n",
      "Epoch [388/1000], Loss: 0.1469\n",
      "Epoch [388/1000], Loss: 0.0974\n",
      "Epoch [388/1000], Loss: 0.1772\n",
      "Epoch [388/1000], Loss: 0.2495\n",
      "Epoch [388/1000], Loss: 0.2337\n",
      "Epoch [388/1000], Loss: 0.1561\n",
      "Epoch [388/1000], Loss: 0.1339\n",
      "tensor(2.1818, grad_fn=<MeanBackward0>)\n",
      "388\n",
      "Epoch [389/1000], Loss: 0.2188\n",
      "Epoch [389/1000], Loss: 0.2321\n",
      "Epoch [389/1000], Loss: 0.2238\n",
      "Epoch [389/1000], Loss: 0.1417\n",
      "Epoch [389/1000], Loss: 0.1427\n",
      "Epoch [389/1000], Loss: 0.2200\n",
      "Epoch [389/1000], Loss: 0.2483\n",
      "Epoch [389/1000], Loss: 0.2265\n",
      "Epoch [389/1000], Loss: 0.1237\n",
      "Epoch [389/1000], Loss: 0.1239\n",
      "Epoch [389/1000], Loss: 0.2199\n",
      "tensor(2.1543, grad_fn=<MeanBackward0>)\n",
      "389\n",
      "Epoch [390/1000], Loss: 0.2790\n",
      "Epoch [390/1000], Loss: 0.2345\n",
      "Epoch [390/1000], Loss: 0.2235\n",
      "Epoch [390/1000], Loss: 0.1138\n",
      "Epoch [390/1000], Loss: 0.1595\n",
      "Epoch [390/1000], Loss: 0.2336\n",
      "Epoch [390/1000], Loss: 0.2743\n",
      "Epoch [390/1000], Loss: 0.2416\n",
      "Epoch [390/1000], Loss: 0.1484\n",
      "Epoch [390/1000], Loss: 0.1209\n",
      "Epoch [390/1000], Loss: 0.2170\n",
      "tensor(2.1208, grad_fn=<MeanBackward0>)\n",
      "390\n",
      "Epoch [391/1000], Loss: 0.3112\n",
      "Epoch [391/1000], Loss: 0.3095\n",
      "Epoch [391/1000], Loss: 0.2741\n",
      "Epoch [391/1000], Loss: 0.1709\n",
      "Epoch [391/1000], Loss: 0.1103\n",
      "Epoch [391/1000], Loss: 0.2277\n",
      "Epoch [391/1000], Loss: 0.3003\n",
      "Epoch [391/1000], Loss: 0.3062\n",
      "Epoch [391/1000], Loss: 0.2248\n",
      "Epoch [391/1000], Loss: 0.1538\n",
      "Epoch [391/1000], Loss: 0.1460\n",
      "tensor(2.1294, grad_fn=<MeanBackward0>)\n",
      "391\n",
      "Epoch [392/1000], Loss: 0.2685\n",
      "Epoch [392/1000], Loss: 0.2845\n",
      "Epoch [392/1000], Loss: 0.3396\n",
      "Epoch [392/1000], Loss: 0.2813\n",
      "Epoch [392/1000], Loss: 0.1481\n",
      "Epoch [392/1000], Loss: 0.1154\n",
      "Epoch [392/1000], Loss: 0.2505\n",
      "Epoch [392/1000], Loss: 0.3342\n",
      "Epoch [392/1000], Loss: 0.3522\n",
      "Epoch [392/1000], Loss: 0.3100\n",
      "Epoch [392/1000], Loss: 0.1324\n",
      "tensor(2.2307, grad_fn=<MeanBackward0>)\n",
      "392\n",
      "Epoch [393/1000], Loss: 0.1687\n",
      "Epoch [393/1000], Loss: 0.2802\n",
      "Epoch [393/1000], Loss: 0.3999\n",
      "Epoch [393/1000], Loss: 0.4493\n",
      "Epoch [393/1000], Loss: 0.3415\n",
      "Epoch [393/1000], Loss: 0.2569\n",
      "Epoch [393/1000], Loss: 0.1268\n",
      "Epoch [393/1000], Loss: 0.2107\n",
      "Epoch [393/1000], Loss: 0.3445\n",
      "Epoch [393/1000], Loss: 0.4171\n",
      "Epoch [393/1000], Loss: 0.3506\n",
      "tensor(2.4258, grad_fn=<MeanBackward0>)\n",
      "393\n",
      "Epoch [394/1000], Loss: 0.2061\n",
      "Epoch [394/1000], Loss: 0.1551\n",
      "Epoch [394/1000], Loss: 0.2788\n",
      "Epoch [394/1000], Loss: 0.4180\n",
      "Epoch [394/1000], Loss: 0.4199\n",
      "Epoch [394/1000], Loss: 0.4312\n",
      "Epoch [394/1000], Loss: 0.2972\n",
      "Epoch [394/1000], Loss: 0.1800\n",
      "Epoch [394/1000], Loss: 0.1555\n",
      "Epoch [394/1000], Loss: 0.3729\n",
      "Epoch [394/1000], Loss: 0.4562\n",
      "tensor(2.6256, grad_fn=<MeanBackward0>)\n",
      "394\n",
      "Epoch [395/1000], Loss: 0.4163\n",
      "Epoch [395/1000], Loss: 0.3265\n",
      "Epoch [395/1000], Loss: 0.2095\n",
      "Epoch [395/1000], Loss: 0.2468\n",
      "Epoch [395/1000], Loss: 0.3207\n",
      "Epoch [395/1000], Loss: 0.4142\n",
      "Epoch [395/1000], Loss: 0.3273\n",
      "Epoch [395/1000], Loss: 0.3089\n",
      "Epoch [395/1000], Loss: 0.1929\n",
      "Epoch [395/1000], Loss: 0.1288\n",
      "Epoch [395/1000], Loss: 0.2795\n",
      "tensor(2.5342, grad_fn=<MeanBackward0>)\n",
      "395\n",
      "Epoch [396/1000], Loss: 0.3539\n",
      "Epoch [396/1000], Loss: 0.3231\n",
      "Epoch [396/1000], Loss: 0.1900\n",
      "Epoch [396/1000], Loss: 0.1695\n",
      "Epoch [396/1000], Loss: 0.2119\n",
      "Epoch [396/1000], Loss: 0.2598\n",
      "Epoch [396/1000], Loss: 0.1757\n",
      "Epoch [396/1000], Loss: 0.1065\n",
      "Epoch [396/1000], Loss: 0.0931\n",
      "Epoch [396/1000], Loss: 0.1641\n",
      "Epoch [396/1000], Loss: 0.1819\n",
      "tensor(2.4294, grad_fn=<MeanBackward0>)\n",
      "396\n",
      "Epoch [397/1000], Loss: 0.1142\n",
      "Epoch [397/1000], Loss: 0.1433\n",
      "Epoch [397/1000], Loss: 0.1922\n",
      "Epoch [397/1000], Loss: 0.1532\n",
      "Epoch [397/1000], Loss: 0.1071\n",
      "Epoch [397/1000], Loss: 0.0954\n",
      "Epoch [397/1000], Loss: 0.0995\n",
      "Epoch [397/1000], Loss: 0.1252\n",
      "Epoch [397/1000], Loss: 0.0962\n",
      "Epoch [397/1000], Loss: 0.0963\n",
      "Epoch [397/1000], Loss: 0.0798\n",
      "tensor(2.3563, grad_fn=<MeanBackward0>)\n",
      "397\n",
      "Epoch [398/1000], Loss: 0.0688\n",
      "Epoch [398/1000], Loss: 0.0726\n",
      "Epoch [398/1000], Loss: 0.0836\n",
      "Epoch [398/1000], Loss: 0.0746\n",
      "Epoch [398/1000], Loss: 0.0608\n",
      "Epoch [398/1000], Loss: 0.0770\n",
      "Epoch [398/1000], Loss: 0.0643\n",
      "Epoch [398/1000], Loss: 0.0631\n",
      "Epoch [398/1000], Loss: 0.0675\n",
      "Epoch [398/1000], Loss: 0.0662\n",
      "Epoch [398/1000], Loss: 0.0512\n",
      "tensor(2.3721, grad_fn=<MeanBackward0>)\n",
      "398\n",
      "Epoch [399/1000], Loss: 0.0629\n",
      "Epoch [399/1000], Loss: 0.0562\n",
      "Epoch [399/1000], Loss: 0.0583\n",
      "Epoch [399/1000], Loss: 0.0743\n",
      "Epoch [399/1000], Loss: 0.0506\n",
      "Epoch [399/1000], Loss: 0.0594\n",
      "Epoch [399/1000], Loss: 0.0624\n",
      "Epoch [399/1000], Loss: 0.0533\n",
      "Epoch [399/1000], Loss: 0.0551\n",
      "Epoch [399/1000], Loss: 0.0712\n",
      "Epoch [399/1000], Loss: 0.0809\n",
      "tensor(2.3517, grad_fn=<MeanBackward0>)\n",
      "399\n",
      "Epoch [400/1000], Loss: 0.0630\n",
      "Epoch [400/1000], Loss: 0.0808\n",
      "Epoch [400/1000], Loss: 0.0903\n",
      "Epoch [400/1000], Loss: 0.0512\n",
      "Epoch [400/1000], Loss: 0.0742\n",
      "Epoch [400/1000], Loss: 0.0524\n",
      "Epoch [400/1000], Loss: 0.0500\n",
      "Epoch [400/1000], Loss: 0.0875\n",
      "Epoch [400/1000], Loss: 0.0746\n",
      "Epoch [400/1000], Loss: 0.0665\n",
      "Epoch [400/1000], Loss: 0.1273\n",
      "tensor(2.4379, grad_fn=<MeanBackward0>)\n",
      "400\n",
      "Epoch [401/1000], Loss: 0.1123\n",
      "Epoch [401/1000], Loss: 0.0624\n",
      "Epoch [401/1000], Loss: 0.1203\n",
      "Epoch [401/1000], Loss: 0.1294\n",
      "Epoch [401/1000], Loss: 0.0752\n",
      "Epoch [401/1000], Loss: 0.0801\n",
      "Epoch [401/1000], Loss: 0.0965\n",
      "Epoch [401/1000], Loss: 0.0472\n",
      "Epoch [401/1000], Loss: 0.1107\n",
      "Epoch [401/1000], Loss: 0.1209\n",
      "Epoch [401/1000], Loss: 0.0574\n",
      "tensor(2.4688, grad_fn=<MeanBackward0>)\n",
      "401\n",
      "Epoch [402/1000], Loss: 0.1464\n",
      "Epoch [402/1000], Loss: 0.2030\n",
      "Epoch [402/1000], Loss: 0.1276\n",
      "Epoch [402/1000], Loss: 0.0620\n",
      "Epoch [402/1000], Loss: 0.1259\n",
      "Epoch [402/1000], Loss: 0.1227\n",
      "Epoch [402/1000], Loss: 0.0607\n",
      "Epoch [402/1000], Loss: 0.0839\n",
      "Epoch [402/1000], Loss: 0.0838\n",
      "Epoch [402/1000], Loss: 0.0930\n",
      "Epoch [402/1000], Loss: 0.0624\n",
      "tensor(2.4197, grad_fn=<MeanBackward0>)\n",
      "402\n",
      "Epoch [403/1000], Loss: 0.0503\n",
      "Epoch [403/1000], Loss: 0.0691\n",
      "Epoch [403/1000], Loss: 0.0505\n",
      "Epoch [403/1000], Loss: 0.0686\n",
      "Epoch [403/1000], Loss: 0.0442\n",
      "Epoch [403/1000], Loss: 0.0431\n",
      "Epoch [403/1000], Loss: 0.0536\n",
      "Epoch [403/1000], Loss: 0.0383\n",
      "Epoch [403/1000], Loss: 0.0376\n",
      "Epoch [403/1000], Loss: 0.0371\n",
      "Epoch [403/1000], Loss: 0.0569\n",
      "tensor(2.4037, grad_fn=<MeanBackward0>)\n",
      "403\n",
      "Epoch [404/1000], Loss: 0.0455\n",
      "Epoch [404/1000], Loss: 0.0592\n",
      "Epoch [404/1000], Loss: 0.0575\n",
      "Epoch [404/1000], Loss: 0.0401\n",
      "Epoch [404/1000], Loss: 0.0387\n",
      "Epoch [404/1000], Loss: 0.0360\n",
      "Epoch [404/1000], Loss: 0.0437\n",
      "Epoch [404/1000], Loss: 0.0398\n",
      "Epoch [404/1000], Loss: 0.0453\n",
      "Epoch [404/1000], Loss: 0.0662\n",
      "Epoch [404/1000], Loss: 0.0447\n",
      "tensor(2.4380, grad_fn=<MeanBackward0>)\n",
      "404\n",
      "Epoch [405/1000], Loss: 0.0604\n",
      "Epoch [405/1000], Loss: 0.0637\n",
      "Epoch [405/1000], Loss: 0.0444\n",
      "Epoch [405/1000], Loss: 0.0742\n",
      "Epoch [405/1000], Loss: 0.0613\n",
      "Epoch [405/1000], Loss: 0.0599\n",
      "Epoch [405/1000], Loss: 0.0470\n",
      "Epoch [405/1000], Loss: 0.0374\n",
      "Epoch [405/1000], Loss: 0.0526\n",
      "Epoch [405/1000], Loss: 0.0614\n",
      "Epoch [405/1000], Loss: 0.0481\n",
      "tensor(2.4618, grad_fn=<MeanBackward0>)\n",
      "405\n",
      "Epoch [406/1000], Loss: 0.0953\n",
      "Epoch [406/1000], Loss: 0.1052\n",
      "Epoch [406/1000], Loss: 0.0621\n",
      "Epoch [406/1000], Loss: 0.0697\n",
      "Epoch [406/1000], Loss: 0.0737\n",
      "Epoch [406/1000], Loss: 0.0582\n",
      "Epoch [406/1000], Loss: 0.0581\n",
      "Epoch [406/1000], Loss: 0.0802\n",
      "Epoch [406/1000], Loss: 0.0619\n",
      "Epoch [406/1000], Loss: 0.0993\n",
      "Epoch [406/1000], Loss: 0.1237\n",
      "tensor(2.4013, grad_fn=<MeanBackward0>)\n",
      "406\n",
      "Epoch [407/1000], Loss: 0.0663\n",
      "Epoch [407/1000], Loss: 0.1225\n",
      "Epoch [407/1000], Loss: 0.2250\n",
      "Epoch [407/1000], Loss: 0.2251\n",
      "Epoch [407/1000], Loss: 0.1164\n",
      "Epoch [407/1000], Loss: 0.1183\n",
      "Epoch [407/1000], Loss: 0.2079\n",
      "Epoch [407/1000], Loss: 0.2423\n",
      "Epoch [407/1000], Loss: 0.1759\n",
      "Epoch [407/1000], Loss: 0.0771\n",
      "Epoch [407/1000], Loss: 0.1303\n",
      "tensor(2.4922, grad_fn=<MeanBackward0>)\n",
      "407\n",
      "Epoch [408/1000], Loss: 0.1096\n",
      "Epoch [408/1000], Loss: 0.0809\n",
      "Epoch [408/1000], Loss: 0.0696\n",
      "Epoch [408/1000], Loss: 0.0435\n",
      "Epoch [408/1000], Loss: 0.0558\n",
      "Epoch [408/1000], Loss: 0.0730\n",
      "Epoch [408/1000], Loss: 0.0786\n",
      "Epoch [408/1000], Loss: 0.0606\n",
      "Epoch [408/1000], Loss: 0.1028\n",
      "Epoch [408/1000], Loss: 0.0932\n",
      "Epoch [408/1000], Loss: 0.0680\n",
      "tensor(2.3731, grad_fn=<MeanBackward0>)\n",
      "408\n",
      "Epoch [409/1000], Loss: 0.0995\n",
      "Epoch [409/1000], Loss: 0.0662\n",
      "Epoch [409/1000], Loss: 0.1410\n",
      "Epoch [409/1000], Loss: 0.1938\n",
      "Epoch [409/1000], Loss: 0.1475\n",
      "Epoch [409/1000], Loss: 0.1143\n",
      "Epoch [409/1000], Loss: 0.1717\n",
      "Epoch [409/1000], Loss: 0.2521\n",
      "Epoch [409/1000], Loss: 0.2116\n",
      "Epoch [409/1000], Loss: 0.1104\n",
      "Epoch [409/1000], Loss: 0.0928\n",
      "tensor(2.4956, grad_fn=<MeanBackward0>)\n",
      "409\n",
      "Epoch [410/1000], Loss: 0.1513\n",
      "Epoch [410/1000], Loss: 0.1495\n",
      "Epoch [410/1000], Loss: 0.1010\n",
      "Epoch [410/1000], Loss: 0.0693\n",
      "Epoch [410/1000], Loss: 0.0857\n",
      "Epoch [410/1000], Loss: 0.0861\n",
      "Epoch [410/1000], Loss: 0.0767\n",
      "Epoch [410/1000], Loss: 0.0551\n",
      "Epoch [410/1000], Loss: 0.0824\n",
      "Epoch [410/1000], Loss: 0.0823\n",
      "Epoch [410/1000], Loss: 0.0725\n",
      "tensor(2.3900, grad_fn=<MeanBackward0>)\n",
      "410\n",
      "Epoch [411/1000], Loss: 0.1009\n",
      "Epoch [411/1000], Loss: 0.0627\n",
      "Epoch [411/1000], Loss: 0.1609\n",
      "Epoch [411/1000], Loss: 0.2657\n",
      "Epoch [411/1000], Loss: 0.2439\n",
      "Epoch [411/1000], Loss: 0.1462\n",
      "Epoch [411/1000], Loss: 0.1453\n",
      "Epoch [411/1000], Loss: 0.2405\n",
      "Epoch [411/1000], Loss: 0.2669\n",
      "Epoch [411/1000], Loss: 0.2200\n",
      "Epoch [411/1000], Loss: 0.1681\n",
      "tensor(2.4000, grad_fn=<MeanBackward0>)\n",
      "411\n",
      "Epoch [412/1000], Loss: 0.1191\n",
      "Epoch [412/1000], Loss: 0.1612\n",
      "Epoch [412/1000], Loss: 0.1746\n",
      "Epoch [412/1000], Loss: 0.1568\n",
      "Epoch [412/1000], Loss: 0.1049\n",
      "Epoch [412/1000], Loss: 0.1058\n",
      "Epoch [412/1000], Loss: 0.1465\n",
      "Epoch [412/1000], Loss: 0.2162\n",
      "Epoch [412/1000], Loss: 0.2084\n",
      "Epoch [412/1000], Loss: 0.1190\n",
      "Epoch [412/1000], Loss: 0.1165\n",
      "tensor(2.5149, grad_fn=<MeanBackward0>)\n",
      "412\n",
      "Epoch [413/1000], Loss: 0.1446\n",
      "Epoch [413/1000], Loss: 0.1347\n",
      "Epoch [413/1000], Loss: 0.1251\n",
      "Epoch [413/1000], Loss: 0.1001\n",
      "Epoch [413/1000], Loss: 0.0933\n",
      "Epoch [413/1000], Loss: 0.0910\n",
      "Epoch [413/1000], Loss: 0.1099\n",
      "Epoch [413/1000], Loss: 0.1148\n",
      "Epoch [413/1000], Loss: 0.0832\n",
      "Epoch [413/1000], Loss: 0.0947\n",
      "Epoch [413/1000], Loss: 0.0881\n",
      "tensor(2.4585, grad_fn=<MeanBackward0>)\n",
      "413\n",
      "Epoch [414/1000], Loss: 0.0743\n",
      "Epoch [414/1000], Loss: 0.0868\n",
      "Epoch [414/1000], Loss: 0.0682\n",
      "Epoch [414/1000], Loss: 0.1398\n",
      "Epoch [414/1000], Loss: 0.1463\n",
      "Epoch [414/1000], Loss: 0.0935\n",
      "Epoch [414/1000], Loss: 0.1251\n",
      "Epoch [414/1000], Loss: 0.2354\n",
      "Epoch [414/1000], Loss: 0.2838\n",
      "Epoch [414/1000], Loss: 0.3116\n",
      "Epoch [414/1000], Loss: 0.2059\n",
      "tensor(2.4014, grad_fn=<MeanBackward0>)\n",
      "414\n",
      "Epoch [415/1000], Loss: 0.0757\n",
      "Epoch [415/1000], Loss: 0.1836\n",
      "Epoch [415/1000], Loss: 0.2954\n",
      "Epoch [415/1000], Loss: 0.3355\n",
      "Epoch [415/1000], Loss: 0.3037\n",
      "Epoch [415/1000], Loss: 0.2028\n",
      "Epoch [415/1000], Loss: 0.1277\n",
      "Epoch [415/1000], Loss: 0.2127\n",
      "Epoch [415/1000], Loss: 0.3647\n",
      "Epoch [415/1000], Loss: 0.3677\n",
      "Epoch [415/1000], Loss: 0.3569\n",
      "tensor(2.2516, grad_fn=<MeanBackward0>)\n",
      "415\n",
      "Epoch [416/1000], Loss: 0.2545\n",
      "Epoch [416/1000], Loss: 0.1217\n",
      "Epoch [416/1000], Loss: 0.1824\n",
      "Epoch [416/1000], Loss: 0.3399\n",
      "Epoch [416/1000], Loss: 0.4483\n",
      "Epoch [416/1000], Loss: 0.4300\n",
      "Epoch [416/1000], Loss: 0.3256\n",
      "Epoch [416/1000], Loss: 0.2000\n",
      "Epoch [416/1000], Loss: 0.1462\n",
      "Epoch [416/1000], Loss: 0.3053\n",
      "Epoch [416/1000], Loss: 0.4553\n",
      "tensor(2.0756, grad_fn=<MeanBackward0>)\n",
      "416\n",
      "Epoch [417/1000], Loss: 0.4827\n",
      "Epoch [417/1000], Loss: 0.4523\n",
      "Epoch [417/1000], Loss: 0.3473\n",
      "Epoch [417/1000], Loss: 0.1298\n",
      "Epoch [417/1000], Loss: 0.2247\n",
      "Epoch [417/1000], Loss: 0.4047\n",
      "Epoch [417/1000], Loss: 0.4941\n",
      "Epoch [417/1000], Loss: 0.4510\n",
      "Epoch [417/1000], Loss: 0.3649\n",
      "Epoch [417/1000], Loss: 0.2213\n",
      "Epoch [417/1000], Loss: 0.1977\n",
      "tensor(2.1429, grad_fn=<MeanBackward0>)\n",
      "417\n",
      "Epoch [418/1000], Loss: 0.3255\n",
      "Epoch [418/1000], Loss: 0.4136\n",
      "Epoch [418/1000], Loss: 0.4813\n",
      "Epoch [418/1000], Loss: 0.4244\n",
      "Epoch [418/1000], Loss: 0.2454\n",
      "Epoch [418/1000], Loss: 0.0930\n",
      "Epoch [418/1000], Loss: 0.2280\n",
      "Epoch [418/1000], Loss: 0.3248\n",
      "Epoch [418/1000], Loss: 0.3904\n",
      "Epoch [418/1000], Loss: 0.3046\n",
      "Epoch [418/1000], Loss: 0.1343\n",
      "tensor(2.2746, grad_fn=<MeanBackward0>)\n",
      "418\n",
      "Epoch [419/1000], Loss: 0.1741\n",
      "Epoch [419/1000], Loss: 0.2896\n",
      "Epoch [419/1000], Loss: 0.3563\n",
      "Epoch [419/1000], Loss: 0.3465\n",
      "Epoch [419/1000], Loss: 0.2009\n",
      "Epoch [419/1000], Loss: 0.1308\n",
      "Epoch [419/1000], Loss: 0.1604\n",
      "Epoch [419/1000], Loss: 0.2250\n",
      "Epoch [419/1000], Loss: 0.2367\n",
      "Epoch [419/1000], Loss: 0.1939\n",
      "Epoch [419/1000], Loss: 0.1091\n",
      "tensor(2.2727, grad_fn=<MeanBackward0>)\n",
      "419\n",
      "Epoch [420/1000], Loss: 0.1549\n",
      "Epoch [420/1000], Loss: 0.2106\n",
      "Epoch [420/1000], Loss: 0.2529\n",
      "Epoch [420/1000], Loss: 0.1930\n",
      "Epoch [420/1000], Loss: 0.0921\n",
      "Epoch [420/1000], Loss: 0.1330\n",
      "Epoch [420/1000], Loss: 0.1808\n",
      "Epoch [420/1000], Loss: 0.1978\n",
      "Epoch [420/1000], Loss: 0.1640\n",
      "Epoch [420/1000], Loss: 0.1030\n",
      "Epoch [420/1000], Loss: 0.1083\n",
      "tensor(2.2217, grad_fn=<MeanBackward0>)\n",
      "420\n",
      "Epoch [421/1000], Loss: 0.1550\n",
      "Epoch [421/1000], Loss: 0.1533\n",
      "Epoch [421/1000], Loss: 0.1247\n",
      "Epoch [421/1000], Loss: 0.0815\n",
      "Epoch [421/1000], Loss: 0.1289\n",
      "Epoch [421/1000], Loss: 0.1316\n",
      "Epoch [421/1000], Loss: 0.1067\n",
      "Epoch [421/1000], Loss: 0.0838\n",
      "Epoch [421/1000], Loss: 0.0856\n",
      "Epoch [421/1000], Loss: 0.0818\n",
      "Epoch [421/1000], Loss: 0.0646\n",
      "tensor(2.3288, grad_fn=<MeanBackward0>)\n",
      "421\n",
      "Epoch [422/1000], Loss: 0.0747\n",
      "Epoch [422/1000], Loss: 0.0891\n",
      "Epoch [422/1000], Loss: 0.1127\n",
      "Epoch [422/1000], Loss: 0.0657\n",
      "Epoch [422/1000], Loss: 0.0682\n",
      "Epoch [422/1000], Loss: 0.1045\n",
      "Epoch [422/1000], Loss: 0.0990\n",
      "Epoch [422/1000], Loss: 0.0643\n",
      "Epoch [422/1000], Loss: 0.0670\n",
      "Epoch [422/1000], Loss: 0.0716\n",
      "Epoch [422/1000], Loss: 0.0595\n",
      "tensor(2.3361, grad_fn=<MeanBackward0>)\n",
      "422\n",
      "Epoch [423/1000], Loss: 0.0598\n",
      "Epoch [423/1000], Loss: 0.0702\n",
      "Epoch [423/1000], Loss: 0.0834\n",
      "Epoch [423/1000], Loss: 0.0436\n",
      "Epoch [423/1000], Loss: 0.0503\n",
      "Epoch [423/1000], Loss: 0.0581\n",
      "Epoch [423/1000], Loss: 0.0627\n",
      "Epoch [423/1000], Loss: 0.0372\n",
      "Epoch [423/1000], Loss: 0.0437\n",
      "Epoch [423/1000], Loss: 0.0545\n",
      "Epoch [423/1000], Loss: 0.0326\n",
      "tensor(2.3533, grad_fn=<MeanBackward0>)\n",
      "423\n",
      "Epoch [424/1000], Loss: 0.0478\n",
      "Epoch [424/1000], Loss: 0.0473\n",
      "Epoch [424/1000], Loss: 0.0498\n",
      "Epoch [424/1000], Loss: 0.0443\n",
      "Epoch [424/1000], Loss: 0.0418\n",
      "Epoch [424/1000], Loss: 0.0397\n",
      "Epoch [424/1000], Loss: 0.0592\n",
      "Epoch [424/1000], Loss: 0.0449\n",
      "Epoch [424/1000], Loss: 0.0455\n",
      "Epoch [424/1000], Loss: 0.0507\n",
      "Epoch [424/1000], Loss: 0.0388\n",
      "tensor(2.3369, grad_fn=<MeanBackward0>)\n",
      "424\n",
      "Epoch [425/1000], Loss: 0.0446\n",
      "Epoch [425/1000], Loss: 0.0462\n",
      "Epoch [425/1000], Loss: 0.0511\n",
      "Epoch [425/1000], Loss: 0.0473\n",
      "Epoch [425/1000], Loss: 0.0379\n",
      "Epoch [425/1000], Loss: 0.0491\n",
      "Epoch [425/1000], Loss: 0.0588\n",
      "Epoch [425/1000], Loss: 0.0567\n",
      "Epoch [425/1000], Loss: 0.0524\n",
      "Epoch [425/1000], Loss: 0.0633\n",
      "Epoch [425/1000], Loss: 0.0747\n",
      "tensor(2.3465, grad_fn=<MeanBackward0>)\n",
      "425\n",
      "Epoch [426/1000], Loss: 0.0516\n",
      "Epoch [426/1000], Loss: 0.0674\n",
      "Epoch [426/1000], Loss: 0.0849\n",
      "Epoch [426/1000], Loss: 0.0737\n",
      "Epoch [426/1000], Loss: 0.0767\n",
      "Epoch [426/1000], Loss: 0.0688\n",
      "Epoch [426/1000], Loss: 0.0527\n",
      "Epoch [426/1000], Loss: 0.0712\n",
      "Epoch [426/1000], Loss: 0.0509\n",
      "Epoch [426/1000], Loss: 0.0846\n",
      "Epoch [426/1000], Loss: 0.1063\n",
      "tensor(2.3193, grad_fn=<MeanBackward0>)\n",
      "426\n",
      "Epoch [427/1000], Loss: 0.0895\n",
      "Epoch [427/1000], Loss: 0.0644\n",
      "Epoch [427/1000], Loss: 0.0712\n",
      "Epoch [427/1000], Loss: 0.0718\n",
      "Epoch [427/1000], Loss: 0.0572\n",
      "Epoch [427/1000], Loss: 0.0795\n",
      "Epoch [427/1000], Loss: 0.0769\n",
      "Epoch [427/1000], Loss: 0.0692\n",
      "Epoch [427/1000], Loss: 0.1091\n",
      "Epoch [427/1000], Loss: 0.0881\n",
      "Epoch [427/1000], Loss: 0.0791\n",
      "tensor(2.2793, grad_fn=<MeanBackward0>)\n",
      "427\n",
      "Epoch [428/1000], Loss: 0.1591\n",
      "Epoch [428/1000], Loss: 0.1805\n",
      "Epoch [428/1000], Loss: 0.1510\n",
      "Epoch [428/1000], Loss: 0.1073\n",
      "Epoch [428/1000], Loss: 0.1246\n",
      "Epoch [428/1000], Loss: 0.1316\n",
      "Epoch [428/1000], Loss: 0.0875\n",
      "Epoch [428/1000], Loss: 0.0704\n",
      "Epoch [428/1000], Loss: 0.0924\n",
      "Epoch [428/1000], Loss: 0.1073\n",
      "Epoch [428/1000], Loss: 0.0860\n",
      "tensor(2.3141, grad_fn=<MeanBackward0>)\n",
      "428\n",
      "Epoch [429/1000], Loss: 0.0790\n",
      "Epoch [429/1000], Loss: 0.0842\n",
      "Epoch [429/1000], Loss: 0.1078\n",
      "Epoch [429/1000], Loss: 0.0531\n",
      "Epoch [429/1000], Loss: 0.0974\n",
      "Epoch [429/1000], Loss: 0.1360\n",
      "Epoch [429/1000], Loss: 0.1056\n",
      "Epoch [429/1000], Loss: 0.0655\n",
      "Epoch [429/1000], Loss: 0.0602\n",
      "Epoch [429/1000], Loss: 0.0470\n",
      "Epoch [429/1000], Loss: 0.0483\n",
      "tensor(2.2942, grad_fn=<MeanBackward0>)\n",
      "429\n",
      "Epoch [430/1000], Loss: 0.0540\n",
      "Epoch [430/1000], Loss: 0.0557\n",
      "Epoch [430/1000], Loss: 0.0780\n",
      "Epoch [430/1000], Loss: 0.0431\n",
      "Epoch [430/1000], Loss: 0.0529\n",
      "Epoch [430/1000], Loss: 0.0492\n",
      "Epoch [430/1000], Loss: 0.0403\n",
      "Epoch [430/1000], Loss: 0.0446\n",
      "Epoch [430/1000], Loss: 0.0471\n",
      "Epoch [430/1000], Loss: 0.0491\n",
      "Epoch [430/1000], Loss: 0.0559\n",
      "tensor(2.2923, grad_fn=<MeanBackward0>)\n",
      "430\n",
      "Epoch [431/1000], Loss: 0.0737\n",
      "Epoch [431/1000], Loss: 0.0531\n",
      "Epoch [431/1000], Loss: 0.0532\n",
      "Epoch [431/1000], Loss: 0.0364\n",
      "Epoch [431/1000], Loss: 0.0346\n",
      "Epoch [431/1000], Loss: 0.0455\n",
      "Epoch [431/1000], Loss: 0.0412\n",
      "Epoch [431/1000], Loss: 0.0384\n",
      "Epoch [431/1000], Loss: 0.0359\n",
      "Epoch [431/1000], Loss: 0.0490\n",
      "Epoch [431/1000], Loss: 0.0466\n",
      "tensor(2.3017, grad_fn=<MeanBackward0>)\n",
      "431\n",
      "Epoch [432/1000], Loss: 0.0555\n",
      "Epoch [432/1000], Loss: 0.0888\n",
      "Epoch [432/1000], Loss: 0.0563\n",
      "Epoch [432/1000], Loss: 0.0440\n",
      "Epoch [432/1000], Loss: 0.0457\n",
      "Epoch [432/1000], Loss: 0.0398\n",
      "Epoch [432/1000], Loss: 0.0431\n",
      "Epoch [432/1000], Loss: 0.0571\n",
      "Epoch [432/1000], Loss: 0.0589\n",
      "Epoch [432/1000], Loss: 0.0499\n",
      "Epoch [432/1000], Loss: 0.0466\n",
      "tensor(2.3159, grad_fn=<MeanBackward0>)\n",
      "432\n",
      "Epoch [433/1000], Loss: 0.0476\n",
      "Epoch [433/1000], Loss: 0.0542\n",
      "Epoch [433/1000], Loss: 0.0606\n",
      "Epoch [433/1000], Loss: 0.0641\n",
      "Epoch [433/1000], Loss: 0.0638\n",
      "Epoch [433/1000], Loss: 0.0866\n",
      "Epoch [433/1000], Loss: 0.0780\n",
      "Epoch [433/1000], Loss: 0.0813\n",
      "Epoch [433/1000], Loss: 0.0750\n",
      "Epoch [433/1000], Loss: 0.0754\n",
      "Epoch [433/1000], Loss: 0.0763\n",
      "tensor(2.3179, grad_fn=<MeanBackward0>)\n",
      "433\n",
      "Epoch [434/1000], Loss: 0.0699\n",
      "Epoch [434/1000], Loss: 0.0824\n",
      "Epoch [434/1000], Loss: 0.0964\n",
      "Epoch [434/1000], Loss: 0.0736\n",
      "Epoch [434/1000], Loss: 0.0692\n",
      "Epoch [434/1000], Loss: 0.1148\n",
      "Epoch [434/1000], Loss: 0.1426\n",
      "Epoch [434/1000], Loss: 0.1165\n",
      "Epoch [434/1000], Loss: 0.0868\n",
      "Epoch [434/1000], Loss: 0.1078\n",
      "Epoch [434/1000], Loss: 0.1168\n",
      "tensor(2.2664, grad_fn=<MeanBackward0>)\n",
      "434\n",
      "Epoch [435/1000], Loss: 0.0794\n",
      "Epoch [435/1000], Loss: 0.0936\n",
      "Epoch [435/1000], Loss: 0.0982\n",
      "Epoch [435/1000], Loss: 0.0858\n",
      "Epoch [435/1000], Loss: 0.1031\n",
      "Epoch [435/1000], Loss: 0.0712\n",
      "Epoch [435/1000], Loss: 0.1224\n",
      "Epoch [435/1000], Loss: 0.1616\n",
      "Epoch [435/1000], Loss: 0.1597\n",
      "Epoch [435/1000], Loss: 0.0626\n",
      "Epoch [435/1000], Loss: 0.1222\n",
      "tensor(2.1629, grad_fn=<MeanBackward0>)\n",
      "435\n",
      "Epoch [436/1000], Loss: 0.1705\n",
      "Epoch [436/1000], Loss: 0.1590\n",
      "Epoch [436/1000], Loss: 0.0926\n",
      "Epoch [436/1000], Loss: 0.0781\n",
      "Epoch [436/1000], Loss: 0.0957\n",
      "Epoch [436/1000], Loss: 0.0866\n",
      "Epoch [436/1000], Loss: 0.0505\n",
      "Epoch [436/1000], Loss: 0.0480\n",
      "Epoch [436/1000], Loss: 0.0402\n",
      "Epoch [436/1000], Loss: 0.0617\n",
      "Epoch [436/1000], Loss: 0.0476\n",
      "tensor(2.2289, grad_fn=<MeanBackward0>)\n",
      "436\n",
      "Epoch [437/1000], Loss: 0.0791\n",
      "Epoch [437/1000], Loss: 0.0766\n",
      "Epoch [437/1000], Loss: 0.0489\n",
      "Epoch [437/1000], Loss: 0.0428\n",
      "Epoch [437/1000], Loss: 0.0531\n",
      "Epoch [437/1000], Loss: 0.0591\n",
      "Epoch [437/1000], Loss: 0.0410\n",
      "Epoch [437/1000], Loss: 0.0418\n",
      "Epoch [437/1000], Loss: 0.0494\n",
      "Epoch [437/1000], Loss: 0.0480\n",
      "Epoch [437/1000], Loss: 0.0428\n",
      "tensor(2.2233, grad_fn=<MeanBackward0>)\n",
      "437\n",
      "Epoch [438/1000], Loss: 0.0649\n",
      "Epoch [438/1000], Loss: 0.0671\n",
      "Epoch [438/1000], Loss: 0.0644\n",
      "Epoch [438/1000], Loss: 0.0611\n",
      "Epoch [438/1000], Loss: 0.0519\n",
      "Epoch [438/1000], Loss: 0.0706\n",
      "Epoch [438/1000], Loss: 0.0724\n",
      "Epoch [438/1000], Loss: 0.0436\n",
      "Epoch [438/1000], Loss: 0.0651\n",
      "Epoch [438/1000], Loss: 0.0547\n",
      "Epoch [438/1000], Loss: 0.0662\n",
      "tensor(2.2325, grad_fn=<MeanBackward0>)\n",
      "438\n",
      "Epoch [439/1000], Loss: 0.0628\n",
      "Epoch [439/1000], Loss: 0.1067\n",
      "Epoch [439/1000], Loss: 0.1529\n",
      "Epoch [439/1000], Loss: 0.1182\n",
      "Epoch [439/1000], Loss: 0.0717\n",
      "Epoch [439/1000], Loss: 0.1271\n",
      "Epoch [439/1000], Loss: 0.2147\n",
      "Epoch [439/1000], Loss: 0.2384\n",
      "Epoch [439/1000], Loss: 0.1988\n",
      "Epoch [439/1000], Loss: 0.0744\n",
      "Epoch [439/1000], Loss: 0.1405\n",
      "tensor(2.1496, grad_fn=<MeanBackward0>)\n",
      "439\n",
      "Epoch [440/1000], Loss: 0.1810\n",
      "Epoch [440/1000], Loss: 0.2087\n",
      "Epoch [440/1000], Loss: 0.2016\n",
      "Epoch [440/1000], Loss: 0.1543\n",
      "Epoch [440/1000], Loss: 0.0886\n",
      "Epoch [440/1000], Loss: 0.1295\n",
      "Epoch [440/1000], Loss: 0.2114\n",
      "Epoch [440/1000], Loss: 0.2619\n",
      "Epoch [440/1000], Loss: 0.2557\n",
      "Epoch [440/1000], Loss: 0.1613\n",
      "Epoch [440/1000], Loss: 0.0742\n",
      "tensor(2.1086, grad_fn=<MeanBackward0>)\n",
      "440\n",
      "Epoch [441/1000], Loss: 0.1803\n",
      "Epoch [441/1000], Loss: 0.2382\n",
      "Epoch [441/1000], Loss: 0.3252\n",
      "Epoch [441/1000], Loss: 0.3594\n",
      "Epoch [441/1000], Loss: 0.2981\n",
      "Epoch [441/1000], Loss: 0.1662\n",
      "Epoch [441/1000], Loss: 0.1269\n",
      "Epoch [441/1000], Loss: 0.3369\n",
      "Epoch [441/1000], Loss: 0.4930\n",
      "Epoch [441/1000], Loss: 0.5539\n",
      "Epoch [441/1000], Loss: 0.4835\n",
      "tensor(2.4791, grad_fn=<MeanBackward0>)\n",
      "441\n",
      "Epoch [442/1000], Loss: 0.3127\n",
      "Epoch [442/1000], Loss: 0.1370\n",
      "Epoch [442/1000], Loss: 0.2989\n",
      "Epoch [442/1000], Loss: 0.5712\n",
      "Epoch [442/1000], Loss: 0.6701\n",
      "Epoch [442/1000], Loss: 0.7396\n",
      "Epoch [442/1000], Loss: 0.6346\n",
      "Epoch [442/1000], Loss: 0.5412\n",
      "Epoch [442/1000], Loss: 0.3515\n",
      "Epoch [442/1000], Loss: 0.2156\n",
      "Epoch [442/1000], Loss: 0.3996\n",
      "tensor(2.6570, grad_fn=<MeanBackward0>)\n",
      "442\n",
      "Epoch [443/1000], Loss: 0.5695\n",
      "Epoch [443/1000], Loss: 0.6067\n",
      "Epoch [443/1000], Loss: 0.4931\n",
      "Epoch [443/1000], Loss: 0.2321\n",
      "Epoch [443/1000], Loss: 0.2464\n",
      "Epoch [443/1000], Loss: 0.4202\n",
      "Epoch [443/1000], Loss: 0.4398\n",
      "Epoch [443/1000], Loss: 0.4638\n",
      "Epoch [443/1000], Loss: 0.3474\n",
      "Epoch [443/1000], Loss: 0.2061\n",
      "Epoch [443/1000], Loss: 0.1489\n",
      "tensor(2.4620, grad_fn=<MeanBackward0>)\n",
      "443\n",
      "Epoch [444/1000], Loss: 0.3229\n",
      "Epoch [444/1000], Loss: 0.3696\n",
      "Epoch [444/1000], Loss: 0.2088\n",
      "Epoch [444/1000], Loss: 0.1215\n",
      "Epoch [444/1000], Loss: 0.2081\n",
      "Epoch [444/1000], Loss: 0.3002\n",
      "Epoch [444/1000], Loss: 0.2361\n",
      "Epoch [444/1000], Loss: 0.1691\n",
      "Epoch [444/1000], Loss: 0.1345\n",
      "Epoch [444/1000], Loss: 0.2454\n",
      "Epoch [444/1000], Loss: 0.3313\n",
      "tensor(2.4910, grad_fn=<MeanBackward0>)\n",
      "444\n",
      "Epoch [445/1000], Loss: 0.2668\n",
      "Epoch [445/1000], Loss: 0.1189\n",
      "Epoch [445/1000], Loss: 0.1879\n",
      "Epoch [445/1000], Loss: 0.3353\n",
      "Epoch [445/1000], Loss: 0.3745\n",
      "Epoch [445/1000], Loss: 0.3757\n",
      "Epoch [445/1000], Loss: 0.2441\n",
      "Epoch [445/1000], Loss: 0.1239\n",
      "Epoch [445/1000], Loss: 0.1831\n",
      "Epoch [445/1000], Loss: 0.2824\n",
      "Epoch [445/1000], Loss: 0.3543\n",
      "tensor(2.5283, grad_fn=<MeanBackward0>)\n",
      "445\n",
      "Epoch [446/1000], Loss: 0.2979\n",
      "Epoch [446/1000], Loss: 0.1539\n",
      "Epoch [446/1000], Loss: 0.1560\n",
      "Epoch [446/1000], Loss: 0.3502\n",
      "Epoch [446/1000], Loss: 0.4151\n",
      "Epoch [446/1000], Loss: 0.4539\n",
      "Epoch [446/1000], Loss: 0.3531\n",
      "Epoch [446/1000], Loss: 0.2715\n",
      "Epoch [446/1000], Loss: 0.1566\n",
      "Epoch [446/1000], Loss: 0.1882\n",
      "Epoch [446/1000], Loss: 0.3748\n",
      "tensor(2.6486, grad_fn=<MeanBackward0>)\n",
      "446\n",
      "Epoch [447/1000], Loss: 0.4346\n",
      "Epoch [447/1000], Loss: 0.3838\n",
      "Epoch [447/1000], Loss: 0.2595\n",
      "Epoch [447/1000], Loss: 0.1614\n",
      "Epoch [447/1000], Loss: 0.2784\n",
      "Epoch [447/1000], Loss: 0.4017\n",
      "Epoch [447/1000], Loss: 0.3871\n",
      "Epoch [447/1000], Loss: 0.3783\n",
      "Epoch [447/1000], Loss: 0.3084\n",
      "Epoch [447/1000], Loss: 0.1433\n",
      "Epoch [447/1000], Loss: 0.1731\n",
      "tensor(2.5891, grad_fn=<MeanBackward0>)\n",
      "447\n",
      "Epoch [448/1000], Loss: 0.3736\n",
      "Epoch [448/1000], Loss: 0.4327\n",
      "Epoch [448/1000], Loss: 0.3500\n",
      "Epoch [448/1000], Loss: 0.1769\n",
      "Epoch [448/1000], Loss: 0.1290\n",
      "Epoch [448/1000], Loss: 0.2921\n",
      "Epoch [448/1000], Loss: 0.3397\n",
      "Epoch [448/1000], Loss: 0.3642\n",
      "Epoch [448/1000], Loss: 0.3202\n",
      "Epoch [448/1000], Loss: 0.1769\n",
      "Epoch [448/1000], Loss: 0.1057\n",
      "tensor(2.5648, grad_fn=<MeanBackward0>)\n",
      "448\n",
      "Epoch [449/1000], Loss: 0.2647\n",
      "Epoch [449/1000], Loss: 0.3251\n",
      "Epoch [449/1000], Loss: 0.2885\n",
      "Epoch [449/1000], Loss: 0.1473\n",
      "Epoch [449/1000], Loss: 0.1540\n",
      "Epoch [449/1000], Loss: 0.2277\n",
      "Epoch [449/1000], Loss: 0.2489\n",
      "Epoch [449/1000], Loss: 0.2456\n",
      "Epoch [449/1000], Loss: 0.2384\n",
      "Epoch [449/1000], Loss: 0.1352\n",
      "Epoch [449/1000], Loss: 0.1006\n",
      "tensor(2.5369, grad_fn=<MeanBackward0>)\n",
      "449\n",
      "Epoch [450/1000], Loss: 0.2150\n",
      "Epoch [450/1000], Loss: 0.2496\n",
      "Epoch [450/1000], Loss: 0.1999\n",
      "Epoch [450/1000], Loss: 0.1019\n",
      "Epoch [450/1000], Loss: 0.1084\n",
      "Epoch [450/1000], Loss: 0.1717\n",
      "Epoch [450/1000], Loss: 0.1836\n",
      "Epoch [450/1000], Loss: 0.1444\n",
      "Epoch [450/1000], Loss: 0.1188\n",
      "Epoch [450/1000], Loss: 0.1146\n",
      "Epoch [450/1000], Loss: 0.1277\n",
      "tensor(2.5349, grad_fn=<MeanBackward0>)\n",
      "450\n",
      "Epoch [451/1000], Loss: 0.1651\n",
      "Epoch [451/1000], Loss: 0.1507\n",
      "Epoch [451/1000], Loss: 0.1437\n",
      "Epoch [451/1000], Loss: 0.0907\n",
      "Epoch [451/1000], Loss: 0.0859\n",
      "Epoch [451/1000], Loss: 0.1199\n",
      "Epoch [451/1000], Loss: 0.1344\n",
      "Epoch [451/1000], Loss: 0.1089\n",
      "Epoch [451/1000], Loss: 0.1076\n",
      "Epoch [451/1000], Loss: 0.0904\n",
      "Epoch [451/1000], Loss: 0.0753\n",
      "tensor(2.4633, grad_fn=<MeanBackward0>)\n",
      "451\n",
      "Epoch [452/1000], Loss: 0.0740\n",
      "Epoch [452/1000], Loss: 0.0846\n",
      "Epoch [452/1000], Loss: 0.0982\n",
      "Epoch [452/1000], Loss: 0.0900\n",
      "Epoch [452/1000], Loss: 0.0815\n",
      "Epoch [452/1000], Loss: 0.0949\n",
      "Epoch [452/1000], Loss: 0.0989\n",
      "Epoch [452/1000], Loss: 0.0699\n",
      "Epoch [452/1000], Loss: 0.0697\n",
      "Epoch [452/1000], Loss: 0.0667\n",
      "Epoch [452/1000], Loss: 0.0546\n",
      "tensor(2.4578, grad_fn=<MeanBackward0>)\n",
      "452\n",
      "Epoch [453/1000], Loss: 0.0681\n",
      "Epoch [453/1000], Loss: 0.0903\n",
      "Epoch [453/1000], Loss: 0.0860\n",
      "Epoch [453/1000], Loss: 0.0640\n",
      "Epoch [453/1000], Loss: 0.0728\n",
      "Epoch [453/1000], Loss: 0.0763\n",
      "Epoch [453/1000], Loss: 0.0734\n",
      "Epoch [453/1000], Loss: 0.0526\n",
      "Epoch [453/1000], Loss: 0.0635\n",
      "Epoch [453/1000], Loss: 0.0647\n",
      "Epoch [453/1000], Loss: 0.0559\n",
      "tensor(2.4422, grad_fn=<MeanBackward0>)\n",
      "453\n",
      "Epoch [454/1000], Loss: 0.0565\n",
      "Epoch [454/1000], Loss: 0.0918\n",
      "Epoch [454/1000], Loss: 0.0769\n",
      "Epoch [454/1000], Loss: 0.0657\n",
      "Epoch [454/1000], Loss: 0.0846\n",
      "Epoch [454/1000], Loss: 0.0999\n",
      "Epoch [454/1000], Loss: 0.0801\n",
      "Epoch [454/1000], Loss: 0.0626\n",
      "Epoch [454/1000], Loss: 0.0549\n",
      "Epoch [454/1000], Loss: 0.0468\n",
      "Epoch [454/1000], Loss: 0.0686\n",
      "tensor(2.3991, grad_fn=<MeanBackward0>)\n",
      "454\n",
      "Epoch [455/1000], Loss: 0.0799\n",
      "Epoch [455/1000], Loss: 0.1102\n",
      "Epoch [455/1000], Loss: 0.1143\n",
      "Epoch [455/1000], Loss: 0.0988\n",
      "Epoch [455/1000], Loss: 0.0585\n",
      "Epoch [455/1000], Loss: 0.0998\n",
      "Epoch [455/1000], Loss: 0.1263\n",
      "Epoch [455/1000], Loss: 0.1121\n",
      "Epoch [455/1000], Loss: 0.1349\n",
      "Epoch [455/1000], Loss: 0.1183\n",
      "Epoch [455/1000], Loss: 0.1282\n",
      "tensor(2.4595, grad_fn=<MeanBackward0>)\n",
      "455\n",
      "Epoch [456/1000], Loss: 0.0681\n",
      "Epoch [456/1000], Loss: 0.0781\n",
      "Epoch [456/1000], Loss: 0.0845\n",
      "Epoch [456/1000], Loss: 0.1219\n",
      "Epoch [456/1000], Loss: 0.1352\n",
      "Epoch [456/1000], Loss: 0.1244\n",
      "Epoch [456/1000], Loss: 0.1008\n",
      "Epoch [456/1000], Loss: 0.1186\n",
      "Epoch [456/1000], Loss: 0.0980\n",
      "Epoch [456/1000], Loss: 0.0503\n",
      "Epoch [456/1000], Loss: 0.0790\n",
      "tensor(2.4750, grad_fn=<MeanBackward0>)\n",
      "456\n",
      "Epoch [457/1000], Loss: 0.0797\n",
      "Epoch [457/1000], Loss: 0.0801\n",
      "Epoch [457/1000], Loss: 0.1187\n",
      "Epoch [457/1000], Loss: 0.0934\n",
      "Epoch [457/1000], Loss: 0.0667\n",
      "Epoch [457/1000], Loss: 0.0852\n",
      "Epoch [457/1000], Loss: 0.1263\n",
      "Epoch [457/1000], Loss: 0.1037\n",
      "Epoch [457/1000], Loss: 0.0793\n",
      "Epoch [457/1000], Loss: 0.0876\n",
      "Epoch [457/1000], Loss: 0.0714\n",
      "tensor(2.4139, grad_fn=<MeanBackward0>)\n",
      "457\n",
      "Epoch [458/1000], Loss: 0.0506\n",
      "Epoch [458/1000], Loss: 0.0619\n",
      "Epoch [458/1000], Loss: 0.0766\n",
      "Epoch [458/1000], Loss: 0.1192\n",
      "Epoch [458/1000], Loss: 0.1293\n",
      "Epoch [458/1000], Loss: 0.0945\n",
      "Epoch [458/1000], Loss: 0.0920\n",
      "Epoch [458/1000], Loss: 0.1401\n",
      "Epoch [458/1000], Loss: 0.1909\n",
      "Epoch [458/1000], Loss: 0.1490\n",
      "Epoch [458/1000], Loss: 0.0638\n",
      "tensor(2.5044, grad_fn=<MeanBackward0>)\n",
      "458\n",
      "Epoch [459/1000], Loss: 0.1531\n",
      "Epoch [459/1000], Loss: 0.2377\n",
      "Epoch [459/1000], Loss: 0.1885\n",
      "Epoch [459/1000], Loss: 0.1246\n",
      "Epoch [459/1000], Loss: 0.0663\n",
      "Epoch [459/1000], Loss: 0.0879\n",
      "Epoch [459/1000], Loss: 0.1310\n",
      "Epoch [459/1000], Loss: 0.1010\n",
      "Epoch [459/1000], Loss: 0.1066\n",
      "Epoch [459/1000], Loss: 0.0896\n",
      "Epoch [459/1000], Loss: 0.1078\n",
      "tensor(2.5010, grad_fn=<MeanBackward0>)\n",
      "459\n",
      "Epoch [460/1000], Loss: 0.0749\n",
      "Epoch [460/1000], Loss: 0.0607\n",
      "Epoch [460/1000], Loss: 0.0740\n",
      "Epoch [460/1000], Loss: 0.0716\n",
      "Epoch [460/1000], Loss: 0.0933\n",
      "Epoch [460/1000], Loss: 0.1199\n",
      "Epoch [460/1000], Loss: 0.0860\n",
      "Epoch [460/1000], Loss: 0.1230\n",
      "Epoch [460/1000], Loss: 0.1996\n",
      "Epoch [460/1000], Loss: 0.2092\n",
      "Epoch [460/1000], Loss: 0.1549\n",
      "tensor(2.4856, grad_fn=<MeanBackward0>)\n",
      "460\n",
      "Epoch [461/1000], Loss: 0.0754\n",
      "Epoch [461/1000], Loss: 0.1960\n",
      "Epoch [461/1000], Loss: 0.2122\n",
      "Epoch [461/1000], Loss: 0.1880\n",
      "Epoch [461/1000], Loss: 0.1207\n",
      "Epoch [461/1000], Loss: 0.0783\n",
      "Epoch [461/1000], Loss: 0.1164\n",
      "Epoch [461/1000], Loss: 0.1754\n",
      "Epoch [461/1000], Loss: 0.2250\n",
      "Epoch [461/1000], Loss: 0.2068\n",
      "Epoch [461/1000], Loss: 0.1269\n",
      "tensor(2.4683, grad_fn=<MeanBackward0>)\n",
      "461\n",
      "Epoch [462/1000], Loss: 0.0557\n",
      "Epoch [462/1000], Loss: 0.1972\n",
      "Epoch [462/1000], Loss: 0.2591\n",
      "Epoch [462/1000], Loss: 0.2458\n",
      "Epoch [462/1000], Loss: 0.1851\n",
      "Epoch [462/1000], Loss: 0.1127\n",
      "Epoch [462/1000], Loss: 0.1054\n",
      "Epoch [462/1000], Loss: 0.1952\n",
      "Epoch [462/1000], Loss: 0.2937\n",
      "Epoch [462/1000], Loss: 0.3007\n",
      "Epoch [462/1000], Loss: 0.2687\n",
      "tensor(2.3466, grad_fn=<MeanBackward0>)\n",
      "462\n",
      "Epoch [463/1000], Loss: 0.1794\n",
      "Epoch [463/1000], Loss: 0.0921\n",
      "Epoch [463/1000], Loss: 0.2298\n",
      "Epoch [463/1000], Loss: 0.3326\n",
      "Epoch [463/1000], Loss: 0.3671\n",
      "Epoch [463/1000], Loss: 0.3319\n",
      "Epoch [463/1000], Loss: 0.2228\n",
      "Epoch [463/1000], Loss: 0.0850\n",
      "Epoch [463/1000], Loss: 0.2893\n",
      "Epoch [463/1000], Loss: 0.4336\n",
      "Epoch [463/1000], Loss: 0.5483\n",
      "tensor(2.0216, grad_fn=<MeanBackward0>)\n",
      "463\n",
      "Epoch [464/1000], Loss: 0.6015\n",
      "Epoch [464/1000], Loss: 0.5159\n",
      "Epoch [464/1000], Loss: 0.3272\n",
      "Epoch [464/1000], Loss: 0.1062\n",
      "Epoch [464/1000], Loss: 0.2882\n",
      "Epoch [464/1000], Loss: 0.5112\n",
      "Epoch [464/1000], Loss: 0.6373\n",
      "Epoch [464/1000], Loss: 0.6644\n",
      "Epoch [464/1000], Loss: 0.6241\n",
      "Epoch [464/1000], Loss: 0.3636\n",
      "Epoch [464/1000], Loss: 0.1568\n",
      "tensor(2.1460, grad_fn=<MeanBackward0>)\n",
      "464\n",
      "Epoch [465/1000], Loss: 0.3237\n",
      "Epoch [465/1000], Loss: 0.4475\n",
      "Epoch [465/1000], Loss: 0.5332\n",
      "Epoch [465/1000], Loss: 0.5443\n",
      "Epoch [465/1000], Loss: 0.4069\n",
      "Epoch [465/1000], Loss: 0.2678\n",
      "Epoch [465/1000], Loss: 0.1055\n",
      "Epoch [465/1000], Loss: 0.2130\n",
      "Epoch [465/1000], Loss: 0.3343\n",
      "Epoch [465/1000], Loss: 0.3278\n",
      "Epoch [465/1000], Loss: 0.1931\n",
      "tensor(2.4000, grad_fn=<MeanBackward0>)\n",
      "465\n",
      "Epoch [466/1000], Loss: 0.1753\n",
      "Epoch [466/1000], Loss: 0.2043\n",
      "Epoch [466/1000], Loss: 0.2272\n",
      "Epoch [466/1000], Loss: 0.2816\n",
      "Epoch [466/1000], Loss: 0.1920\n",
      "Epoch [466/1000], Loss: 0.1260\n",
      "Epoch [466/1000], Loss: 0.1443\n",
      "Epoch [466/1000], Loss: 0.2023\n",
      "Epoch [466/1000], Loss: 0.1988\n",
      "Epoch [466/1000], Loss: 0.1301\n",
      "Epoch [466/1000], Loss: 0.1390\n",
      "tensor(2.2119, grad_fn=<MeanBackward0>)\n",
      "466\n",
      "Epoch [467/1000], Loss: 0.1924\n",
      "Epoch [467/1000], Loss: 0.1945\n",
      "Epoch [467/1000], Loss: 0.1896\n",
      "Epoch [467/1000], Loss: 0.1181\n",
      "Epoch [467/1000], Loss: 0.1107\n",
      "Epoch [467/1000], Loss: 0.1903\n",
      "Epoch [467/1000], Loss: 0.2248\n",
      "Epoch [467/1000], Loss: 0.2528\n",
      "Epoch [467/1000], Loss: 0.1670\n",
      "Epoch [467/1000], Loss: 0.1075\n",
      "Epoch [467/1000], Loss: 0.1911\n",
      "tensor(2.1542, grad_fn=<MeanBackward0>)\n",
      "467\n",
      "Epoch [468/1000], Loss: 0.2767\n",
      "Epoch [468/1000], Loss: 0.2721\n",
      "Epoch [468/1000], Loss: 0.2066\n",
      "Epoch [468/1000], Loss: 0.1220\n",
      "Epoch [468/1000], Loss: 0.1205\n",
      "Epoch [468/1000], Loss: 0.2209\n",
      "Epoch [468/1000], Loss: 0.2776\n",
      "Epoch [468/1000], Loss: 0.2803\n",
      "Epoch [468/1000], Loss: 0.2005\n",
      "Epoch [468/1000], Loss: 0.1208\n",
      "Epoch [468/1000], Loss: 0.1680\n",
      "tensor(2.1302, grad_fn=<MeanBackward0>)\n",
      "468\n",
      "Epoch [469/1000], Loss: 0.2601\n",
      "Epoch [469/1000], Loss: 0.2524\n",
      "Epoch [469/1000], Loss: 0.2544\n",
      "Epoch [469/1000], Loss: 0.1746\n",
      "Epoch [469/1000], Loss: 0.1014\n",
      "Epoch [469/1000], Loss: 0.1775\n",
      "Epoch [469/1000], Loss: 0.2715\n",
      "Epoch [469/1000], Loss: 0.3186\n",
      "Epoch [469/1000], Loss: 0.2778\n",
      "Epoch [469/1000], Loss: 0.1677\n",
      "Epoch [469/1000], Loss: 0.0926\n",
      "tensor(2.1528, grad_fn=<MeanBackward0>)\n",
      "469\n",
      "Epoch [470/1000], Loss: 0.2363\n",
      "Epoch [470/1000], Loss: 0.3039\n",
      "Epoch [470/1000], Loss: 0.3444\n",
      "Epoch [470/1000], Loss: 0.3334\n",
      "Epoch [470/1000], Loss: 0.2182\n",
      "Epoch [470/1000], Loss: 0.0749\n",
      "Epoch [470/1000], Loss: 0.2164\n",
      "Epoch [470/1000], Loss: 0.3940\n",
      "Epoch [470/1000], Loss: 0.4631\n",
      "Epoch [470/1000], Loss: 0.4381\n",
      "Epoch [470/1000], Loss: 0.3100\n",
      "tensor(2.3615, grad_fn=<MeanBackward0>)\n",
      "470\n",
      "Epoch [471/1000], Loss: 0.1350\n",
      "Epoch [471/1000], Loss: 0.2061\n",
      "Epoch [471/1000], Loss: 0.3580\n",
      "Epoch [471/1000], Loss: 0.4843\n",
      "Epoch [471/1000], Loss: 0.4780\n",
      "Epoch [471/1000], Loss: 0.4478\n",
      "Epoch [471/1000], Loss: 0.2917\n",
      "Epoch [471/1000], Loss: 0.1445\n",
      "Epoch [471/1000], Loss: 0.1776\n",
      "Epoch [471/1000], Loss: 0.3796\n",
      "Epoch [471/1000], Loss: 0.4835\n",
      "tensor(2.6407, grad_fn=<MeanBackward0>)\n",
      "471\n",
      "Epoch [472/1000], Loss: 0.4407\n",
      "Epoch [472/1000], Loss: 0.3502\n",
      "Epoch [472/1000], Loss: 0.2397\n",
      "Epoch [472/1000], Loss: 0.2375\n",
      "Epoch [472/1000], Loss: 0.3236\n",
      "Epoch [472/1000], Loss: 0.4408\n",
      "Epoch [472/1000], Loss: 0.3798\n",
      "Epoch [472/1000], Loss: 0.3704\n",
      "Epoch [472/1000], Loss: 0.2937\n",
      "Epoch [472/1000], Loss: 0.1517\n",
      "Epoch [472/1000], Loss: 0.1860\n",
      "tensor(2.5327, grad_fn=<MeanBackward0>)\n",
      "472\n",
      "Epoch [473/1000], Loss: 0.3091\n",
      "Epoch [473/1000], Loss: 0.3128\n",
      "Epoch [473/1000], Loss: 0.1975\n",
      "Epoch [473/1000], Loss: 0.1300\n",
      "Epoch [473/1000], Loss: 0.1896\n",
      "Epoch [473/1000], Loss: 0.2608\n",
      "Epoch [473/1000], Loss: 0.2020\n",
      "Epoch [473/1000], Loss: 0.1204\n",
      "Epoch [473/1000], Loss: 0.0683\n",
      "Epoch [473/1000], Loss: 0.1425\n",
      "Epoch [473/1000], Loss: 0.1975\n",
      "tensor(2.4595, grad_fn=<MeanBackward0>)\n",
      "473\n",
      "Epoch [474/1000], Loss: 0.1237\n",
      "Epoch [474/1000], Loss: 0.1084\n",
      "Epoch [474/1000], Loss: 0.1464\n",
      "Epoch [474/1000], Loss: 0.1687\n",
      "Epoch [474/1000], Loss: 0.1443\n",
      "Epoch [474/1000], Loss: 0.1004\n",
      "Epoch [474/1000], Loss: 0.0994\n",
      "Epoch [474/1000], Loss: 0.1467\n",
      "Epoch [474/1000], Loss: 0.1075\n",
      "Epoch [474/1000], Loss: 0.1050\n",
      "Epoch [474/1000], Loss: 0.0981\n",
      "tensor(2.3512, grad_fn=<MeanBackward0>)\n",
      "474\n",
      "Epoch [475/1000], Loss: 0.1093\n",
      "Epoch [475/1000], Loss: 0.0933\n",
      "Epoch [475/1000], Loss: 0.0900\n",
      "Epoch [475/1000], Loss: 0.0840\n",
      "Epoch [475/1000], Loss: 0.0716\n",
      "Epoch [475/1000], Loss: 0.0689\n",
      "Epoch [475/1000], Loss: 0.0693\n",
      "Epoch [475/1000], Loss: 0.0642\n",
      "Epoch [475/1000], Loss: 0.0503\n",
      "Epoch [475/1000], Loss: 0.0526\n",
      "Epoch [475/1000], Loss: 0.0597\n",
      "tensor(2.3955, grad_fn=<MeanBackward0>)\n",
      "475\n",
      "Epoch [476/1000], Loss: 0.0524\n",
      "Epoch [476/1000], Loss: 0.0574\n",
      "Epoch [476/1000], Loss: 0.0738\n",
      "Epoch [476/1000], Loss: 0.0545\n",
      "Epoch [476/1000], Loss: 0.0503\n",
      "Epoch [476/1000], Loss: 0.0551\n",
      "Epoch [476/1000], Loss: 0.0465\n",
      "Epoch [476/1000], Loss: 0.0390\n",
      "Epoch [476/1000], Loss: 0.0517\n",
      "Epoch [476/1000], Loss: 0.0408\n",
      "Epoch [476/1000], Loss: 0.0708\n",
      "tensor(2.3904, grad_fn=<MeanBackward0>)\n",
      "476\n",
      "Epoch [477/1000], Loss: 0.0601\n",
      "Epoch [477/1000], Loss: 0.0665\n",
      "Epoch [477/1000], Loss: 0.0776\n",
      "Epoch [477/1000], Loss: 0.0503\n",
      "Epoch [477/1000], Loss: 0.0397\n",
      "Epoch [477/1000], Loss: 0.0438\n",
      "Epoch [477/1000], Loss: 0.0499\n",
      "Epoch [477/1000], Loss: 0.0451\n",
      "Epoch [477/1000], Loss: 0.0641\n",
      "Epoch [477/1000], Loss: 0.0556\n",
      "Epoch [477/1000], Loss: 0.0551\n",
      "tensor(2.4075, grad_fn=<MeanBackward0>)\n",
      "477\n",
      "Epoch [478/1000], Loss: 0.0473\n",
      "Epoch [478/1000], Loss: 0.0464\n",
      "Epoch [478/1000], Loss: 0.0622\n",
      "Epoch [478/1000], Loss: 0.0676\n",
      "Epoch [478/1000], Loss: 0.0399\n",
      "Epoch [478/1000], Loss: 0.0716\n",
      "Epoch [478/1000], Loss: 0.0575\n",
      "Epoch [478/1000], Loss: 0.0445\n",
      "Epoch [478/1000], Loss: 0.0958\n",
      "Epoch [478/1000], Loss: 0.0834\n",
      "Epoch [478/1000], Loss: 0.0493\n",
      "tensor(2.4387, grad_fn=<MeanBackward0>)\n",
      "478\n",
      "Epoch [479/1000], Loss: 0.0810\n",
      "Epoch [479/1000], Loss: 0.1265\n",
      "Epoch [479/1000], Loss: 0.0726\n",
      "Epoch [479/1000], Loss: 0.0997\n",
      "Epoch [479/1000], Loss: 0.1432\n",
      "Epoch [479/1000], Loss: 0.1086\n",
      "Epoch [479/1000], Loss: 0.0754\n",
      "Epoch [479/1000], Loss: 0.1575\n",
      "Epoch [479/1000], Loss: 0.1351\n",
      "Epoch [479/1000], Loss: 0.0715\n",
      "Epoch [479/1000], Loss: 0.1116\n",
      "tensor(2.3600, grad_fn=<MeanBackward0>)\n",
      "479\n",
      "Epoch [480/1000], Loss: 0.1257\n",
      "Epoch [480/1000], Loss: 0.0630\n",
      "Epoch [480/1000], Loss: 0.1203\n",
      "Epoch [480/1000], Loss: 0.1195\n",
      "Epoch [480/1000], Loss: 0.0521\n",
      "Epoch [480/1000], Loss: 0.0936\n",
      "Epoch [480/1000], Loss: 0.0928\n",
      "Epoch [480/1000], Loss: 0.0629\n",
      "Epoch [480/1000], Loss: 0.0709\n",
      "Epoch [480/1000], Loss: 0.0835\n",
      "Epoch [480/1000], Loss: 0.0470\n",
      "tensor(2.3761, grad_fn=<MeanBackward0>)\n",
      "480\n",
      "Epoch [481/1000], Loss: 0.0828\n",
      "Epoch [481/1000], Loss: 0.0442\n",
      "Epoch [481/1000], Loss: 0.0902\n",
      "Epoch [481/1000], Loss: 0.0789\n",
      "Epoch [481/1000], Loss: 0.0575\n",
      "Epoch [481/1000], Loss: 0.0794\n",
      "Epoch [481/1000], Loss: 0.0768\n",
      "Epoch [481/1000], Loss: 0.0596\n",
      "Epoch [481/1000], Loss: 0.0604\n",
      "Epoch [481/1000], Loss: 0.0304\n",
      "Epoch [481/1000], Loss: 0.0589\n",
      "tensor(2.3847, grad_fn=<MeanBackward0>)\n",
      "481\n",
      "Epoch [482/1000], Loss: 0.0670\n",
      "Epoch [482/1000], Loss: 0.0542\n",
      "Epoch [482/1000], Loss: 0.0641\n",
      "Epoch [482/1000], Loss: 0.0445\n",
      "Epoch [482/1000], Loss: 0.0504\n",
      "Epoch [482/1000], Loss: 0.0504\n",
      "Epoch [482/1000], Loss: 0.0394\n",
      "Epoch [482/1000], Loss: 0.0441\n",
      "Epoch [482/1000], Loss: 0.0392\n",
      "Epoch [482/1000], Loss: 0.0441\n",
      "Epoch [482/1000], Loss: 0.0519\n",
      "tensor(2.3807, grad_fn=<MeanBackward0>)\n",
      "482\n",
      "Epoch [483/1000], Loss: 0.0634\n",
      "Epoch [483/1000], Loss: 0.0704\n",
      "Epoch [483/1000], Loss: 0.0682\n",
      "Epoch [483/1000], Loss: 0.0889\n",
      "Epoch [483/1000], Loss: 0.0774\n",
      "Epoch [483/1000], Loss: 0.0565\n",
      "Epoch [483/1000], Loss: 0.0875\n",
      "Epoch [483/1000], Loss: 0.0840\n",
      "Epoch [483/1000], Loss: 0.0566\n",
      "Epoch [483/1000], Loss: 0.1290\n",
      "Epoch [483/1000], Loss: 0.1085\n",
      "tensor(2.3888, grad_fn=<MeanBackward0>)\n",
      "483\n",
      "Epoch [484/1000], Loss: 0.0462\n",
      "Epoch [484/1000], Loss: 0.1004\n",
      "Epoch [484/1000], Loss: 0.1198\n",
      "Epoch [484/1000], Loss: 0.0830\n",
      "Epoch [484/1000], Loss: 0.1294\n",
      "Epoch [484/1000], Loss: 0.1875\n",
      "Epoch [484/1000], Loss: 0.1400\n",
      "Epoch [484/1000], Loss: 0.0565\n",
      "Epoch [484/1000], Loss: 0.1626\n",
      "Epoch [484/1000], Loss: 0.1537\n",
      "Epoch [484/1000], Loss: 0.0860\n",
      "tensor(2.4226, grad_fn=<MeanBackward0>)\n",
      "484\n",
      "Epoch [485/1000], Loss: 0.1147\n",
      "Epoch [485/1000], Loss: 0.1710\n",
      "Epoch [485/1000], Loss: 0.0977\n",
      "Epoch [485/1000], Loss: 0.0620\n",
      "Epoch [485/1000], Loss: 0.0715\n",
      "Epoch [485/1000], Loss: 0.0564\n",
      "Epoch [485/1000], Loss: 0.0687\n",
      "Epoch [485/1000], Loss: 0.0481\n",
      "Epoch [485/1000], Loss: 0.0585\n",
      "Epoch [485/1000], Loss: 0.0524\n",
      "Epoch [485/1000], Loss: 0.0475\n",
      "tensor(2.3799, grad_fn=<MeanBackward0>)\n",
      "485\n",
      "Epoch [486/1000], Loss: 0.0495\n",
      "Epoch [486/1000], Loss: 0.0550\n",
      "Epoch [486/1000], Loss: 0.0922\n",
      "Epoch [486/1000], Loss: 0.0717\n",
      "Epoch [486/1000], Loss: 0.0612\n",
      "Epoch [486/1000], Loss: 0.0930\n",
      "Epoch [486/1000], Loss: 0.0682\n",
      "Epoch [486/1000], Loss: 0.0386\n",
      "Epoch [486/1000], Loss: 0.0858\n",
      "Epoch [486/1000], Loss: 0.0500\n",
      "Epoch [486/1000], Loss: 0.0544\n",
      "tensor(2.3917, grad_fn=<MeanBackward0>)\n",
      "486\n",
      "Epoch [487/1000], Loss: 0.0713\n",
      "Epoch [487/1000], Loss: 0.0747\n",
      "Epoch [487/1000], Loss: 0.0687\n",
      "Epoch [487/1000], Loss: 0.0618\n",
      "Epoch [487/1000], Loss: 0.0527\n",
      "Epoch [487/1000], Loss: 0.0786\n",
      "Epoch [487/1000], Loss: 0.0710\n",
      "Epoch [487/1000], Loss: 0.0698\n",
      "Epoch [487/1000], Loss: 0.0916\n",
      "Epoch [487/1000], Loss: 0.0719\n",
      "Epoch [487/1000], Loss: 0.0518\n",
      "tensor(2.4030, grad_fn=<MeanBackward0>)\n",
      "487\n",
      "Epoch [488/1000], Loss: 0.0496\n",
      "Epoch [488/1000], Loss: 0.0910\n",
      "Epoch [488/1000], Loss: 0.0936\n",
      "Epoch [488/1000], Loss: 0.0951\n",
      "Epoch [488/1000], Loss: 0.1165\n",
      "Epoch [488/1000], Loss: 0.0895\n",
      "Epoch [488/1000], Loss: 0.0472\n",
      "Epoch [488/1000], Loss: 0.1228\n",
      "Epoch [488/1000], Loss: 0.1201\n",
      "Epoch [488/1000], Loss: 0.1087\n",
      "Epoch [488/1000], Loss: 0.1327\n",
      "tensor(2.3405, grad_fn=<MeanBackward0>)\n",
      "488\n",
      "Epoch [489/1000], Loss: 0.1695\n",
      "Epoch [489/1000], Loss: 0.1343\n",
      "Epoch [489/1000], Loss: 0.0617\n",
      "Epoch [489/1000], Loss: 0.1033\n",
      "Epoch [489/1000], Loss: 0.1158\n",
      "Epoch [489/1000], Loss: 0.1207\n",
      "Epoch [489/1000], Loss: 0.1225\n",
      "Epoch [489/1000], Loss: 0.1156\n",
      "Epoch [489/1000], Loss: 0.0669\n",
      "Epoch [489/1000], Loss: 0.0644\n",
      "Epoch [489/1000], Loss: 0.0845\n",
      "tensor(2.4074, grad_fn=<MeanBackward0>)\n",
      "489\n",
      "Epoch [490/1000], Loss: 0.1248\n",
      "Epoch [490/1000], Loss: 0.1145\n",
      "Epoch [490/1000], Loss: 0.1416\n",
      "Epoch [490/1000], Loss: 0.1224\n",
      "Epoch [490/1000], Loss: 0.0850\n",
      "Epoch [490/1000], Loss: 0.0415\n",
      "Epoch [490/1000], Loss: 0.0482\n",
      "Epoch [490/1000], Loss: 0.0860\n",
      "Epoch [490/1000], Loss: 0.1298\n",
      "Epoch [490/1000], Loss: 0.1124\n",
      "Epoch [490/1000], Loss: 0.0904\n",
      "tensor(2.3447, grad_fn=<MeanBackward0>)\n",
      "490\n",
      "Epoch [491/1000], Loss: 0.0860\n",
      "Epoch [491/1000], Loss: 0.0619\n",
      "Epoch [491/1000], Loss: 0.0735\n",
      "Epoch [491/1000], Loss: 0.0853\n",
      "Epoch [491/1000], Loss: 0.0807\n",
      "Epoch [491/1000], Loss: 0.0597\n",
      "Epoch [491/1000], Loss: 0.0708\n",
      "Epoch [491/1000], Loss: 0.0713\n",
      "Epoch [491/1000], Loss: 0.0797\n",
      "Epoch [491/1000], Loss: 0.0828\n",
      "Epoch [491/1000], Loss: 0.0597\n",
      "tensor(2.3242, grad_fn=<MeanBackward0>)\n",
      "491\n",
      "Epoch [492/1000], Loss: 0.0875\n",
      "Epoch [492/1000], Loss: 0.0577\n",
      "Epoch [492/1000], Loss: 0.0543\n",
      "Epoch [492/1000], Loss: 0.0867\n",
      "Epoch [492/1000], Loss: 0.0736\n",
      "Epoch [492/1000], Loss: 0.0600\n",
      "Epoch [492/1000], Loss: 0.0845\n",
      "Epoch [492/1000], Loss: 0.0774\n",
      "Epoch [492/1000], Loss: 0.0361\n",
      "Epoch [492/1000], Loss: 0.0855\n",
      "Epoch [492/1000], Loss: 0.0682\n",
      "tensor(2.3184, grad_fn=<MeanBackward0>)\n",
      "492\n",
      "Epoch [493/1000], Loss: 0.0625\n",
      "Epoch [493/1000], Loss: 0.1104\n",
      "Epoch [493/1000], Loss: 0.1023\n",
      "Epoch [493/1000], Loss: 0.0662\n",
      "Epoch [493/1000], Loss: 0.1021\n",
      "Epoch [493/1000], Loss: 0.1254\n",
      "Epoch [493/1000], Loss: 0.0696\n",
      "Epoch [493/1000], Loss: 0.0769\n",
      "Epoch [493/1000], Loss: 0.1559\n",
      "Epoch [493/1000], Loss: 0.1218\n",
      "Epoch [493/1000], Loss: 0.0774\n",
      "tensor(2.4574, grad_fn=<MeanBackward0>)\n",
      "493\n",
      "Epoch [494/1000], Loss: 0.1179\n",
      "Epoch [494/1000], Loss: 0.1291\n",
      "Epoch [494/1000], Loss: 0.0485\n",
      "Epoch [494/1000], Loss: 0.0999\n",
      "Epoch [494/1000], Loss: 0.1041\n",
      "Epoch [494/1000], Loss: 0.0787\n",
      "Epoch [494/1000], Loss: 0.0981\n",
      "Epoch [494/1000], Loss: 0.0934\n",
      "Epoch [494/1000], Loss: 0.0560\n",
      "Epoch [494/1000], Loss: 0.0926\n",
      "Epoch [494/1000], Loss: 0.0809\n",
      "tensor(2.4155, grad_fn=<MeanBackward0>)\n",
      "494\n",
      "Epoch [495/1000], Loss: 0.0662\n",
      "Epoch [495/1000], Loss: 0.0994\n",
      "Epoch [495/1000], Loss: 0.0457\n",
      "Epoch [495/1000], Loss: 0.0706\n",
      "Epoch [495/1000], Loss: 0.0679\n",
      "Epoch [495/1000], Loss: 0.0395\n",
      "Epoch [495/1000], Loss: 0.0836\n",
      "Epoch [495/1000], Loss: 0.0826\n",
      "Epoch [495/1000], Loss: 0.0408\n",
      "Epoch [495/1000], Loss: 0.0899\n",
      "Epoch [495/1000], Loss: 0.0757\n",
      "tensor(2.3754, grad_fn=<MeanBackward0>)\n",
      "495\n",
      "Epoch [496/1000], Loss: 0.0523\n",
      "Epoch [496/1000], Loss: 0.0919\n",
      "Epoch [496/1000], Loss: 0.0716\n",
      "Epoch [496/1000], Loss: 0.0540\n",
      "Epoch [496/1000], Loss: 0.0905\n",
      "Epoch [496/1000], Loss: 0.0633\n",
      "Epoch [496/1000], Loss: 0.0528\n",
      "Epoch [496/1000], Loss: 0.1327\n",
      "Epoch [496/1000], Loss: 0.1340\n",
      "Epoch [496/1000], Loss: 0.0640\n",
      "Epoch [496/1000], Loss: 0.1085\n",
      "tensor(2.2865, grad_fn=<MeanBackward0>)\n",
      "496\n",
      "Epoch [497/1000], Loss: 0.1547\n",
      "Epoch [497/1000], Loss: 0.1091\n",
      "Epoch [497/1000], Loss: 0.0456\n",
      "Epoch [497/1000], Loss: 0.1432\n",
      "Epoch [497/1000], Loss: 0.1346\n",
      "Epoch [497/1000], Loss: 0.0895\n",
      "Epoch [497/1000], Loss: 0.0820\n",
      "Epoch [497/1000], Loss: 0.0645\n",
      "Epoch [497/1000], Loss: 0.0673\n",
      "Epoch [497/1000], Loss: 0.0932\n",
      "Epoch [497/1000], Loss: 0.0624\n",
      "tensor(2.3152, grad_fn=<MeanBackward0>)\n",
      "497\n",
      "Epoch [498/1000], Loss: 0.1013\n",
      "Epoch [498/1000], Loss: 0.1029\n",
      "Epoch [498/1000], Loss: 0.0913\n",
      "Epoch [498/1000], Loss: 0.0682\n",
      "Epoch [498/1000], Loss: 0.0771\n",
      "Epoch [498/1000], Loss: 0.0469\n",
      "Epoch [498/1000], Loss: 0.1115\n",
      "Epoch [498/1000], Loss: 0.0786\n",
      "Epoch [498/1000], Loss: 0.0886\n",
      "Epoch [498/1000], Loss: 0.1347\n",
      "Epoch [498/1000], Loss: 0.0817\n",
      "tensor(2.3344, grad_fn=<MeanBackward0>)\n",
      "498\n",
      "Epoch [499/1000], Loss: 0.0844\n",
      "Epoch [499/1000], Loss: 0.1309\n",
      "Epoch [499/1000], Loss: 0.1328\n",
      "Epoch [499/1000], Loss: 0.0820\n",
      "Epoch [499/1000], Loss: 0.1116\n",
      "Epoch [499/1000], Loss: 0.1542\n",
      "Epoch [499/1000], Loss: 0.1127\n",
      "Epoch [499/1000], Loss: 0.0525\n",
      "Epoch [499/1000], Loss: 0.0824\n",
      "Epoch [499/1000], Loss: 0.0702\n",
      "Epoch [499/1000], Loss: 0.0714\n",
      "tensor(2.3611, grad_fn=<MeanBackward0>)\n",
      "499\n",
      "Epoch [500/1000], Loss: 0.0613\n",
      "Epoch [500/1000], Loss: 0.0752\n",
      "Epoch [500/1000], Loss: 0.1448\n",
      "Epoch [500/1000], Loss: 0.0962\n",
      "Epoch [500/1000], Loss: 0.0601\n",
      "Epoch [500/1000], Loss: 0.1287\n",
      "Epoch [500/1000], Loss: 0.1199\n",
      "Epoch [500/1000], Loss: 0.0753\n",
      "Epoch [500/1000], Loss: 0.0771\n",
      "Epoch [500/1000], Loss: 0.0929\n",
      "Epoch [500/1000], Loss: 0.0636\n",
      "tensor(2.3634, grad_fn=<MeanBackward0>)\n",
      "500\n",
      "Epoch [501/1000], Loss: 0.0656\n",
      "Epoch [501/1000], Loss: 0.0817\n",
      "Epoch [501/1000], Loss: 0.0842\n",
      "Epoch [501/1000], Loss: 0.0898\n",
      "Epoch [501/1000], Loss: 0.0682\n",
      "Epoch [501/1000], Loss: 0.0775\n",
      "Epoch [501/1000], Loss: 0.1126\n",
      "Epoch [501/1000], Loss: 0.1183\n",
      "Epoch [501/1000], Loss: 0.0539\n",
      "Epoch [501/1000], Loss: 0.0795\n",
      "Epoch [501/1000], Loss: 0.1224\n",
      "tensor(2.2743, grad_fn=<MeanBackward0>)\n",
      "501\n",
      "Epoch [502/1000], Loss: 0.1003\n",
      "Epoch [502/1000], Loss: 0.0553\n",
      "Epoch [502/1000], Loss: 0.0923\n",
      "Epoch [502/1000], Loss: 0.0666\n",
      "Epoch [502/1000], Loss: 0.1032\n",
      "Epoch [502/1000], Loss: 0.1240\n",
      "Epoch [502/1000], Loss: 0.0633\n",
      "Epoch [502/1000], Loss: 0.1195\n",
      "Epoch [502/1000], Loss: 0.2232\n",
      "Epoch [502/1000], Loss: 0.2186\n",
      "Epoch [502/1000], Loss: 0.1093\n",
      "tensor(2.3014, grad_fn=<MeanBackward0>)\n",
      "502\n",
      "Epoch [503/1000], Loss: 0.1296\n",
      "Epoch [503/1000], Loss: 0.2234\n",
      "Epoch [503/1000], Loss: 0.2299\n",
      "Epoch [503/1000], Loss: 0.2220\n",
      "Epoch [503/1000], Loss: 0.1164\n",
      "Epoch [503/1000], Loss: 0.0969\n",
      "Epoch [503/1000], Loss: 0.1185\n",
      "Epoch [503/1000], Loss: 0.1752\n",
      "Epoch [503/1000], Loss: 0.2486\n",
      "Epoch [503/1000], Loss: 0.2207\n",
      "Epoch [503/1000], Loss: 0.0907\n",
      "tensor(2.2465, grad_fn=<MeanBackward0>)\n",
      "503\n",
      "Epoch [504/1000], Loss: 0.1239\n",
      "Epoch [504/1000], Loss: 0.2076\n",
      "Epoch [504/1000], Loss: 0.2663\n",
      "Epoch [504/1000], Loss: 0.2399\n",
      "Epoch [504/1000], Loss: 0.1500\n",
      "Epoch [504/1000], Loss: 0.0650\n",
      "Epoch [504/1000], Loss: 0.1194\n",
      "Epoch [504/1000], Loss: 0.2163\n",
      "Epoch [504/1000], Loss: 0.3121\n",
      "Epoch [504/1000], Loss: 0.3106\n",
      "Epoch [504/1000], Loss: 0.1740\n",
      "tensor(2.2987, grad_fn=<MeanBackward0>)\n",
      "504\n",
      "Epoch [505/1000], Loss: 0.0793\n",
      "Epoch [505/1000], Loss: 0.1988\n",
      "Epoch [505/1000], Loss: 0.2916\n",
      "Epoch [505/1000], Loss: 0.3710\n",
      "Epoch [505/1000], Loss: 0.3162\n",
      "Epoch [505/1000], Loss: 0.2750\n",
      "Epoch [505/1000], Loss: 0.1751\n",
      "Epoch [505/1000], Loss: 0.0957\n",
      "Epoch [505/1000], Loss: 0.2944\n",
      "Epoch [505/1000], Loss: 0.4418\n",
      "Epoch [505/1000], Loss: 0.4389\n",
      "tensor(2.6065, grad_fn=<MeanBackward0>)\n",
      "505\n",
      "Epoch [506/1000], Loss: 0.3902\n",
      "Epoch [506/1000], Loss: 0.2451\n",
      "Epoch [506/1000], Loss: 0.1581\n",
      "Epoch [506/1000], Loss: 0.3060\n",
      "Epoch [506/1000], Loss: 0.3853\n",
      "Epoch [506/1000], Loss: 0.4721\n",
      "Epoch [506/1000], Loss: 0.4635\n",
      "Epoch [506/1000], Loss: 0.4798\n",
      "Epoch [506/1000], Loss: 0.3427\n",
      "Epoch [506/1000], Loss: 0.1717\n",
      "Epoch [506/1000], Loss: 0.2330\n",
      "tensor(2.6535, grad_fn=<MeanBackward0>)\n",
      "506\n",
      "Epoch [507/1000], Loss: 0.4292\n",
      "Epoch [507/1000], Loss: 0.5009\n",
      "Epoch [507/1000], Loss: 0.4446\n",
      "Epoch [507/1000], Loss: 0.2465\n",
      "Epoch [507/1000], Loss: 0.1510\n",
      "Epoch [507/1000], Loss: 0.2739\n",
      "Epoch [507/1000], Loss: 0.3667\n",
      "Epoch [507/1000], Loss: 0.4914\n",
      "Epoch [507/1000], Loss: 0.4464\n",
      "Epoch [507/1000], Loss: 0.2662\n",
      "Epoch [507/1000], Loss: 0.1173\n",
      "tensor(2.5017, grad_fn=<MeanBackward0>)\n",
      "507\n",
      "Epoch [508/1000], Loss: 0.2386\n",
      "Epoch [508/1000], Loss: 0.3916\n",
      "Epoch [508/1000], Loss: 0.3680\n",
      "Epoch [508/1000], Loss: 0.2952\n",
      "Epoch [508/1000], Loss: 0.1517\n",
      "Epoch [508/1000], Loss: 0.1314\n",
      "Epoch [508/1000], Loss: 0.2241\n",
      "Epoch [508/1000], Loss: 0.3201\n",
      "Epoch [508/1000], Loss: 0.2443\n",
      "Epoch [508/1000], Loss: 0.0845\n",
      "Epoch [508/1000], Loss: 0.1096\n",
      "tensor(2.5182, grad_fn=<MeanBackward0>)\n",
      "508\n",
      "Epoch [509/1000], Loss: 0.1883\n",
      "Epoch [509/1000], Loss: 0.1511\n",
      "Epoch [509/1000], Loss: 0.1021\n",
      "Epoch [509/1000], Loss: 0.1104\n",
      "Epoch [509/1000], Loss: 0.1330\n",
      "Epoch [509/1000], Loss: 0.1138\n",
      "Epoch [509/1000], Loss: 0.0781\n",
      "Epoch [509/1000], Loss: 0.0645\n",
      "Epoch [509/1000], Loss: 0.1225\n",
      "Epoch [509/1000], Loss: 0.1254\n",
      "Epoch [509/1000], Loss: 0.0682\n",
      "tensor(2.3858, grad_fn=<MeanBackward0>)\n",
      "509\n",
      "Epoch [510/1000], Loss: 0.0798\n",
      "Epoch [510/1000], Loss: 0.0915\n",
      "Epoch [510/1000], Loss: 0.0618\n",
      "Epoch [510/1000], Loss: 0.0858\n",
      "Epoch [510/1000], Loss: 0.0938\n",
      "Epoch [510/1000], Loss: 0.0738\n",
      "Epoch [510/1000], Loss: 0.1024\n",
      "Epoch [510/1000], Loss: 0.1415\n",
      "Epoch [510/1000], Loss: 0.0709\n",
      "Epoch [510/1000], Loss: 0.1226\n",
      "Epoch [510/1000], Loss: 0.1179\n",
      "tensor(2.4345, grad_fn=<MeanBackward0>)\n",
      "510\n",
      "Epoch [511/1000], Loss: 0.1099\n",
      "Epoch [511/1000], Loss: 0.0765\n",
      "Epoch [511/1000], Loss: 0.0761\n",
      "Epoch [511/1000], Loss: 0.0661\n",
      "Epoch [511/1000], Loss: 0.0601\n",
      "Epoch [511/1000], Loss: 0.0723\n",
      "Epoch [511/1000], Loss: 0.0595\n",
      "Epoch [511/1000], Loss: 0.0926\n",
      "Epoch [511/1000], Loss: 0.0766\n",
      "Epoch [511/1000], Loss: 0.0676\n",
      "Epoch [511/1000], Loss: 0.0600\n",
      "tensor(2.4333, grad_fn=<MeanBackward0>)\n",
      "511\n",
      "Epoch [512/1000], Loss: 0.0541\n",
      "Epoch [512/1000], Loss: 0.0501\n",
      "Epoch [512/1000], Loss: 0.0801\n",
      "Epoch [512/1000], Loss: 0.0753\n",
      "Epoch [512/1000], Loss: 0.0518\n",
      "Epoch [512/1000], Loss: 0.0620\n",
      "Epoch [512/1000], Loss: 0.0439\n",
      "Epoch [512/1000], Loss: 0.0576\n",
      "Epoch [512/1000], Loss: 0.0511\n",
      "Epoch [512/1000], Loss: 0.0547\n",
      "Epoch [512/1000], Loss: 0.0445\n",
      "tensor(2.4523, grad_fn=<MeanBackward0>)\n",
      "512\n",
      "Epoch [513/1000], Loss: 0.0561\n",
      "Epoch [513/1000], Loss: 0.0482\n",
      "Epoch [513/1000], Loss: 0.0550\n",
      "Epoch [513/1000], Loss: 0.0767\n",
      "Epoch [513/1000], Loss: 0.0520\n",
      "Epoch [513/1000], Loss: 0.0624\n",
      "Epoch [513/1000], Loss: 0.0780\n",
      "Epoch [513/1000], Loss: 0.0543\n",
      "Epoch [513/1000], Loss: 0.0539\n",
      "Epoch [513/1000], Loss: 0.0956\n",
      "Epoch [513/1000], Loss: 0.0865\n",
      "tensor(2.4748, grad_fn=<MeanBackward0>)\n",
      "513\n",
      "Epoch [514/1000], Loss: 0.0933\n",
      "Epoch [514/1000], Loss: 0.1743\n",
      "Epoch [514/1000], Loss: 0.1565\n",
      "Epoch [514/1000], Loss: 0.0958\n",
      "Epoch [514/1000], Loss: 0.1178\n",
      "Epoch [514/1000], Loss: 0.1658\n",
      "Epoch [514/1000], Loss: 0.1512\n",
      "Epoch [514/1000], Loss: 0.0888\n",
      "Epoch [514/1000], Loss: 0.1277\n",
      "Epoch [514/1000], Loss: 0.1197\n",
      "Epoch [514/1000], Loss: 0.0963\n",
      "tensor(2.4274, grad_fn=<MeanBackward0>)\n",
      "514\n",
      "Epoch [515/1000], Loss: 0.0817\n",
      "Epoch [515/1000], Loss: 0.0604\n",
      "Epoch [515/1000], Loss: 0.0500\n",
      "Epoch [515/1000], Loss: 0.0498\n",
      "Epoch [515/1000], Loss: 0.0808\n",
      "Epoch [515/1000], Loss: 0.1033\n",
      "Epoch [515/1000], Loss: 0.0898\n",
      "Epoch [515/1000], Loss: 0.0635\n",
      "Epoch [515/1000], Loss: 0.0753\n",
      "Epoch [515/1000], Loss: 0.0501\n",
      "Epoch [515/1000], Loss: 0.0556\n",
      "tensor(2.4547, grad_fn=<MeanBackward0>)\n",
      "515\n",
      "Epoch [516/1000], Loss: 0.0435\n",
      "Epoch [516/1000], Loss: 0.0883\n",
      "Epoch [516/1000], Loss: 0.0880\n",
      "Epoch [516/1000], Loss: 0.0690\n",
      "Epoch [516/1000], Loss: 0.0921\n",
      "Epoch [516/1000], Loss: 0.0912\n",
      "Epoch [516/1000], Loss: 0.0505\n",
      "Epoch [516/1000], Loss: 0.0573\n",
      "Epoch [516/1000], Loss: 0.0711\n",
      "Epoch [516/1000], Loss: 0.0585\n",
      "Epoch [516/1000], Loss: 0.0670\n",
      "tensor(2.4320, grad_fn=<MeanBackward0>)\n",
      "516\n",
      "Epoch [517/1000], Loss: 0.0569\n",
      "Epoch [517/1000], Loss: 0.1028\n",
      "Epoch [517/1000], Loss: 0.1448\n",
      "Epoch [517/1000], Loss: 0.1103\n",
      "Epoch [517/1000], Loss: 0.0943\n",
      "Epoch [517/1000], Loss: 0.1449\n",
      "Epoch [517/1000], Loss: 0.1852\n",
      "Epoch [517/1000], Loss: 0.1891\n",
      "Epoch [517/1000], Loss: 0.1389\n",
      "Epoch [517/1000], Loss: 0.0634\n",
      "Epoch [517/1000], Loss: 0.1718\n",
      "tensor(2.5348, grad_fn=<MeanBackward0>)\n",
      "517\n",
      "Epoch [518/1000], Loss: 0.1587\n",
      "Epoch [518/1000], Loss: 0.0969\n",
      "Epoch [518/1000], Loss: 0.0927\n",
      "Epoch [518/1000], Loss: 0.0675\n",
      "Epoch [518/1000], Loss: 0.0551\n",
      "Epoch [518/1000], Loss: 0.0567\n",
      "Epoch [518/1000], Loss: 0.0607\n",
      "Epoch [518/1000], Loss: 0.0750\n",
      "Epoch [518/1000], Loss: 0.0732\n",
      "Epoch [518/1000], Loss: 0.0524\n",
      "Epoch [518/1000], Loss: 0.0848\n",
      "tensor(2.4606, grad_fn=<MeanBackward0>)\n",
      "518\n",
      "Epoch [519/1000], Loss: 0.0701\n",
      "Epoch [519/1000], Loss: 0.0554\n",
      "Epoch [519/1000], Loss: 0.0565\n",
      "Epoch [519/1000], Loss: 0.0563\n",
      "Epoch [519/1000], Loss: 0.0425\n",
      "Epoch [519/1000], Loss: 0.0634\n",
      "Epoch [519/1000], Loss: 0.0764\n",
      "Epoch [519/1000], Loss: 0.0918\n",
      "Epoch [519/1000], Loss: 0.0839\n",
      "Epoch [519/1000], Loss: 0.0495\n",
      "Epoch [519/1000], Loss: 0.0759\n",
      "tensor(2.4694, grad_fn=<MeanBackward0>)\n",
      "519\n",
      "Epoch [520/1000], Loss: 0.0564\n",
      "Epoch [520/1000], Loss: 0.0616\n",
      "Epoch [520/1000], Loss: 0.0556\n",
      "Epoch [520/1000], Loss: 0.0567\n",
      "Epoch [520/1000], Loss: 0.0461\n",
      "Epoch [520/1000], Loss: 0.0630\n",
      "Epoch [520/1000], Loss: 0.0842\n",
      "Epoch [520/1000], Loss: 0.0852\n",
      "Epoch [520/1000], Loss: 0.0911\n",
      "Epoch [520/1000], Loss: 0.0692\n",
      "Epoch [520/1000], Loss: 0.0898\n",
      "tensor(2.5071, grad_fn=<MeanBackward0>)\n",
      "520\n",
      "Epoch [521/1000], Loss: 0.1035\n",
      "Epoch [521/1000], Loss: 0.0800\n",
      "Epoch [521/1000], Loss: 0.0633\n",
      "Epoch [521/1000], Loss: 0.0651\n",
      "Epoch [521/1000], Loss: 0.0581\n",
      "Epoch [521/1000], Loss: 0.0611\n",
      "Epoch [521/1000], Loss: 0.0769\n",
      "Epoch [521/1000], Loss: 0.0936\n",
      "Epoch [521/1000], Loss: 0.1318\n",
      "Epoch [521/1000], Loss: 0.0984\n",
      "Epoch [521/1000], Loss: 0.0841\n",
      "tensor(2.5834, grad_fn=<MeanBackward0>)\n",
      "521\n",
      "Epoch [522/1000], Loss: 0.1395\n",
      "Epoch [522/1000], Loss: 0.1972\n",
      "Epoch [522/1000], Loss: 0.1998\n",
      "Epoch [522/1000], Loss: 0.1163\n",
      "Epoch [522/1000], Loss: 0.0925\n",
      "Epoch [522/1000], Loss: 0.1627\n",
      "Epoch [522/1000], Loss: 0.2214\n",
      "Epoch [522/1000], Loss: 0.2363\n",
      "Epoch [522/1000], Loss: 0.1669\n",
      "Epoch [522/1000], Loss: 0.0821\n",
      "Epoch [522/1000], Loss: 0.0883\n",
      "tensor(2.6251, grad_fn=<MeanBackward0>)\n",
      "522\n",
      "Epoch [523/1000], Loss: 0.1765\n",
      "Epoch [523/1000], Loss: 0.2215\n",
      "Epoch [523/1000], Loss: 0.1964\n",
      "Epoch [523/1000], Loss: 0.1257\n",
      "Epoch [523/1000], Loss: 0.1113\n",
      "Epoch [523/1000], Loss: 0.1262\n",
      "Epoch [523/1000], Loss: 0.1488\n",
      "Epoch [523/1000], Loss: 0.1702\n",
      "Epoch [523/1000], Loss: 0.1917\n",
      "Epoch [523/1000], Loss: 0.1637\n",
      "Epoch [523/1000], Loss: 0.1025\n",
      "tensor(2.5998, grad_fn=<MeanBackward0>)\n",
      "523\n",
      "Epoch [524/1000], Loss: 0.1706\n",
      "Epoch [524/1000], Loss: 0.3080\n",
      "Epoch [524/1000], Loss: 0.3814\n",
      "Epoch [524/1000], Loss: 0.3882\n",
      "Epoch [524/1000], Loss: 0.3091\n",
      "Epoch [524/1000], Loss: 0.1897\n",
      "Epoch [524/1000], Loss: 0.1887\n",
      "Epoch [524/1000], Loss: 0.3722\n",
      "Epoch [524/1000], Loss: 0.5014\n",
      "Epoch [524/1000], Loss: 0.5529\n",
      "Epoch [524/1000], Loss: 0.5950\n",
      "tensor(2.1987, grad_fn=<MeanBackward0>)\n",
      "524\n",
      "Epoch [525/1000], Loss: 0.4920\n",
      "Epoch [525/1000], Loss: 0.2508\n",
      "Epoch [525/1000], Loss: 0.2145\n",
      "Epoch [525/1000], Loss: 0.4179\n",
      "Epoch [525/1000], Loss: 0.6322\n",
      "Epoch [525/1000], Loss: 0.6536\n",
      "Epoch [525/1000], Loss: 0.6363\n",
      "Epoch [525/1000], Loss: 0.4448\n",
      "Epoch [525/1000], Loss: 0.2216\n",
      "Epoch [525/1000], Loss: 0.2363\n",
      "Epoch [525/1000], Loss: 0.4941\n",
      "tensor(2.0917, grad_fn=<MeanBackward0>)\n",
      "525\n",
      "Epoch [526/1000], Loss: 0.6092\n",
      "Epoch [526/1000], Loss: 0.5862\n",
      "Epoch [526/1000], Loss: 0.5393\n",
      "Epoch [526/1000], Loss: 0.3940\n",
      "Epoch [526/1000], Loss: 0.2006\n",
      "Epoch [526/1000], Loss: 0.1767\n",
      "Epoch [526/1000], Loss: 0.2886\n",
      "Epoch [526/1000], Loss: 0.2778\n",
      "Epoch [526/1000], Loss: 0.2742\n",
      "Epoch [526/1000], Loss: 0.1823\n",
      "Epoch [526/1000], Loss: 0.2041\n",
      "tensor(2.2569, grad_fn=<MeanBackward0>)\n",
      "526\n",
      "Epoch [527/1000], Loss: 0.2394\n",
      "Epoch [527/1000], Loss: 0.2279\n",
      "Epoch [527/1000], Loss: 0.1524\n",
      "Epoch [527/1000], Loss: 0.1124\n",
      "Epoch [527/1000], Loss: 0.1707\n",
      "Epoch [527/1000], Loss: 0.1298\n",
      "Epoch [527/1000], Loss: 0.0898\n",
      "Epoch [527/1000], Loss: 0.1307\n",
      "Epoch [527/1000], Loss: 0.1287\n",
      "Epoch [527/1000], Loss: 0.1287\n",
      "Epoch [527/1000], Loss: 0.1038\n",
      "tensor(2.4745, grad_fn=<MeanBackward0>)\n",
      "527\n",
      "Epoch [528/1000], Loss: 0.1175\n",
      "Epoch [528/1000], Loss: 0.1561\n",
      "Epoch [528/1000], Loss: 0.1121\n",
      "Epoch [528/1000], Loss: 0.0949\n",
      "Epoch [528/1000], Loss: 0.0914\n",
      "Epoch [528/1000], Loss: 0.1240\n",
      "Epoch [528/1000], Loss: 0.1357\n",
      "Epoch [528/1000], Loss: 0.1013\n",
      "Epoch [528/1000], Loss: 0.0863\n",
      "Epoch [528/1000], Loss: 0.0901\n",
      "Epoch [528/1000], Loss: 0.0800\n",
      "tensor(2.4238, grad_fn=<MeanBackward0>)\n",
      "528\n",
      "Epoch [529/1000], Loss: 0.0852\n",
      "Epoch [529/1000], Loss: 0.0852\n",
      "Epoch [529/1000], Loss: 0.1347\n",
      "Epoch [529/1000], Loss: 0.1225\n",
      "Epoch [529/1000], Loss: 0.0989\n",
      "Epoch [529/1000], Loss: 0.0890\n",
      "Epoch [529/1000], Loss: 0.1077\n",
      "Epoch [529/1000], Loss: 0.1225\n",
      "Epoch [529/1000], Loss: 0.0993\n",
      "Epoch [529/1000], Loss: 0.0671\n",
      "Epoch [529/1000], Loss: 0.0783\n",
      "tensor(2.4967, grad_fn=<MeanBackward0>)\n",
      "529\n",
      "Epoch [530/1000], Loss: 0.1009\n",
      "Epoch [530/1000], Loss: 0.1050\n",
      "Epoch [530/1000], Loss: 0.0751\n",
      "Epoch [530/1000], Loss: 0.0669\n",
      "Epoch [530/1000], Loss: 0.0544\n",
      "Epoch [530/1000], Loss: 0.0525\n",
      "Epoch [530/1000], Loss: 0.0668\n",
      "Epoch [530/1000], Loss: 0.0830\n",
      "Epoch [530/1000], Loss: 0.1245\n",
      "Epoch [530/1000], Loss: 0.0988\n",
      "Epoch [530/1000], Loss: 0.0703\n",
      "tensor(2.5359, grad_fn=<MeanBackward0>)\n",
      "530\n",
      "Epoch [531/1000], Loss: 0.0948\n",
      "Epoch [531/1000], Loss: 0.1578\n",
      "Epoch [531/1000], Loss: 0.1825\n",
      "Epoch [531/1000], Loss: 0.1666\n",
      "Epoch [531/1000], Loss: 0.1115\n",
      "Epoch [531/1000], Loss: 0.0597\n",
      "Epoch [531/1000], Loss: 0.1286\n",
      "Epoch [531/1000], Loss: 0.1866\n",
      "Epoch [531/1000], Loss: 0.2149\n",
      "Epoch [531/1000], Loss: 0.1495\n",
      "Epoch [531/1000], Loss: 0.1025\n",
      "tensor(2.5261, grad_fn=<MeanBackward0>)\n",
      "531\n",
      "Epoch [532/1000], Loss: 0.0910\n",
      "Epoch [532/1000], Loss: 0.1897\n",
      "Epoch [532/1000], Loss: 0.2889\n",
      "Epoch [532/1000], Loss: 0.3168\n",
      "Epoch [532/1000], Loss: 0.2901\n",
      "Epoch [532/1000], Loss: 0.1829\n",
      "Epoch [532/1000], Loss: 0.1044\n",
      "Epoch [532/1000], Loss: 0.2409\n",
      "Epoch [532/1000], Loss: 0.4267\n",
      "Epoch [532/1000], Loss: 0.4845\n",
      "Epoch [532/1000], Loss: 0.5637\n",
      "tensor(2.1233, grad_fn=<MeanBackward0>)\n",
      "532\n",
      "Epoch [533/1000], Loss: 0.5570\n",
      "Epoch [533/1000], Loss: 0.4077\n",
      "Epoch [533/1000], Loss: 0.1486\n",
      "Epoch [533/1000], Loss: 0.2514\n",
      "Epoch [533/1000], Loss: 0.5248\n",
      "Epoch [533/1000], Loss: 0.6575\n",
      "Epoch [533/1000], Loss: 0.6909\n",
      "Epoch [533/1000], Loss: 0.5854\n",
      "Epoch [533/1000], Loss: 0.4083\n",
      "Epoch [533/1000], Loss: 0.1901\n",
      "Epoch [533/1000], Loss: 0.3229\n",
      "tensor(2.0894, grad_fn=<MeanBackward0>)\n",
      "533\n",
      "Epoch [534/1000], Loss: 0.4906\n",
      "Epoch [534/1000], Loss: 0.5779\n",
      "Epoch [534/1000], Loss: 0.6186\n",
      "Epoch [534/1000], Loss: 0.5741\n",
      "Epoch [534/1000], Loss: 0.3803\n",
      "Epoch [534/1000], Loss: 0.2295\n",
      "Epoch [534/1000], Loss: 0.1657\n",
      "Epoch [534/1000], Loss: 0.2286\n",
      "Epoch [534/1000], Loss: 0.3680\n",
      "Epoch [534/1000], Loss: 0.3680\n",
      "Epoch [534/1000], Loss: 0.1776\n",
      "tensor(2.3746, grad_fn=<MeanBackward0>)\n",
      "534\n",
      "Epoch [535/1000], Loss: 0.1594\n",
      "Epoch [535/1000], Loss: 0.1982\n",
      "Epoch [535/1000], Loss: 0.1938\n",
      "Epoch [535/1000], Loss: 0.2169\n",
      "Epoch [535/1000], Loss: 0.1461\n",
      "Epoch [535/1000], Loss: 0.1346\n",
      "Epoch [535/1000], Loss: 0.1284\n",
      "Epoch [535/1000], Loss: 0.1098\n",
      "Epoch [535/1000], Loss: 0.0984\n",
      "Epoch [535/1000], Loss: 0.1239\n",
      "Epoch [535/1000], Loss: 0.1135\n",
      "tensor(2.3287, grad_fn=<MeanBackward0>)\n",
      "535\n",
      "Epoch [536/1000], Loss: 0.1125\n",
      "Epoch [536/1000], Loss: 0.0998\n",
      "Epoch [536/1000], Loss: 0.1223\n",
      "Epoch [536/1000], Loss: 0.1234\n",
      "Epoch [536/1000], Loss: 0.1091\n",
      "Epoch [536/1000], Loss: 0.1223\n",
      "Epoch [536/1000], Loss: 0.1235\n",
      "Epoch [536/1000], Loss: 0.0963\n",
      "Epoch [536/1000], Loss: 0.1108\n",
      "Epoch [536/1000], Loss: 0.1181\n",
      "Epoch [536/1000], Loss: 0.1040\n",
      "tensor(2.3418, grad_fn=<MeanBackward0>)\n",
      "536\n",
      "Epoch [537/1000], Loss: 0.0985\n",
      "Epoch [537/1000], Loss: 0.0723\n",
      "Epoch [537/1000], Loss: 0.1037\n",
      "Epoch [537/1000], Loss: 0.0811\n",
      "Epoch [537/1000], Loss: 0.0722\n",
      "Epoch [537/1000], Loss: 0.0686\n",
      "Epoch [537/1000], Loss: 0.0905\n",
      "Epoch [537/1000], Loss: 0.0606\n",
      "Epoch [537/1000], Loss: 0.0486\n",
      "Epoch [537/1000], Loss: 0.0521\n",
      "Epoch [537/1000], Loss: 0.0466\n",
      "tensor(2.3767, grad_fn=<MeanBackward0>)\n",
      "537\n",
      "Epoch [538/1000], Loss: 0.0765\n",
      "Epoch [538/1000], Loss: 0.0679\n",
      "Epoch [538/1000], Loss: 0.0757\n",
      "Epoch [538/1000], Loss: 0.0823\n",
      "Epoch [538/1000], Loss: 0.0778\n",
      "Epoch [538/1000], Loss: 0.0572\n",
      "Epoch [538/1000], Loss: 0.0752\n",
      "Epoch [538/1000], Loss: 0.0857\n",
      "Epoch [538/1000], Loss: 0.0767\n",
      "Epoch [538/1000], Loss: 0.0445\n",
      "Epoch [538/1000], Loss: 0.0589\n",
      "tensor(2.3907, grad_fn=<MeanBackward0>)\n",
      "538\n",
      "Epoch [539/1000], Loss: 0.0660\n",
      "Epoch [539/1000], Loss: 0.0770\n",
      "Epoch [539/1000], Loss: 0.0817\n",
      "Epoch [539/1000], Loss: 0.0872\n",
      "Epoch [539/1000], Loss: 0.1396\n",
      "Epoch [539/1000], Loss: 0.1263\n",
      "Epoch [539/1000], Loss: 0.0626\n",
      "Epoch [539/1000], Loss: 0.1289\n",
      "Epoch [539/1000], Loss: 0.2031\n",
      "Epoch [539/1000], Loss: 0.1837\n",
      "Epoch [539/1000], Loss: 0.1692\n",
      "tensor(2.3876, grad_fn=<MeanBackward0>)\n",
      "539\n",
      "Epoch [540/1000], Loss: 0.0898\n",
      "Epoch [540/1000], Loss: 0.1147\n",
      "Epoch [540/1000], Loss: 0.1306\n",
      "Epoch [540/1000], Loss: 0.1179\n",
      "Epoch [540/1000], Loss: 0.1252\n",
      "Epoch [540/1000], Loss: 0.1067\n",
      "Epoch [540/1000], Loss: 0.0635\n",
      "Epoch [540/1000], Loss: 0.1006\n",
      "Epoch [540/1000], Loss: 0.2016\n",
      "Epoch [540/1000], Loss: 0.2394\n",
      "Epoch [540/1000], Loss: 0.1778\n",
      "tensor(2.3518, grad_fn=<MeanBackward0>)\n",
      "540\n",
      "Epoch [541/1000], Loss: 0.1074\n",
      "Epoch [541/1000], Loss: 0.0906\n",
      "Epoch [541/1000], Loss: 0.2122\n",
      "Epoch [541/1000], Loss: 0.2287\n",
      "Epoch [541/1000], Loss: 0.2256\n",
      "Epoch [541/1000], Loss: 0.1790\n",
      "Epoch [541/1000], Loss: 0.0876\n",
      "Epoch [541/1000], Loss: 0.1196\n",
      "Epoch [541/1000], Loss: 0.2893\n",
      "Epoch [541/1000], Loss: 0.3223\n",
      "Epoch [541/1000], Loss: 0.3876\n",
      "tensor(2.1540, grad_fn=<MeanBackward0>)\n",
      "541\n",
      "Epoch [542/1000], Loss: 0.3735\n",
      "Epoch [542/1000], Loss: 0.2759\n",
      "Epoch [542/1000], Loss: 0.1829\n",
      "Epoch [542/1000], Loss: 0.1563\n",
      "Epoch [542/1000], Loss: 0.3492\n",
      "Epoch [542/1000], Loss: 0.4795\n",
      "Epoch [542/1000], Loss: 0.5350\n",
      "Epoch [542/1000], Loss: 0.5367\n",
      "Epoch [542/1000], Loss: 0.4012\n",
      "Epoch [542/1000], Loss: 0.1633\n",
      "Epoch [542/1000], Loss: 0.2484\n",
      "tensor(2.0505, grad_fn=<MeanBackward0>)\n",
      "542\n",
      "Epoch [543/1000], Loss: 0.4452\n",
      "Epoch [543/1000], Loss: 0.5658\n",
      "Epoch [543/1000], Loss: 0.6186\n",
      "Epoch [543/1000], Loss: 0.6127\n",
      "Epoch [543/1000], Loss: 0.4168\n",
      "Epoch [543/1000], Loss: 0.1987\n",
      "Epoch [543/1000], Loss: 0.1494\n",
      "Epoch [543/1000], Loss: 0.3582\n",
      "Epoch [543/1000], Loss: 0.4695\n",
      "Epoch [543/1000], Loss: 0.4574\n",
      "Epoch [543/1000], Loss: 0.3034\n",
      "tensor(2.4275, grad_fn=<MeanBackward0>)\n",
      "543\n",
      "Epoch [544/1000], Loss: 0.1867\n",
      "Epoch [544/1000], Loss: 0.2230\n",
      "Epoch [544/1000], Loss: 0.2883\n",
      "Epoch [544/1000], Loss: 0.3825\n",
      "Epoch [544/1000], Loss: 0.2901\n",
      "Epoch [544/1000], Loss: 0.2385\n",
      "Epoch [544/1000], Loss: 0.1590\n",
      "Epoch [544/1000], Loss: 0.1506\n",
      "Epoch [544/1000], Loss: 0.1644\n",
      "Epoch [544/1000], Loss: 0.2185\n",
      "Epoch [544/1000], Loss: 0.1490\n",
      "tensor(2.3612, grad_fn=<MeanBackward0>)\n",
      "544\n",
      "Epoch [545/1000], Loss: 0.1403\n",
      "Epoch [545/1000], Loss: 0.1736\n",
      "Epoch [545/1000], Loss: 0.1855\n",
      "Epoch [545/1000], Loss: 0.2049\n",
      "Epoch [545/1000], Loss: 0.1155\n",
      "Epoch [545/1000], Loss: 0.0833\n",
      "Epoch [545/1000], Loss: 0.1201\n",
      "Epoch [545/1000], Loss: 0.1817\n",
      "Epoch [545/1000], Loss: 0.1178\n",
      "Epoch [545/1000], Loss: 0.1161\n",
      "Epoch [545/1000], Loss: 0.1354\n",
      "tensor(2.2275, grad_fn=<MeanBackward0>)\n",
      "545\n",
      "Epoch [546/1000], Loss: 0.1653\n",
      "Epoch [546/1000], Loss: 0.1419\n",
      "Epoch [546/1000], Loss: 0.1257\n",
      "Epoch [546/1000], Loss: 0.0817\n",
      "Epoch [546/1000], Loss: 0.1238\n",
      "Epoch [546/1000], Loss: 0.1918\n",
      "Epoch [546/1000], Loss: 0.1691\n",
      "Epoch [546/1000], Loss: 0.1705\n",
      "Epoch [546/1000], Loss: 0.1093\n",
      "Epoch [546/1000], Loss: 0.1126\n",
      "Epoch [546/1000], Loss: 0.1629\n",
      "tensor(2.1815, grad_fn=<MeanBackward0>)\n",
      "546\n",
      "Epoch [547/1000], Loss: 0.1629\n",
      "Epoch [547/1000], Loss: 0.0985\n",
      "Epoch [547/1000], Loss: 0.0727\n",
      "Epoch [547/1000], Loss: 0.0790\n",
      "Epoch [547/1000], Loss: 0.1179\n",
      "Epoch [547/1000], Loss: 0.1193\n",
      "Epoch [547/1000], Loss: 0.0821\n",
      "Epoch [547/1000], Loss: 0.0808\n",
      "Epoch [547/1000], Loss: 0.0951\n",
      "Epoch [547/1000], Loss: 0.0862\n",
      "Epoch [547/1000], Loss: 0.0660\n",
      "tensor(2.3174, grad_fn=<MeanBackward0>)\n",
      "547\n",
      "Epoch [548/1000], Loss: 0.0697\n",
      "Epoch [548/1000], Loss: 0.0626\n",
      "Epoch [548/1000], Loss: 0.0789\n",
      "Epoch [548/1000], Loss: 0.0775\n",
      "Epoch [548/1000], Loss: 0.0794\n",
      "Epoch [548/1000], Loss: 0.1073\n",
      "Epoch [548/1000], Loss: 0.0885\n",
      "Epoch [548/1000], Loss: 0.0605\n",
      "Epoch [548/1000], Loss: 0.0816\n",
      "Epoch [548/1000], Loss: 0.0935\n",
      "Epoch [548/1000], Loss: 0.0690\n",
      "tensor(2.3048, grad_fn=<MeanBackward0>)\n",
      "548\n",
      "Epoch [549/1000], Loss: 0.0620\n",
      "Epoch [549/1000], Loss: 0.0642\n",
      "Epoch [549/1000], Loss: 0.0692\n",
      "Epoch [549/1000], Loss: 0.0661\n",
      "Epoch [549/1000], Loss: 0.0643\n",
      "Epoch [549/1000], Loss: 0.1208\n",
      "Epoch [549/1000], Loss: 0.1394\n",
      "Epoch [549/1000], Loss: 0.1129\n",
      "Epoch [549/1000], Loss: 0.0513\n",
      "Epoch [549/1000], Loss: 0.1172\n",
      "Epoch [549/1000], Loss: 0.1484\n",
      "tensor(2.2022, grad_fn=<MeanBackward0>)\n",
      "549\n",
      "Epoch [550/1000], Loss: 0.1678\n",
      "Epoch [550/1000], Loss: 0.1236\n",
      "Epoch [550/1000], Loss: 0.0589\n",
      "Epoch [550/1000], Loss: 0.0975\n",
      "Epoch [550/1000], Loss: 0.1031\n",
      "Epoch [550/1000], Loss: 0.1040\n",
      "Epoch [550/1000], Loss: 0.0905\n",
      "Epoch [550/1000], Loss: 0.0781\n",
      "Epoch [550/1000], Loss: 0.0610\n",
      "Epoch [550/1000], Loss: 0.0807\n",
      "Epoch [550/1000], Loss: 0.1008\n",
      "tensor(2.2162, grad_fn=<MeanBackward0>)\n",
      "550\n",
      "Epoch [551/1000], Loss: 0.1466\n",
      "Epoch [551/1000], Loss: 0.1396\n",
      "Epoch [551/1000], Loss: 0.0822\n",
      "Epoch [551/1000], Loss: 0.0697\n",
      "Epoch [551/1000], Loss: 0.1071\n",
      "Epoch [551/1000], Loss: 0.1083\n",
      "Epoch [551/1000], Loss: 0.0918\n",
      "Epoch [551/1000], Loss: 0.0991\n",
      "Epoch [551/1000], Loss: 0.0843\n",
      "Epoch [551/1000], Loss: 0.0811\n",
      "Epoch [551/1000], Loss: 0.1190\n",
      "tensor(2.1754, grad_fn=<MeanBackward0>)\n",
      "551\n",
      "Epoch [552/1000], Loss: 0.1638\n",
      "Epoch [552/1000], Loss: 0.1675\n",
      "Epoch [552/1000], Loss: 0.1287\n",
      "Epoch [552/1000], Loss: 0.0819\n",
      "Epoch [552/1000], Loss: 0.1168\n",
      "Epoch [552/1000], Loss: 0.2119\n",
      "Epoch [552/1000], Loss: 0.2653\n",
      "Epoch [552/1000], Loss: 0.2680\n",
      "Epoch [552/1000], Loss: 0.2363\n",
      "Epoch [552/1000], Loss: 0.1302\n",
      "Epoch [552/1000], Loss: 0.1452\n",
      "tensor(2.0008, grad_fn=<MeanBackward0>)\n",
      "552\n",
      "Epoch [553/1000], Loss: 0.3211\n",
      "Epoch [553/1000], Loss: 0.4217\n",
      "Epoch [553/1000], Loss: 0.4674\n",
      "Epoch [553/1000], Loss: 0.4730\n",
      "Epoch [553/1000], Loss: 0.3473\n",
      "Epoch [553/1000], Loss: 0.1652\n",
      "Epoch [553/1000], Loss: 0.1949\n",
      "Epoch [553/1000], Loss: 0.4266\n",
      "Epoch [553/1000], Loss: 0.6080\n",
      "Epoch [553/1000], Loss: 0.6799\n",
      "Epoch [553/1000], Loss: 0.5752\n",
      "tensor(2.5318, grad_fn=<MeanBackward0>)\n",
      "553\n",
      "Epoch [554/1000], Loss: 0.3862\n",
      "Epoch [554/1000], Loss: 0.2087\n",
      "Epoch [554/1000], Loss: 0.2657\n",
      "Epoch [554/1000], Loss: 0.4562\n",
      "Epoch [554/1000], Loss: 0.5047\n",
      "Epoch [554/1000], Loss: 0.5520\n",
      "Epoch [554/1000], Loss: 0.4433\n",
      "Epoch [554/1000], Loss: 0.3734\n",
      "Epoch [554/1000], Loss: 0.2845\n",
      "Epoch [554/1000], Loss: 0.2624\n",
      "Epoch [554/1000], Loss: 0.3582\n",
      "tensor(2.5635, grad_fn=<MeanBackward0>)\n",
      "554\n",
      "Epoch [555/1000], Loss: 0.4179\n",
      "Epoch [555/1000], Loss: 0.3771\n",
      "Epoch [555/1000], Loss: 0.2795\n",
      "Epoch [555/1000], Loss: 0.1911\n",
      "Epoch [555/1000], Loss: 0.2434\n",
      "Epoch [555/1000], Loss: 0.3007\n",
      "Epoch [555/1000], Loss: 0.2650\n",
      "Epoch [555/1000], Loss: 0.2342\n",
      "Epoch [555/1000], Loss: 0.1153\n",
      "Epoch [555/1000], Loss: 0.2004\n",
      "Epoch [555/1000], Loss: 0.2797\n",
      "tensor(2.5032, grad_fn=<MeanBackward0>)\n",
      "555\n",
      "Epoch [556/1000], Loss: 0.2617\n",
      "Epoch [556/1000], Loss: 0.1729\n",
      "Epoch [556/1000], Loss: 0.1368\n",
      "Epoch [556/1000], Loss: 0.2417\n",
      "Epoch [556/1000], Loss: 0.2903\n",
      "Epoch [556/1000], Loss: 0.2857\n",
      "Epoch [556/1000], Loss: 0.1856\n",
      "Epoch [556/1000], Loss: 0.1119\n",
      "Epoch [556/1000], Loss: 0.1803\n",
      "Epoch [556/1000], Loss: 0.2303\n",
      "Epoch [556/1000], Loss: 0.1938\n",
      "tensor(2.3271, grad_fn=<MeanBackward0>)\n",
      "556\n",
      "Epoch [557/1000], Loss: 0.0814\n",
      "Epoch [557/1000], Loss: 0.1578\n",
      "Epoch [557/1000], Loss: 0.2261\n",
      "Epoch [557/1000], Loss: 0.2302\n",
      "Epoch [557/1000], Loss: 0.1813\n",
      "Epoch [557/1000], Loss: 0.0957\n",
      "Epoch [557/1000], Loss: 0.1494\n",
      "Epoch [557/1000], Loss: 0.2334\n",
      "Epoch [557/1000], Loss: 0.2562\n",
      "Epoch [557/1000], Loss: 0.2162\n",
      "Epoch [557/1000], Loss: 0.0807\n",
      "tensor(2.2640, grad_fn=<MeanBackward0>)\n",
      "557\n",
      "Epoch [558/1000], Loss: 0.1501\n",
      "Epoch [558/1000], Loss: 0.2213\n",
      "Epoch [558/1000], Loss: 0.2749\n",
      "Epoch [558/1000], Loss: 0.2719\n",
      "Epoch [558/1000], Loss: 0.2124\n",
      "Epoch [558/1000], Loss: 0.1036\n",
      "Epoch [558/1000], Loss: 0.1067\n",
      "Epoch [558/1000], Loss: 0.2393\n",
      "Epoch [558/1000], Loss: 0.3231\n",
      "Epoch [558/1000], Loss: 0.3261\n",
      "Epoch [558/1000], Loss: 0.2117\n",
      "tensor(2.3381, grad_fn=<MeanBackward0>)\n",
      "558\n",
      "Epoch [559/1000], Loss: 0.0814\n",
      "Epoch [559/1000], Loss: 0.1624\n",
      "Epoch [559/1000], Loss: 0.2903\n",
      "Epoch [559/1000], Loss: 0.3921\n",
      "Epoch [559/1000], Loss: 0.3807\n",
      "Epoch [559/1000], Loss: 0.3380\n",
      "Epoch [559/1000], Loss: 0.2331\n",
      "Epoch [559/1000], Loss: 0.1121\n",
      "Epoch [559/1000], Loss: 0.2045\n",
      "Epoch [559/1000], Loss: 0.3542\n",
      "Epoch [559/1000], Loss: 0.4226\n",
      "tensor(2.6159, grad_fn=<MeanBackward0>)\n",
      "559\n",
      "Epoch [560/1000], Loss: 0.3924\n",
      "Epoch [560/1000], Loss: 0.3015\n",
      "Epoch [560/1000], Loss: 0.1720\n",
      "Epoch [560/1000], Loss: 0.2281\n",
      "Epoch [560/1000], Loss: 0.3230\n",
      "Epoch [560/1000], Loss: 0.4253\n",
      "Epoch [560/1000], Loss: 0.4292\n",
      "Epoch [560/1000], Loss: 0.4328\n",
      "Epoch [560/1000], Loss: 0.3419\n",
      "Epoch [560/1000], Loss: 0.1526\n",
      "Epoch [560/1000], Loss: 0.1519\n",
      "tensor(2.5718, grad_fn=<MeanBackward0>)\n",
      "560\n",
      "Epoch [561/1000], Loss: 0.3296\n",
      "Epoch [561/1000], Loss: 0.4023\n",
      "Epoch [561/1000], Loss: 0.3335\n",
      "Epoch [561/1000], Loss: 0.1848\n",
      "Epoch [561/1000], Loss: 0.1504\n",
      "Epoch [561/1000], Loss: 0.2384\n",
      "Epoch [561/1000], Loss: 0.3243\n",
      "Epoch [561/1000], Loss: 0.3654\n",
      "Epoch [561/1000], Loss: 0.3108\n",
      "Epoch [561/1000], Loss: 0.1470\n",
      "Epoch [561/1000], Loss: 0.0975\n",
      "tensor(2.5568, grad_fn=<MeanBackward0>)\n",
      "561\n",
      "Epoch [562/1000], Loss: 0.2643\n",
      "Epoch [562/1000], Loss: 0.3487\n",
      "Epoch [562/1000], Loss: 0.2838\n",
      "Epoch [562/1000], Loss: 0.1632\n",
      "Epoch [562/1000], Loss: 0.1244\n",
      "Epoch [562/1000], Loss: 0.2066\n",
      "Epoch [562/1000], Loss: 0.2322\n",
      "Epoch [562/1000], Loss: 0.2478\n",
      "Epoch [562/1000], Loss: 0.1945\n",
      "Epoch [562/1000], Loss: 0.0650\n",
      "Epoch [562/1000], Loss: 0.1254\n",
      "tensor(2.5287, grad_fn=<MeanBackward0>)\n",
      "562\n",
      "Epoch [563/1000], Loss: 0.2191\n",
      "Epoch [563/1000], Loss: 0.2217\n",
      "Epoch [563/1000], Loss: 0.1319\n",
      "Epoch [563/1000], Loss: 0.0961\n",
      "Epoch [563/1000], Loss: 0.1500\n",
      "Epoch [563/1000], Loss: 0.1594\n",
      "Epoch [563/1000], Loss: 0.1202\n",
      "Epoch [563/1000], Loss: 0.0684\n",
      "Epoch [563/1000], Loss: 0.0778\n",
      "Epoch [563/1000], Loss: 0.0944\n",
      "Epoch [563/1000], Loss: 0.0752\n",
      "tensor(2.4316, grad_fn=<MeanBackward0>)\n",
      "563\n",
      "Epoch [564/1000], Loss: 0.0958\n",
      "Epoch [564/1000], Loss: 0.0962\n",
      "Epoch [564/1000], Loss: 0.0793\n",
      "Epoch [564/1000], Loss: 0.0718\n",
      "Epoch [564/1000], Loss: 0.0757\n",
      "Epoch [564/1000], Loss: 0.0875\n",
      "Epoch [564/1000], Loss: 0.0835\n",
      "Epoch [564/1000], Loss: 0.0895\n",
      "Epoch [564/1000], Loss: 0.0769\n",
      "Epoch [564/1000], Loss: 0.0655\n",
      "Epoch [564/1000], Loss: 0.0523\n",
      "tensor(2.4478, grad_fn=<MeanBackward0>)\n",
      "564\n",
      "Epoch [565/1000], Loss: 0.0607\n",
      "Epoch [565/1000], Loss: 0.0867\n",
      "Epoch [565/1000], Loss: 0.0989\n",
      "Epoch [565/1000], Loss: 0.0718\n",
      "Epoch [565/1000], Loss: 0.0661\n",
      "Epoch [565/1000], Loss: 0.0519\n",
      "Epoch [565/1000], Loss: 0.0619\n",
      "Epoch [565/1000], Loss: 0.0662\n",
      "Epoch [565/1000], Loss: 0.0686\n",
      "Epoch [565/1000], Loss: 0.0705\n",
      "Epoch [565/1000], Loss: 0.0651\n",
      "tensor(2.4299, grad_fn=<MeanBackward0>)\n",
      "565\n",
      "Epoch [566/1000], Loss: 0.0347\n",
      "Epoch [566/1000], Loss: 0.0499\n",
      "Epoch [566/1000], Loss: 0.0626\n",
      "Epoch [566/1000], Loss: 0.0615\n",
      "Epoch [566/1000], Loss: 0.0728\n",
      "Epoch [566/1000], Loss: 0.0635\n",
      "Epoch [566/1000], Loss: 0.0583\n",
      "Epoch [566/1000], Loss: 0.0495\n",
      "Epoch [566/1000], Loss: 0.0561\n",
      "Epoch [566/1000], Loss: 0.0609\n",
      "Epoch [566/1000], Loss: 0.0733\n",
      "tensor(2.4022, grad_fn=<MeanBackward0>)\n",
      "566\n",
      "Epoch [567/1000], Loss: 0.0661\n",
      "Epoch [567/1000], Loss: 0.0605\n",
      "Epoch [567/1000], Loss: 0.0628\n",
      "Epoch [567/1000], Loss: 0.0641\n",
      "Epoch [567/1000], Loss: 0.0615\n",
      "Epoch [567/1000], Loss: 0.0565\n",
      "Epoch [567/1000], Loss: 0.0697\n",
      "Epoch [567/1000], Loss: 0.0649\n",
      "Epoch [567/1000], Loss: 0.0728\n",
      "Epoch [567/1000], Loss: 0.0707\n",
      "Epoch [567/1000], Loss: 0.0774\n",
      "tensor(2.4238, grad_fn=<MeanBackward0>)\n",
      "567\n",
      "Epoch [568/1000], Loss: 0.0925\n",
      "Epoch [568/1000], Loss: 0.0793\n",
      "Epoch [568/1000], Loss: 0.0756\n",
      "Epoch [568/1000], Loss: 0.0574\n",
      "Epoch [568/1000], Loss: 0.0641\n",
      "Epoch [568/1000], Loss: 0.0917\n",
      "Epoch [568/1000], Loss: 0.0803\n",
      "Epoch [568/1000], Loss: 0.0915\n",
      "Epoch [568/1000], Loss: 0.1231\n",
      "Epoch [568/1000], Loss: 0.1357\n",
      "Epoch [568/1000], Loss: 0.1063\n",
      "tensor(2.3886, grad_fn=<MeanBackward0>)\n",
      "568\n",
      "Epoch [569/1000], Loss: 0.0766\n",
      "Epoch [569/1000], Loss: 0.1272\n",
      "Epoch [569/1000], Loss: 0.1567\n",
      "Epoch [569/1000], Loss: 0.1554\n",
      "Epoch [569/1000], Loss: 0.1367\n",
      "Epoch [569/1000], Loss: 0.1054\n",
      "Epoch [569/1000], Loss: 0.1135\n",
      "Epoch [569/1000], Loss: 0.1173\n",
      "Epoch [569/1000], Loss: 0.1408\n",
      "Epoch [569/1000], Loss: 0.1192\n",
      "Epoch [569/1000], Loss: 0.1160\n",
      "tensor(2.4453, grad_fn=<MeanBackward0>)\n",
      "569\n",
      "Epoch [570/1000], Loss: 0.0997\n",
      "Epoch [570/1000], Loss: 0.0866\n",
      "Epoch [570/1000], Loss: 0.1555\n",
      "Epoch [570/1000], Loss: 0.1798\n",
      "Epoch [570/1000], Loss: 0.1498\n",
      "Epoch [570/1000], Loss: 0.1142\n",
      "Epoch [570/1000], Loss: 0.0985\n",
      "Epoch [570/1000], Loss: 0.1275\n",
      "Epoch [570/1000], Loss: 0.1521\n",
      "Epoch [570/1000], Loss: 0.1201\n",
      "Epoch [570/1000], Loss: 0.0976\n",
      "tensor(2.4284, grad_fn=<MeanBackward0>)\n",
      "570\n",
      "Epoch [571/1000], Loss: 0.0765\n",
      "Epoch [571/1000], Loss: 0.0962\n",
      "Epoch [571/1000], Loss: 0.1383\n",
      "Epoch [571/1000], Loss: 0.1444\n",
      "Epoch [571/1000], Loss: 0.1034\n",
      "Epoch [571/1000], Loss: 0.0934\n",
      "Epoch [571/1000], Loss: 0.0833\n",
      "Epoch [571/1000], Loss: 0.0929\n",
      "Epoch [571/1000], Loss: 0.1562\n",
      "Epoch [571/1000], Loss: 0.1588\n",
      "Epoch [571/1000], Loss: 0.1047\n",
      "tensor(2.4038, grad_fn=<MeanBackward0>)\n",
      "571\n",
      "Epoch [572/1000], Loss: 0.0925\n",
      "Epoch [572/1000], Loss: 0.1154\n",
      "Epoch [572/1000], Loss: 0.2034\n",
      "Epoch [572/1000], Loss: 0.2377\n",
      "Epoch [572/1000], Loss: 0.2234\n",
      "Epoch [572/1000], Loss: 0.1508\n",
      "Epoch [572/1000], Loss: 0.0655\n",
      "Epoch [572/1000], Loss: 0.1222\n",
      "Epoch [572/1000], Loss: 0.2434\n",
      "Epoch [572/1000], Loss: 0.2514\n",
      "Epoch [572/1000], Loss: 0.2875\n",
      "tensor(2.2660, grad_fn=<MeanBackward0>)\n",
      "572\n",
      "Epoch [573/1000], Loss: 0.2486\n",
      "Epoch [573/1000], Loss: 0.1608\n",
      "Epoch [573/1000], Loss: 0.1329\n",
      "Epoch [573/1000], Loss: 0.2835\n",
      "Epoch [573/1000], Loss: 0.4014\n",
      "Epoch [573/1000], Loss: 0.4736\n",
      "Epoch [573/1000], Loss: 0.4338\n",
      "Epoch [573/1000], Loss: 0.3673\n",
      "Epoch [573/1000], Loss: 0.1742\n",
      "Epoch [573/1000], Loss: 0.1373\n",
      "Epoch [573/1000], Loss: 0.3691\n",
      "tensor(2.0507, grad_fn=<MeanBackward0>)\n",
      "573\n",
      "Epoch [574/1000], Loss: 0.5300\n",
      "Epoch [574/1000], Loss: 0.6030\n",
      "Epoch [574/1000], Loss: 0.5558\n",
      "Epoch [574/1000], Loss: 0.4568\n",
      "Epoch [574/1000], Loss: 0.2229\n",
      "Epoch [574/1000], Loss: 0.1122\n",
      "Epoch [574/1000], Loss: 0.3360\n",
      "Epoch [574/1000], Loss: 0.4865\n",
      "Epoch [574/1000], Loss: 0.5734\n",
      "Epoch [574/1000], Loss: 0.4794\n",
      "Epoch [574/1000], Loss: 0.2463\n",
      "tensor(2.3126, grad_fn=<MeanBackward0>)\n",
      "574\n",
      "Epoch [575/1000], Loss: 0.1574\n",
      "Epoch [575/1000], Loss: 0.2696\n",
      "Epoch [575/1000], Loss: 0.3485\n",
      "Epoch [575/1000], Loss: 0.4134\n",
      "Epoch [575/1000], Loss: 0.3045\n",
      "Epoch [575/1000], Loss: 0.2638\n",
      "Epoch [575/1000], Loss: 0.1679\n",
      "Epoch [575/1000], Loss: 0.1547\n",
      "Epoch [575/1000], Loss: 0.2125\n",
      "Epoch [575/1000], Loss: 0.2779\n",
      "Epoch [575/1000], Loss: 0.1955\n",
      "tensor(2.4291, grad_fn=<MeanBackward0>)\n",
      "575\n",
      "Epoch [576/1000], Loss: 0.1525\n",
      "Epoch [576/1000], Loss: 0.1476\n",
      "Epoch [576/1000], Loss: 0.1840\n",
      "Epoch [576/1000], Loss: 0.2277\n",
      "Epoch [576/1000], Loss: 0.1587\n",
      "Epoch [576/1000], Loss: 0.1088\n",
      "Epoch [576/1000], Loss: 0.0911\n",
      "Epoch [576/1000], Loss: 0.1694\n",
      "Epoch [576/1000], Loss: 0.1955\n",
      "Epoch [576/1000], Loss: 0.1554\n",
      "Epoch [576/1000], Loss: 0.0905\n",
      "tensor(2.2732, grad_fn=<MeanBackward0>)\n",
      "576\n",
      "Epoch [577/1000], Loss: 0.1248\n",
      "Epoch [577/1000], Loss: 0.1648\n",
      "Epoch [577/1000], Loss: 0.1786\n",
      "Epoch [577/1000], Loss: 0.1402\n",
      "Epoch [577/1000], Loss: 0.0780\n",
      "Epoch [577/1000], Loss: 0.1175\n",
      "Epoch [577/1000], Loss: 0.1576\n",
      "Epoch [577/1000], Loss: 0.2005\n",
      "Epoch [577/1000], Loss: 0.1608\n",
      "Epoch [577/1000], Loss: 0.0963\n",
      "Epoch [577/1000], Loss: 0.1078\n",
      "tensor(2.1901, grad_fn=<MeanBackward0>)\n",
      "577\n",
      "Epoch [578/1000], Loss: 0.1757\n",
      "Epoch [578/1000], Loss: 0.1885\n",
      "Epoch [578/1000], Loss: 0.1647\n",
      "Epoch [578/1000], Loss: 0.0879\n",
      "Epoch [578/1000], Loss: 0.0886\n",
      "Epoch [578/1000], Loss: 0.1481\n",
      "Epoch [578/1000], Loss: 0.1396\n",
      "Epoch [578/1000], Loss: 0.1447\n",
      "Epoch [578/1000], Loss: 0.0879\n",
      "Epoch [578/1000], Loss: 0.0770\n",
      "Epoch [578/1000], Loss: 0.0888\n",
      "tensor(2.2222, grad_fn=<MeanBackward0>)\n",
      "578\n",
      "Epoch [579/1000], Loss: 0.1122\n",
      "Epoch [579/1000], Loss: 0.0962\n",
      "Epoch [579/1000], Loss: 0.0834\n",
      "Epoch [579/1000], Loss: 0.0551\n",
      "Epoch [579/1000], Loss: 0.0611\n",
      "Epoch [579/1000], Loss: 0.0919\n",
      "Epoch [579/1000], Loss: 0.0808\n",
      "Epoch [579/1000], Loss: 0.1027\n",
      "Epoch [579/1000], Loss: 0.0913\n",
      "Epoch [579/1000], Loss: 0.0675\n",
      "Epoch [579/1000], Loss: 0.0609\n",
      "tensor(2.2408, grad_fn=<MeanBackward0>)\n",
      "579\n",
      "Epoch [580/1000], Loss: 0.1099\n",
      "Epoch [580/1000], Loss: 0.1239\n",
      "Epoch [580/1000], Loss: 0.1081\n",
      "Epoch [580/1000], Loss: 0.0624\n",
      "Epoch [580/1000], Loss: 0.0601\n",
      "Epoch [580/1000], Loss: 0.0800\n",
      "Epoch [580/1000], Loss: 0.0766\n",
      "Epoch [580/1000], Loss: 0.0997\n",
      "Epoch [580/1000], Loss: 0.0750\n",
      "Epoch [580/1000], Loss: 0.0642\n",
      "Epoch [580/1000], Loss: 0.0483\n",
      "tensor(2.2111, grad_fn=<MeanBackward0>)\n",
      "580\n",
      "Epoch [581/1000], Loss: 0.1259\n",
      "Epoch [581/1000], Loss: 0.1892\n",
      "Epoch [581/1000], Loss: 0.1814\n",
      "Epoch [581/1000], Loss: 0.1321\n",
      "Epoch [581/1000], Loss: 0.0477\n",
      "Epoch [581/1000], Loss: 0.1257\n",
      "Epoch [581/1000], Loss: 0.1688\n",
      "Epoch [581/1000], Loss: 0.1874\n",
      "Epoch [581/1000], Loss: 0.1297\n",
      "Epoch [581/1000], Loss: 0.0932\n",
      "Epoch [581/1000], Loss: 0.0736\n",
      "tensor(2.1819, grad_fn=<MeanBackward0>)\n",
      "581\n",
      "Epoch [582/1000], Loss: 0.1220\n",
      "Epoch [582/1000], Loss: 0.1464\n",
      "Epoch [582/1000], Loss: 0.1674\n",
      "Epoch [582/1000], Loss: 0.1387\n",
      "Epoch [582/1000], Loss: 0.0793\n",
      "Epoch [582/1000], Loss: 0.1107\n",
      "Epoch [582/1000], Loss: 0.1768\n",
      "Epoch [582/1000], Loss: 0.2356\n",
      "Epoch [582/1000], Loss: 0.2300\n",
      "Epoch [582/1000], Loss: 0.1770\n",
      "Epoch [582/1000], Loss: 0.0774\n",
      "tensor(2.1486, grad_fn=<MeanBackward0>)\n",
      "582\n",
      "Epoch [583/1000], Loss: 0.1447\n",
      "Epoch [583/1000], Loss: 0.2664\n",
      "Epoch [583/1000], Loss: 0.3385\n",
      "Epoch [583/1000], Loss: 0.3911\n",
      "Epoch [583/1000], Loss: 0.3263\n",
      "Epoch [583/1000], Loss: 0.2728\n",
      "Epoch [583/1000], Loss: 0.1548\n",
      "Epoch [583/1000], Loss: 0.1839\n",
      "Epoch [583/1000], Loss: 0.3415\n",
      "Epoch [583/1000], Loss: 0.4482\n",
      "Epoch [583/1000], Loss: 0.4386\n",
      "tensor(2.5277, grad_fn=<MeanBackward0>)\n",
      "583\n",
      "Epoch [584/1000], Loss: 0.3716\n",
      "Epoch [584/1000], Loss: 0.2084\n",
      "Epoch [584/1000], Loss: 0.1767\n",
      "Epoch [584/1000], Loss: 0.3430\n",
      "Epoch [584/1000], Loss: 0.4344\n",
      "Epoch [584/1000], Loss: 0.5196\n",
      "Epoch [584/1000], Loss: 0.4678\n",
      "Epoch [584/1000], Loss: 0.4487\n",
      "Epoch [584/1000], Loss: 0.3370\n",
      "Epoch [584/1000], Loss: 0.2177\n",
      "Epoch [584/1000], Loss: 0.2543\n",
      "tensor(2.6221, grad_fn=<MeanBackward0>)\n",
      "584\n",
      "Epoch [585/1000], Loss: 0.4330\n",
      "Epoch [585/1000], Loss: 0.4843\n",
      "Epoch [585/1000], Loss: 0.4297\n",
      "Epoch [585/1000], Loss: 0.2494\n",
      "Epoch [585/1000], Loss: 0.1704\n",
      "Epoch [585/1000], Loss: 0.2969\n",
      "Epoch [585/1000], Loss: 0.3473\n",
      "Epoch [585/1000], Loss: 0.4384\n",
      "Epoch [585/1000], Loss: 0.4267\n",
      "Epoch [585/1000], Loss: 0.2701\n",
      "Epoch [585/1000], Loss: 0.1247\n",
      "tensor(2.4518, grad_fn=<MeanBackward0>)\n",
      "585\n",
      "Epoch [586/1000], Loss: 0.2204\n",
      "Epoch [586/1000], Loss: 0.3130\n",
      "Epoch [586/1000], Loss: 0.2543\n",
      "Epoch [586/1000], Loss: 0.1742\n",
      "Epoch [586/1000], Loss: 0.0812\n",
      "Epoch [586/1000], Loss: 0.1816\n",
      "Epoch [586/1000], Loss: 0.1730\n",
      "Epoch [586/1000], Loss: 0.1856\n",
      "Epoch [586/1000], Loss: 0.1304\n",
      "Epoch [586/1000], Loss: 0.1204\n",
      "Epoch [586/1000], Loss: 0.2264\n",
      "tensor(2.4816, grad_fn=<MeanBackward0>)\n",
      "586\n",
      "Epoch [587/1000], Loss: 0.2086\n",
      "Epoch [587/1000], Loss: 0.0982\n",
      "Epoch [587/1000], Loss: 0.1187\n",
      "Epoch [587/1000], Loss: 0.1817\n",
      "Epoch [587/1000], Loss: 0.1947\n",
      "Epoch [587/1000], Loss: 0.1503\n",
      "Epoch [587/1000], Loss: 0.0780\n",
      "Epoch [587/1000], Loss: 0.1259\n",
      "Epoch [587/1000], Loss: 0.1608\n",
      "Epoch [587/1000], Loss: 0.1384\n",
      "Epoch [587/1000], Loss: 0.0750\n",
      "tensor(2.3467, grad_fn=<MeanBackward0>)\n",
      "587\n",
      "Epoch [588/1000], Loss: 0.0976\n",
      "Epoch [588/1000], Loss: 0.1500\n",
      "Epoch [588/1000], Loss: 0.1517\n",
      "Epoch [588/1000], Loss: 0.1298\n",
      "Epoch [588/1000], Loss: 0.0847\n",
      "Epoch [588/1000], Loss: 0.1072\n",
      "Epoch [588/1000], Loss: 0.1262\n",
      "Epoch [588/1000], Loss: 0.0798\n",
      "Epoch [588/1000], Loss: 0.0637\n",
      "Epoch [588/1000], Loss: 0.0570\n",
      "Epoch [588/1000], Loss: 0.0525\n",
      "tensor(2.3650, grad_fn=<MeanBackward0>)\n",
      "588\n",
      "Epoch [589/1000], Loss: 0.0619\n",
      "Epoch [589/1000], Loss: 0.0580\n",
      "Epoch [589/1000], Loss: 0.0649\n",
      "Epoch [589/1000], Loss: 0.0604\n",
      "Epoch [589/1000], Loss: 0.0545\n",
      "Epoch [589/1000], Loss: 0.0607\n",
      "Epoch [589/1000], Loss: 0.0596\n",
      "Epoch [589/1000], Loss: 0.0801\n",
      "Epoch [589/1000], Loss: 0.0664\n",
      "Epoch [589/1000], Loss: 0.0611\n",
      "Epoch [589/1000], Loss: 0.0487\n",
      "tensor(2.3290, grad_fn=<MeanBackward0>)\n",
      "589\n",
      "Epoch [590/1000], Loss: 0.0485\n",
      "Epoch [590/1000], Loss: 0.0796\n",
      "Epoch [590/1000], Loss: 0.0717\n",
      "Epoch [590/1000], Loss: 0.0650\n",
      "Epoch [590/1000], Loss: 0.0588\n",
      "Epoch [590/1000], Loss: 0.0485\n",
      "Epoch [590/1000], Loss: 0.0519\n",
      "Epoch [590/1000], Loss: 0.0613\n",
      "Epoch [590/1000], Loss: 0.0686\n",
      "Epoch [590/1000], Loss: 0.0768\n",
      "Epoch [590/1000], Loss: 0.0569\n",
      "tensor(2.2999, grad_fn=<MeanBackward0>)\n",
      "590\n",
      "Epoch [591/1000], Loss: 0.0494\n",
      "Epoch [591/1000], Loss: 0.1031\n",
      "Epoch [591/1000], Loss: 0.0987\n",
      "Epoch [591/1000], Loss: 0.0992\n",
      "Epoch [591/1000], Loss: 0.0704\n",
      "Epoch [591/1000], Loss: 0.0612\n",
      "Epoch [591/1000], Loss: 0.0560\n",
      "Epoch [591/1000], Loss: 0.0613\n",
      "Epoch [591/1000], Loss: 0.0646\n",
      "Epoch [591/1000], Loss: 0.0818\n",
      "Epoch [591/1000], Loss: 0.1063\n",
      "tensor(2.4003, grad_fn=<MeanBackward0>)\n",
      "591\n",
      "Epoch [592/1000], Loss: 0.1013\n",
      "Epoch [592/1000], Loss: 0.0942\n",
      "Epoch [592/1000], Loss: 0.1028\n",
      "Epoch [592/1000], Loss: 0.1452\n",
      "Epoch [592/1000], Loss: 0.1302\n",
      "Epoch [592/1000], Loss: 0.1115\n",
      "Epoch [592/1000], Loss: 0.1000\n",
      "Epoch [592/1000], Loss: 0.1365\n",
      "Epoch [592/1000], Loss: 0.2179\n",
      "Epoch [592/1000], Loss: 0.1887\n",
      "Epoch [592/1000], Loss: 0.1355\n",
      "tensor(2.3572, grad_fn=<MeanBackward0>)\n",
      "592\n",
      "Epoch [593/1000], Loss: 0.0898\n",
      "Epoch [593/1000], Loss: 0.1066\n",
      "Epoch [593/1000], Loss: 0.1787\n",
      "Epoch [593/1000], Loss: 0.2409\n",
      "Epoch [593/1000], Loss: 0.2295\n",
      "Epoch [593/1000], Loss: 0.1947\n",
      "Epoch [593/1000], Loss: 0.1293\n",
      "Epoch [593/1000], Loss: 0.0856\n",
      "Epoch [593/1000], Loss: 0.1469\n",
      "Epoch [593/1000], Loss: 0.2632\n",
      "Epoch [593/1000], Loss: 0.2868\n",
      "tensor(2.5467, grad_fn=<MeanBackward0>)\n",
      "593\n",
      "Epoch [594/1000], Loss: 0.2758\n",
      "Epoch [594/1000], Loss: 0.1821\n",
      "Epoch [594/1000], Loss: 0.0804\n",
      "Epoch [594/1000], Loss: 0.2340\n",
      "Epoch [594/1000], Loss: 0.3497\n",
      "Epoch [594/1000], Loss: 0.4172\n",
      "Epoch [594/1000], Loss: 0.4141\n",
      "Epoch [594/1000], Loss: 0.3795\n",
      "Epoch [594/1000], Loss: 0.2520\n",
      "Epoch [594/1000], Loss: 0.1228\n",
      "Epoch [594/1000], Loss: 0.2826\n",
      "tensor(2.7127, grad_fn=<MeanBackward0>)\n",
      "594\n",
      "Epoch [595/1000], Loss: 0.5140\n",
      "Epoch [595/1000], Loss: 0.6169\n",
      "Epoch [595/1000], Loss: 0.6214\n",
      "Epoch [595/1000], Loss: 0.4520\n",
      "Epoch [595/1000], Loss: 0.2285\n",
      "Epoch [595/1000], Loss: 0.1884\n",
      "Epoch [595/1000], Loss: 0.2847\n",
      "Epoch [595/1000], Loss: 0.5142\n",
      "Epoch [595/1000], Loss: 0.5994\n",
      "Epoch [595/1000], Loss: 0.5251\n",
      "Epoch [595/1000], Loss: 0.4634\n",
      "tensor(2.2478, grad_fn=<MeanBackward0>)\n",
      "595\n",
      "Epoch [596/1000], Loss: 0.2251\n",
      "Epoch [596/1000], Loss: 0.1303\n",
      "Epoch [596/1000], Loss: 0.2330\n",
      "Epoch [596/1000], Loss: 0.3138\n",
      "Epoch [596/1000], Loss: 0.2756\n",
      "Epoch [596/1000], Loss: 0.1824\n",
      "Epoch [596/1000], Loss: 0.1076\n",
      "Epoch [596/1000], Loss: 0.1625\n",
      "Epoch [596/1000], Loss: 0.2388\n",
      "Epoch [596/1000], Loss: 0.1910\n",
      "Epoch [596/1000], Loss: 0.1174\n",
      "tensor(2.4575, grad_fn=<MeanBackward0>)\n",
      "596\n",
      "Epoch [597/1000], Loss: 0.1165\n",
      "Epoch [597/1000], Loss: 0.1773\n",
      "Epoch [597/1000], Loss: 0.1741\n",
      "Epoch [597/1000], Loss: 0.1150\n",
      "Epoch [597/1000], Loss: 0.1041\n",
      "Epoch [597/1000], Loss: 0.1266\n",
      "Epoch [597/1000], Loss: 0.1186\n",
      "Epoch [597/1000], Loss: 0.0871\n",
      "Epoch [597/1000], Loss: 0.0702\n",
      "Epoch [597/1000], Loss: 0.0980\n",
      "Epoch [597/1000], Loss: 0.1203\n",
      "tensor(2.4810, grad_fn=<MeanBackward0>)\n",
      "597\n",
      "Epoch [598/1000], Loss: 0.1108\n",
      "Epoch [598/1000], Loss: 0.0690\n",
      "Epoch [598/1000], Loss: 0.0811\n",
      "Epoch [598/1000], Loss: 0.0819\n",
      "Epoch [598/1000], Loss: 0.0837\n",
      "Epoch [598/1000], Loss: 0.0833\n",
      "Epoch [598/1000], Loss: 0.0686\n",
      "Epoch [598/1000], Loss: 0.0570\n",
      "Epoch [598/1000], Loss: 0.0721\n",
      "Epoch [598/1000], Loss: 0.0711\n",
      "Epoch [598/1000], Loss: 0.0573\n",
      "tensor(2.4387, grad_fn=<MeanBackward0>)\n",
      "598\n",
      "Epoch [599/1000], Loss: 0.0752\n",
      "Epoch [599/1000], Loss: 0.0459\n",
      "Epoch [599/1000], Loss: 0.0585\n",
      "Epoch [599/1000], Loss: 0.0788\n",
      "Epoch [599/1000], Loss: 0.0591\n",
      "Epoch [599/1000], Loss: 0.0557\n",
      "Epoch [599/1000], Loss: 0.0511\n",
      "Epoch [599/1000], Loss: 0.0440\n",
      "Epoch [599/1000], Loss: 0.0587\n",
      "Epoch [599/1000], Loss: 0.0539\n",
      "Epoch [599/1000], Loss: 0.0557\n",
      "tensor(2.4668, grad_fn=<MeanBackward0>)\n",
      "599\n",
      "Epoch [600/1000], Loss: 0.1089\n",
      "Epoch [600/1000], Loss: 0.0950\n",
      "Epoch [600/1000], Loss: 0.0458\n",
      "Epoch [600/1000], Loss: 0.0873\n",
      "Epoch [600/1000], Loss: 0.1161\n",
      "Epoch [600/1000], Loss: 0.0957\n",
      "Epoch [600/1000], Loss: 0.0490\n",
      "Epoch [600/1000], Loss: 0.0646\n",
      "Epoch [600/1000], Loss: 0.0650\n",
      "Epoch [600/1000], Loss: 0.0421\n",
      "Epoch [600/1000], Loss: 0.0522\n",
      "tensor(2.4248, grad_fn=<MeanBackward0>)\n",
      "600\n",
      "Epoch [601/1000], Loss: 0.0398\n",
      "Epoch [601/1000], Loss: 0.0669\n",
      "Epoch [601/1000], Loss: 0.0554\n",
      "Epoch [601/1000], Loss: 0.0466\n",
      "Epoch [601/1000], Loss: 0.0968\n",
      "Epoch [601/1000], Loss: 0.1207\n",
      "Epoch [601/1000], Loss: 0.0836\n",
      "Epoch [601/1000], Loss: 0.0552\n",
      "Epoch [601/1000], Loss: 0.0795\n",
      "Epoch [601/1000], Loss: 0.1008\n",
      "Epoch [601/1000], Loss: 0.0596\n",
      "tensor(2.4021, grad_fn=<MeanBackward0>)\n",
      "601\n",
      "Epoch [602/1000], Loss: 0.0742\n",
      "Epoch [602/1000], Loss: 0.0752\n",
      "Epoch [602/1000], Loss: 0.0438\n",
      "Epoch [602/1000], Loss: 0.0907\n",
      "Epoch [602/1000], Loss: 0.0659\n",
      "Epoch [602/1000], Loss: 0.0931\n",
      "Epoch [602/1000], Loss: 0.1263\n",
      "Epoch [602/1000], Loss: 0.1373\n",
      "Epoch [602/1000], Loss: 0.0919\n",
      "Epoch [602/1000], Loss: 0.1178\n",
      "Epoch [602/1000], Loss: 0.1844\n",
      "tensor(2.5602, grad_fn=<MeanBackward0>)\n",
      "602\n",
      "Epoch [603/1000], Loss: 0.1795\n",
      "Epoch [603/1000], Loss: 0.1245\n",
      "Epoch [603/1000], Loss: 0.0953\n",
      "Epoch [603/1000], Loss: 0.1004\n",
      "Epoch [603/1000], Loss: 0.1110\n",
      "Epoch [603/1000], Loss: 0.1132\n",
      "Epoch [603/1000], Loss: 0.0891\n",
      "Epoch [603/1000], Loss: 0.0837\n",
      "Epoch [603/1000], Loss: 0.0642\n",
      "Epoch [603/1000], Loss: 0.0891\n",
      "Epoch [603/1000], Loss: 0.1116\n",
      "tensor(2.4945, grad_fn=<MeanBackward0>)\n",
      "603\n",
      "Epoch [604/1000], Loss: 0.0812\n",
      "Epoch [604/1000], Loss: 0.0533\n",
      "Epoch [604/1000], Loss: 0.0663\n",
      "Epoch [604/1000], Loss: 0.0551\n",
      "Epoch [604/1000], Loss: 0.0603\n",
      "Epoch [604/1000], Loss: 0.0695\n",
      "Epoch [604/1000], Loss: 0.0654\n",
      "Epoch [604/1000], Loss: 0.0938\n",
      "Epoch [604/1000], Loss: 0.0732\n",
      "Epoch [604/1000], Loss: 0.0552\n",
      "Epoch [604/1000], Loss: 0.1037\n",
      "tensor(2.5210, grad_fn=<MeanBackward0>)\n",
      "604\n",
      "Epoch [605/1000], Loss: 0.1120\n",
      "Epoch [605/1000], Loss: 0.0848\n",
      "Epoch [605/1000], Loss: 0.0390\n",
      "Epoch [605/1000], Loss: 0.0668\n",
      "Epoch [605/1000], Loss: 0.0749\n",
      "Epoch [605/1000], Loss: 0.0547\n",
      "Epoch [605/1000], Loss: 0.0509\n",
      "Epoch [605/1000], Loss: 0.0442\n",
      "Epoch [605/1000], Loss: 0.0567\n",
      "Epoch [605/1000], Loss: 0.0585\n",
      "Epoch [605/1000], Loss: 0.0844\n",
      "tensor(2.5430, grad_fn=<MeanBackward0>)\n",
      "605\n",
      "Epoch [606/1000], Loss: 0.1519\n",
      "Epoch [606/1000], Loss: 0.1715\n",
      "Epoch [606/1000], Loss: 0.1226\n",
      "Epoch [606/1000], Loss: 0.0800\n",
      "Epoch [606/1000], Loss: 0.0917\n",
      "Epoch [606/1000], Loss: 0.1502\n",
      "Epoch [606/1000], Loss: 0.1644\n",
      "Epoch [606/1000], Loss: 0.1964\n",
      "Epoch [606/1000], Loss: 0.2087\n",
      "Epoch [606/1000], Loss: 0.1759\n",
      "Epoch [606/1000], Loss: 0.1086\n",
      "tensor(2.6433, grad_fn=<MeanBackward0>)\n",
      "606\n",
      "Epoch [607/1000], Loss: 0.2137\n",
      "Epoch [607/1000], Loss: 0.3376\n",
      "Epoch [607/1000], Loss: 0.4236\n",
      "Epoch [607/1000], Loss: 0.4141\n",
      "Epoch [607/1000], Loss: 0.2788\n",
      "Epoch [607/1000], Loss: 0.0971\n",
      "Epoch [607/1000], Loss: 0.1926\n",
      "Epoch [607/1000], Loss: 0.3840\n",
      "Epoch [607/1000], Loss: 0.5506\n",
      "Epoch [607/1000], Loss: 0.6132\n",
      "Epoch [607/1000], Loss: 0.6221\n",
      "tensor(2.1809, grad_fn=<MeanBackward0>)\n",
      "607\n",
      "Epoch [608/1000], Loss: 0.4949\n",
      "Epoch [608/1000], Loss: 0.2619\n",
      "Epoch [608/1000], Loss: 0.1433\n",
      "Epoch [608/1000], Loss: 0.3554\n",
      "Epoch [608/1000], Loss: 0.5262\n",
      "Epoch [608/1000], Loss: 0.5667\n",
      "Epoch [608/1000], Loss: 0.5507\n",
      "Epoch [608/1000], Loss: 0.4064\n",
      "Epoch [608/1000], Loss: 0.2351\n",
      "Epoch [608/1000], Loss: 0.1946\n",
      "Epoch [608/1000], Loss: 0.4063\n",
      "tensor(2.1329, grad_fn=<MeanBackward0>)\n",
      "608\n",
      "Epoch [609/1000], Loss: 0.5130\n",
      "Epoch [609/1000], Loss: 0.5170\n",
      "Epoch [609/1000], Loss: 0.4330\n",
      "Epoch [609/1000], Loss: 0.2786\n",
      "Epoch [609/1000], Loss: 0.1311\n",
      "Epoch [609/1000], Loss: 0.1901\n",
      "Epoch [609/1000], Loss: 0.3046\n",
      "Epoch [609/1000], Loss: 0.2602\n",
      "Epoch [609/1000], Loss: 0.2099\n",
      "Epoch [609/1000], Loss: 0.1303\n",
      "Epoch [609/1000], Loss: 0.2075\n",
      "tensor(2.2974, grad_fn=<MeanBackward0>)\n",
      "609\n",
      "Epoch [610/1000], Loss: 0.2068\n",
      "Epoch [610/1000], Loss: 0.1447\n",
      "Epoch [610/1000], Loss: 0.0912\n",
      "Epoch [610/1000], Loss: 0.1185\n",
      "Epoch [610/1000], Loss: 0.1302\n",
      "Epoch [610/1000], Loss: 0.1090\n",
      "Epoch [610/1000], Loss: 0.0802\n",
      "Epoch [610/1000], Loss: 0.1358\n",
      "Epoch [610/1000], Loss: 0.1415\n",
      "Epoch [610/1000], Loss: 0.0950\n",
      "Epoch [610/1000], Loss: 0.0763\n",
      "tensor(2.4900, grad_fn=<MeanBackward0>)\n",
      "610\n",
      "Epoch [611/1000], Loss: 0.1333\n",
      "Epoch [611/1000], Loss: 0.1698\n",
      "Epoch [611/1000], Loss: 0.1569\n",
      "Epoch [611/1000], Loss: 0.0638\n",
      "Epoch [611/1000], Loss: 0.0978\n",
      "Epoch [611/1000], Loss: 0.1706\n",
      "Epoch [611/1000], Loss: 0.1508\n",
      "Epoch [611/1000], Loss: 0.1003\n",
      "Epoch [611/1000], Loss: 0.0581\n",
      "Epoch [611/1000], Loss: 0.0709\n",
      "Epoch [611/1000], Loss: 0.0847\n",
      "tensor(2.4931, grad_fn=<MeanBackward0>)\n",
      "611\n",
      "Epoch [612/1000], Loss: 0.0851\n",
      "Epoch [612/1000], Loss: 0.0610\n",
      "Epoch [612/1000], Loss: 0.0757\n",
      "Epoch [612/1000], Loss: 0.0718\n",
      "Epoch [612/1000], Loss: 0.0800\n",
      "Epoch [612/1000], Loss: 0.0838\n",
      "Epoch [612/1000], Loss: 0.0832\n",
      "Epoch [612/1000], Loss: 0.0617\n",
      "Epoch [612/1000], Loss: 0.0524\n",
      "Epoch [612/1000], Loss: 0.0520\n",
      "Epoch [612/1000], Loss: 0.0584\n",
      "tensor(2.4976, grad_fn=<MeanBackward0>)\n",
      "612\n",
      "Epoch [613/1000], Loss: 0.0833\n",
      "Epoch [613/1000], Loss: 0.1290\n",
      "Epoch [613/1000], Loss: 0.1090\n",
      "Epoch [613/1000], Loss: 0.0928\n",
      "Epoch [613/1000], Loss: 0.0848\n",
      "Epoch [613/1000], Loss: 0.1445\n",
      "Epoch [613/1000], Loss: 0.1265\n",
      "Epoch [613/1000], Loss: 0.1092\n",
      "Epoch [613/1000], Loss: 0.0905\n",
      "Epoch [613/1000], Loss: 0.0502\n",
      "Epoch [613/1000], Loss: 0.0522\n",
      "tensor(2.5103, grad_fn=<MeanBackward0>)\n",
      "613\n",
      "Epoch [614/1000], Loss: 0.0674\n",
      "Epoch [614/1000], Loss: 0.1072\n",
      "Epoch [614/1000], Loss: 0.1250\n",
      "Epoch [614/1000], Loss: 0.1052\n",
      "Epoch [614/1000], Loss: 0.0786\n",
      "Epoch [614/1000], Loss: 0.1078\n",
      "Epoch [614/1000], Loss: 0.1465\n",
      "Epoch [614/1000], Loss: 0.2012\n",
      "Epoch [614/1000], Loss: 0.2449\n",
      "Epoch [614/1000], Loss: 0.1929\n",
      "Epoch [614/1000], Loss: 0.1269\n",
      "tensor(2.5636, grad_fn=<MeanBackward0>)\n",
      "614\n",
      "Epoch [615/1000], Loss: 0.1160\n",
      "Epoch [615/1000], Loss: 0.2446\n",
      "Epoch [615/1000], Loss: 0.3491\n",
      "Epoch [615/1000], Loss: 0.3870\n",
      "Epoch [615/1000], Loss: 0.3626\n",
      "Epoch [615/1000], Loss: 0.2358\n",
      "Epoch [615/1000], Loss: 0.1305\n",
      "Epoch [615/1000], Loss: 0.2224\n",
      "Epoch [615/1000], Loss: 0.4300\n",
      "Epoch [615/1000], Loss: 0.4958\n",
      "Epoch [615/1000], Loss: 0.5927\n",
      "tensor(2.1011, grad_fn=<MeanBackward0>)\n",
      "615\n",
      "Epoch [616/1000], Loss: 0.5826\n",
      "Epoch [616/1000], Loss: 0.4099\n",
      "Epoch [616/1000], Loss: 0.1932\n",
      "Epoch [616/1000], Loss: 0.1937\n",
      "Epoch [616/1000], Loss: 0.4523\n",
      "Epoch [616/1000], Loss: 0.5681\n",
      "Epoch [616/1000], Loss: 0.5914\n",
      "Epoch [616/1000], Loss: 0.5238\n",
      "Epoch [616/1000], Loss: 0.3960\n",
      "Epoch [616/1000], Loss: 0.2156\n",
      "Epoch [616/1000], Loss: 0.3032\n",
      "tensor(2.1385, grad_fn=<MeanBackward0>)\n",
      "616\n",
      "Epoch [617/1000], Loss: 0.4377\n",
      "Epoch [617/1000], Loss: 0.4895\n",
      "Epoch [617/1000], Loss: 0.4902\n",
      "Epoch [617/1000], Loss: 0.4042\n",
      "Epoch [617/1000], Loss: 0.2269\n",
      "Epoch [617/1000], Loss: 0.1403\n",
      "Epoch [617/1000], Loss: 0.2152\n",
      "Epoch [617/1000], Loss: 0.2766\n",
      "Epoch [617/1000], Loss: 0.2716\n",
      "Epoch [617/1000], Loss: 0.1848\n",
      "Epoch [617/1000], Loss: 0.1265\n",
      "tensor(2.3426, grad_fn=<MeanBackward0>)\n",
      "617\n",
      "Epoch [618/1000], Loss: 0.1649\n",
      "Epoch [618/1000], Loss: 0.1749\n",
      "Epoch [618/1000], Loss: 0.1100\n",
      "Epoch [618/1000], Loss: 0.0865\n",
      "Epoch [618/1000], Loss: 0.1431\n",
      "Epoch [618/1000], Loss: 0.1319\n",
      "Epoch [618/1000], Loss: 0.0833\n",
      "Epoch [618/1000], Loss: 0.1012\n",
      "Epoch [618/1000], Loss: 0.1094\n",
      "Epoch [618/1000], Loss: 0.1243\n",
      "Epoch [618/1000], Loss: 0.0920\n",
      "tensor(2.4295, grad_fn=<MeanBackward0>)\n",
      "618\n",
      "Epoch [619/1000], Loss: 0.0997\n",
      "Epoch [619/1000], Loss: 0.1366\n",
      "Epoch [619/1000], Loss: 0.1232\n",
      "Epoch [619/1000], Loss: 0.0710\n",
      "Epoch [619/1000], Loss: 0.0902\n",
      "Epoch [619/1000], Loss: 0.1411\n",
      "Epoch [619/1000], Loss: 0.1227\n",
      "Epoch [619/1000], Loss: 0.0727\n",
      "Epoch [619/1000], Loss: 0.0678\n",
      "Epoch [619/1000], Loss: 0.0798\n",
      "Epoch [619/1000], Loss: 0.0690\n",
      "tensor(2.4399, grad_fn=<MeanBackward0>)\n",
      "619\n",
      "Epoch [620/1000], Loss: 0.0671\n",
      "Epoch [620/1000], Loss: 0.0535\n",
      "Epoch [620/1000], Loss: 0.0560\n",
      "Epoch [620/1000], Loss: 0.0569\n",
      "Epoch [620/1000], Loss: 0.0657\n",
      "Epoch [620/1000], Loss: 0.0662\n",
      "Epoch [620/1000], Loss: 0.0850\n",
      "Epoch [620/1000], Loss: 0.0774\n",
      "Epoch [620/1000], Loss: 0.0927\n",
      "Epoch [620/1000], Loss: 0.0686\n",
      "Epoch [620/1000], Loss: 0.0422\n",
      "tensor(2.4733, grad_fn=<MeanBackward0>)\n",
      "620\n",
      "Epoch [621/1000], Loss: 0.0561\n",
      "Epoch [621/1000], Loss: 0.0654\n",
      "Epoch [621/1000], Loss: 0.0651\n",
      "Epoch [621/1000], Loss: 0.0794\n",
      "Epoch [621/1000], Loss: 0.0703\n",
      "Epoch [621/1000], Loss: 0.0716\n",
      "Epoch [621/1000], Loss: 0.0628\n",
      "Epoch [621/1000], Loss: 0.0708\n",
      "Epoch [621/1000], Loss: 0.1146\n",
      "Epoch [621/1000], Loss: 0.1059\n",
      "Epoch [621/1000], Loss: 0.0910\n",
      "tensor(2.5252, grad_fn=<MeanBackward0>)\n",
      "621\n",
      "Epoch [622/1000], Loss: 0.0872\n",
      "Epoch [622/1000], Loss: 0.1518\n",
      "Epoch [622/1000], Loss: 0.2140\n",
      "Epoch [622/1000], Loss: 0.1947\n",
      "Epoch [622/1000], Loss: 0.1209\n",
      "Epoch [622/1000], Loss: 0.0614\n",
      "Epoch [622/1000], Loss: 0.1191\n",
      "Epoch [622/1000], Loss: 0.2022\n",
      "Epoch [622/1000], Loss: 0.2529\n",
      "Epoch [622/1000], Loss: 0.2294\n",
      "Epoch [622/1000], Loss: 0.1822\n",
      "tensor(2.4560, grad_fn=<MeanBackward0>)\n",
      "622\n",
      "Epoch [623/1000], Loss: 0.0709\n",
      "Epoch [623/1000], Loss: 0.1661\n",
      "Epoch [623/1000], Loss: 0.2640\n",
      "Epoch [623/1000], Loss: 0.3273\n",
      "Epoch [623/1000], Loss: 0.3316\n",
      "Epoch [623/1000], Loss: 0.2705\n",
      "Epoch [623/1000], Loss: 0.1723\n",
      "Epoch [623/1000], Loss: 0.1426\n",
      "Epoch [623/1000], Loss: 0.3350\n",
      "Epoch [623/1000], Loss: 0.4123\n",
      "Epoch [623/1000], Loss: 0.5241\n",
      "tensor(2.0958, grad_fn=<MeanBackward0>)\n",
      "623\n",
      "Epoch [624/1000], Loss: 0.5863\n",
      "Epoch [624/1000], Loss: 0.5153\n",
      "Epoch [624/1000], Loss: 0.3152\n",
      "Epoch [624/1000], Loss: 0.1059\n",
      "Epoch [624/1000], Loss: 0.2846\n",
      "Epoch [624/1000], Loss: 0.5064\n",
      "Epoch [624/1000], Loss: 0.5961\n",
      "Epoch [624/1000], Loss: 0.5950\n",
      "Epoch [624/1000], Loss: 0.4968\n",
      "Epoch [624/1000], Loss: 0.2910\n",
      "Epoch [624/1000], Loss: 0.1710\n",
      "tensor(2.1836, grad_fn=<MeanBackward0>)\n",
      "624\n",
      "Epoch [625/1000], Loss: 0.3392\n",
      "Epoch [625/1000], Loss: 0.4261\n",
      "Epoch [625/1000], Loss: 0.4707\n",
      "Epoch [625/1000], Loss: 0.4261\n",
      "Epoch [625/1000], Loss: 0.2870\n",
      "Epoch [625/1000], Loss: 0.1840\n",
      "Epoch [625/1000], Loss: 0.1717\n",
      "Epoch [625/1000], Loss: 0.2334\n",
      "Epoch [625/1000], Loss: 0.3002\n",
      "Epoch [625/1000], Loss: 0.2454\n",
      "Epoch [625/1000], Loss: 0.0974\n",
      "tensor(2.3405, grad_fn=<MeanBackward0>)\n",
      "625\n",
      "Epoch [626/1000], Loss: 0.1337\n",
      "Epoch [626/1000], Loss: 0.1806\n",
      "Epoch [626/1000], Loss: 0.1305\n",
      "Epoch [626/1000], Loss: 0.1059\n",
      "Epoch [626/1000], Loss: 0.1140\n",
      "Epoch [626/1000], Loss: 0.1588\n",
      "Epoch [626/1000], Loss: 0.1175\n",
      "Epoch [626/1000], Loss: 0.0957\n",
      "Epoch [626/1000], Loss: 0.0941\n",
      "Epoch [626/1000], Loss: 0.1675\n",
      "Epoch [626/1000], Loss: 0.1484\n",
      "tensor(2.3705, grad_fn=<MeanBackward0>)\n",
      "626\n",
      "Epoch [627/1000], Loss: 0.0851\n",
      "Epoch [627/1000], Loss: 0.1268\n",
      "Epoch [627/1000], Loss: 0.1645\n",
      "Epoch [627/1000], Loss: 0.1645\n",
      "Epoch [627/1000], Loss: 0.0962\n",
      "Epoch [627/1000], Loss: 0.1103\n",
      "Epoch [627/1000], Loss: 0.1042\n",
      "Epoch [627/1000], Loss: 0.0787\n",
      "Epoch [627/1000], Loss: 0.0781\n",
      "Epoch [627/1000], Loss: 0.0875\n",
      "Epoch [627/1000], Loss: 0.0765\n",
      "tensor(2.3872, grad_fn=<MeanBackward0>)\n",
      "627\n",
      "Epoch [628/1000], Loss: 0.0671\n",
      "Epoch [628/1000], Loss: 0.0646\n",
      "Epoch [628/1000], Loss: 0.0963\n",
      "Epoch [628/1000], Loss: 0.0822\n",
      "Epoch [628/1000], Loss: 0.0731\n",
      "Epoch [628/1000], Loss: 0.0791\n",
      "Epoch [628/1000], Loss: 0.0948\n",
      "Epoch [628/1000], Loss: 0.0662\n",
      "Epoch [628/1000], Loss: 0.0575\n",
      "Epoch [628/1000], Loss: 0.0528\n",
      "Epoch [628/1000], Loss: 0.0478\n",
      "tensor(2.4302, grad_fn=<MeanBackward0>)\n",
      "628\n",
      "Epoch [629/1000], Loss: 0.0536\n",
      "Epoch [629/1000], Loss: 0.0598\n",
      "Epoch [629/1000], Loss: 0.0950\n",
      "Epoch [629/1000], Loss: 0.1104\n",
      "Epoch [629/1000], Loss: 0.0691\n",
      "Epoch [629/1000], Loss: 0.0595\n",
      "Epoch [629/1000], Loss: 0.0777\n",
      "Epoch [629/1000], Loss: 0.0886\n",
      "Epoch [629/1000], Loss: 0.0879\n",
      "Epoch [629/1000], Loss: 0.0544\n",
      "Epoch [629/1000], Loss: 0.0551\n",
      "tensor(2.4298, grad_fn=<MeanBackward0>)\n",
      "629\n",
      "Epoch [630/1000], Loss: 0.0618\n",
      "Epoch [630/1000], Loss: 0.0712\n",
      "Epoch [630/1000], Loss: 0.0899\n",
      "Epoch [630/1000], Loss: 0.0971\n",
      "Epoch [630/1000], Loss: 0.1132\n",
      "Epoch [630/1000], Loss: 0.1001\n",
      "Epoch [630/1000], Loss: 0.0622\n",
      "Epoch [630/1000], Loss: 0.1112\n",
      "Epoch [630/1000], Loss: 0.1709\n",
      "Epoch [630/1000], Loss: 0.1641\n",
      "Epoch [630/1000], Loss: 0.1511\n",
      "tensor(2.3697, grad_fn=<MeanBackward0>)\n",
      "630\n",
      "Epoch [631/1000], Loss: 0.0986\n",
      "Epoch [631/1000], Loss: 0.1065\n",
      "Epoch [631/1000], Loss: 0.1105\n",
      "Epoch [631/1000], Loss: 0.1342\n",
      "Epoch [631/1000], Loss: 0.1267\n",
      "Epoch [631/1000], Loss: 0.0986\n",
      "Epoch [631/1000], Loss: 0.0773\n",
      "Epoch [631/1000], Loss: 0.0889\n",
      "Epoch [631/1000], Loss: 0.1733\n",
      "Epoch [631/1000], Loss: 0.1860\n",
      "Epoch [631/1000], Loss: 0.1727\n",
      "tensor(2.3405, grad_fn=<MeanBackward0>)\n",
      "631\n",
      "Epoch [632/1000], Loss: 0.1332\n",
      "Epoch [632/1000], Loss: 0.0693\n",
      "Epoch [632/1000], Loss: 0.1520\n",
      "Epoch [632/1000], Loss: 0.2681\n",
      "Epoch [632/1000], Loss: 0.3090\n",
      "Epoch [632/1000], Loss: 0.3240\n",
      "Epoch [632/1000], Loss: 0.2524\n",
      "Epoch [632/1000], Loss: 0.1813\n",
      "Epoch [632/1000], Loss: 0.1589\n",
      "Epoch [632/1000], Loss: 0.2458\n",
      "Epoch [632/1000], Loss: 0.4079\n",
      "tensor(2.0631, grad_fn=<MeanBackward0>)\n",
      "632\n",
      "Epoch [633/1000], Loss: 0.4980\n",
      "Epoch [633/1000], Loss: 0.5099\n",
      "Epoch [633/1000], Loss: 0.4729\n",
      "Epoch [633/1000], Loss: 0.2989\n",
      "Epoch [633/1000], Loss: 0.1551\n",
      "Epoch [633/1000], Loss: 0.2890\n",
      "Epoch [633/1000], Loss: 0.4995\n",
      "Epoch [633/1000], Loss: 0.6020\n",
      "Epoch [633/1000], Loss: 0.6334\n",
      "Epoch [633/1000], Loss: 0.5368\n",
      "Epoch [633/1000], Loss: 0.2481\n",
      "tensor(2.2969, grad_fn=<MeanBackward0>)\n",
      "633\n",
      "Epoch [634/1000], Loss: 0.1917\n",
      "Epoch [634/1000], Loss: 0.3167\n",
      "Epoch [634/1000], Loss: 0.4605\n",
      "Epoch [634/1000], Loss: 0.5178\n",
      "Epoch [634/1000], Loss: 0.4342\n",
      "Epoch [634/1000], Loss: 0.3251\n",
      "Epoch [634/1000], Loss: 0.1496\n",
      "Epoch [634/1000], Loss: 0.1152\n",
      "Epoch [634/1000], Loss: 0.2243\n",
      "Epoch [634/1000], Loss: 0.2644\n",
      "Epoch [634/1000], Loss: 0.1919\n",
      "tensor(2.4339, grad_fn=<MeanBackward0>)\n",
      "634\n",
      "Epoch [635/1000], Loss: 0.1300\n",
      "Epoch [635/1000], Loss: 0.1383\n",
      "Epoch [635/1000], Loss: 0.1591\n",
      "Epoch [635/1000], Loss: 0.1786\n",
      "Epoch [635/1000], Loss: 0.1296\n",
      "Epoch [635/1000], Loss: 0.1063\n",
      "Epoch [635/1000], Loss: 0.1077\n",
      "Epoch [635/1000], Loss: 0.1244\n",
      "Epoch [635/1000], Loss: 0.0945\n",
      "Epoch [635/1000], Loss: 0.1019\n",
      "Epoch [635/1000], Loss: 0.1323\n",
      "tensor(2.2892, grad_fn=<MeanBackward0>)\n",
      "635\n",
      "Epoch [636/1000], Loss: 0.1055\n",
      "Epoch [636/1000], Loss: 0.0807\n",
      "Epoch [636/1000], Loss: 0.0802\n",
      "Epoch [636/1000], Loss: 0.0960\n",
      "Epoch [636/1000], Loss: 0.0971\n",
      "Epoch [636/1000], Loss: 0.1177\n",
      "Epoch [636/1000], Loss: 0.1151\n",
      "Epoch [636/1000], Loss: 0.0981\n",
      "Epoch [636/1000], Loss: 0.0905\n",
      "Epoch [636/1000], Loss: 0.0953\n",
      "Epoch [636/1000], Loss: 0.1040\n",
      "tensor(2.2899, grad_fn=<MeanBackward0>)\n",
      "636\n",
      "Epoch [637/1000], Loss: 0.1009\n",
      "Epoch [637/1000], Loss: 0.0733\n",
      "Epoch [637/1000], Loss: 0.0578\n",
      "Epoch [637/1000], Loss: 0.0914\n",
      "Epoch [637/1000], Loss: 0.1057\n",
      "Epoch [637/1000], Loss: 0.1092\n",
      "Epoch [637/1000], Loss: 0.0701\n",
      "Epoch [637/1000], Loss: 0.0580\n",
      "Epoch [637/1000], Loss: 0.0738\n",
      "Epoch [637/1000], Loss: 0.0707\n",
      "Epoch [637/1000], Loss: 0.0465\n",
      "tensor(2.3795, grad_fn=<MeanBackward0>)\n",
      "637\n",
      "Epoch [638/1000], Loss: 0.0494\n",
      "Epoch [638/1000], Loss: 0.0558\n",
      "Epoch [638/1000], Loss: 0.0655\n",
      "Epoch [638/1000], Loss: 0.0568\n",
      "Epoch [638/1000], Loss: 0.0558\n",
      "Epoch [638/1000], Loss: 0.0678\n",
      "Epoch [638/1000], Loss: 0.0705\n",
      "Epoch [638/1000], Loss: 0.0839\n",
      "Epoch [638/1000], Loss: 0.0831\n",
      "Epoch [638/1000], Loss: 0.0880\n",
      "Epoch [638/1000], Loss: 0.1220\n",
      "tensor(2.2976, grad_fn=<MeanBackward0>)\n",
      "638\n",
      "Epoch [639/1000], Loss: 0.0905\n",
      "Epoch [639/1000], Loss: 0.0618\n",
      "Epoch [639/1000], Loss: 0.0917\n",
      "Epoch [639/1000], Loss: 0.0919\n",
      "Epoch [639/1000], Loss: 0.1188\n",
      "Epoch [639/1000], Loss: 0.1143\n",
      "Epoch [639/1000], Loss: 0.1138\n",
      "Epoch [639/1000], Loss: 0.0691\n",
      "Epoch [639/1000], Loss: 0.0827\n",
      "Epoch [639/1000], Loss: 0.0957\n",
      "Epoch [639/1000], Loss: 0.1648\n",
      "tensor(2.2131, grad_fn=<MeanBackward0>)\n",
      "639\n",
      "Epoch [640/1000], Loss: 0.1615\n",
      "Epoch [640/1000], Loss: 0.1186\n",
      "Epoch [640/1000], Loss: 0.0822\n",
      "Epoch [640/1000], Loss: 0.1143\n",
      "Epoch [640/1000], Loss: 0.1806\n",
      "Epoch [640/1000], Loss: 0.2046\n",
      "Epoch [640/1000], Loss: 0.1980\n",
      "Epoch [640/1000], Loss: 0.1641\n",
      "Epoch [640/1000], Loss: 0.0993\n",
      "Epoch [640/1000], Loss: 0.1048\n",
      "Epoch [640/1000], Loss: 0.2014\n",
      "tensor(2.1212, grad_fn=<MeanBackward0>)\n",
      "640\n",
      "Epoch [641/1000], Loss: 0.3142\n",
      "Epoch [641/1000], Loss: 0.3382\n",
      "Epoch [641/1000], Loss: 0.3318\n",
      "Epoch [641/1000], Loss: 0.2372\n",
      "Epoch [641/1000], Loss: 0.0908\n",
      "Epoch [641/1000], Loss: 0.2269\n",
      "Epoch [641/1000], Loss: 0.3782\n",
      "Epoch [641/1000], Loss: 0.4879\n",
      "Epoch [641/1000], Loss: 0.5650\n",
      "Epoch [641/1000], Loss: 0.4949\n",
      "Epoch [641/1000], Loss: 0.2419\n",
      "tensor(2.2556, grad_fn=<MeanBackward0>)\n",
      "641\n",
      "Epoch [642/1000], Loss: 0.1340\n",
      "Epoch [642/1000], Loss: 0.2796\n",
      "Epoch [642/1000], Loss: 0.4332\n",
      "Epoch [642/1000], Loss: 0.5459\n",
      "Epoch [642/1000], Loss: 0.5072\n",
      "Epoch [642/1000], Loss: 0.4724\n",
      "Epoch [642/1000], Loss: 0.3153\n",
      "Epoch [642/1000], Loss: 0.2065\n",
      "Epoch [642/1000], Loss: 0.2012\n",
      "Epoch [642/1000], Loss: 0.3258\n",
      "Epoch [642/1000], Loss: 0.3853\n",
      "tensor(2.6052, grad_fn=<MeanBackward0>)\n",
      "642\n",
      "Epoch [643/1000], Loss: 0.3910\n",
      "Epoch [643/1000], Loss: 0.3154\n",
      "Epoch [643/1000], Loss: 0.2002\n",
      "Epoch [643/1000], Loss: 0.1583\n",
      "Epoch [643/1000], Loss: 0.2183\n",
      "Epoch [643/1000], Loss: 0.2751\n",
      "Epoch [643/1000], Loss: 0.2695\n",
      "Epoch [643/1000], Loss: 0.2472\n",
      "Epoch [643/1000], Loss: 0.1445\n",
      "Epoch [643/1000], Loss: 0.1558\n",
      "Epoch [643/1000], Loss: 0.1857\n",
      "tensor(2.5143, grad_fn=<MeanBackward0>)\n",
      "643\n",
      "Epoch [644/1000], Loss: 0.2300\n",
      "Epoch [644/1000], Loss: 0.1878\n",
      "Epoch [644/1000], Loss: 0.1076\n",
      "Epoch [644/1000], Loss: 0.1335\n",
      "Epoch [644/1000], Loss: 0.1366\n",
      "Epoch [644/1000], Loss: 0.1401\n",
      "Epoch [644/1000], Loss: 0.1192\n",
      "Epoch [644/1000], Loss: 0.0837\n",
      "Epoch [644/1000], Loss: 0.1061\n",
      "Epoch [644/1000], Loss: 0.0931\n",
      "Epoch [644/1000], Loss: 0.0645\n",
      "tensor(2.3161, grad_fn=<MeanBackward0>)\n",
      "644\n",
      "Epoch [645/1000], Loss: 0.0643\n",
      "Epoch [645/1000], Loss: 0.0905\n",
      "Epoch [645/1000], Loss: 0.0901\n",
      "Epoch [645/1000], Loss: 0.0566\n",
      "Epoch [645/1000], Loss: 0.0830\n",
      "Epoch [645/1000], Loss: 0.1011\n",
      "Epoch [645/1000], Loss: 0.0890\n",
      "Epoch [645/1000], Loss: 0.0638\n",
      "Epoch [645/1000], Loss: 0.0633\n",
      "Epoch [645/1000], Loss: 0.0646\n",
      "Epoch [645/1000], Loss: 0.0787\n",
      "tensor(2.2795, grad_fn=<MeanBackward0>)\n",
      "645\n",
      "Epoch [646/1000], Loss: 0.0686\n",
      "Epoch [646/1000], Loss: 0.0614\n",
      "Epoch [646/1000], Loss: 0.0715\n",
      "Epoch [646/1000], Loss: 0.0718\n",
      "Epoch [646/1000], Loss: 0.0765\n",
      "Epoch [646/1000], Loss: 0.0700\n",
      "Epoch [646/1000], Loss: 0.0687\n",
      "Epoch [646/1000], Loss: 0.0825\n",
      "Epoch [646/1000], Loss: 0.0662\n",
      "Epoch [646/1000], Loss: 0.0554\n",
      "Epoch [646/1000], Loss: 0.1177\n",
      "tensor(2.2824, grad_fn=<MeanBackward0>)\n",
      "646\n",
      "Epoch [647/1000], Loss: 0.1159\n",
      "Epoch [647/1000], Loss: 0.0845\n",
      "Epoch [647/1000], Loss: 0.0670\n",
      "Epoch [647/1000], Loss: 0.0825\n",
      "Epoch [647/1000], Loss: 0.0858\n",
      "Epoch [647/1000], Loss: 0.0704\n",
      "Epoch [647/1000], Loss: 0.0486\n",
      "Epoch [647/1000], Loss: 0.0672\n",
      "Epoch [647/1000], Loss: 0.0780\n",
      "Epoch [647/1000], Loss: 0.0482\n",
      "Epoch [647/1000], Loss: 0.0522\n",
      "tensor(2.2678, grad_fn=<MeanBackward0>)\n",
      "647\n",
      "Epoch [648/1000], Loss: 0.0725\n",
      "Epoch [648/1000], Loss: 0.0800\n",
      "Epoch [648/1000], Loss: 0.0540\n",
      "Epoch [648/1000], Loss: 0.0572\n",
      "Epoch [648/1000], Loss: 0.1062\n",
      "Epoch [648/1000], Loss: 0.1171\n",
      "Epoch [648/1000], Loss: 0.0776\n",
      "Epoch [648/1000], Loss: 0.0615\n",
      "Epoch [648/1000], Loss: 0.0706\n",
      "Epoch [648/1000], Loss: 0.0644\n",
      "Epoch [648/1000], Loss: 0.0778\n",
      "tensor(2.2467, grad_fn=<MeanBackward0>)\n",
      "648\n",
      "Epoch [649/1000], Loss: 0.0758\n",
      "Epoch [649/1000], Loss: 0.0783\n",
      "Epoch [649/1000], Loss: 0.0740\n",
      "Epoch [649/1000], Loss: 0.0530\n",
      "Epoch [649/1000], Loss: 0.0724\n",
      "Epoch [649/1000], Loss: 0.1404\n",
      "Epoch [649/1000], Loss: 0.1545\n",
      "Epoch [649/1000], Loss: 0.1424\n",
      "Epoch [649/1000], Loss: 0.0824\n",
      "Epoch [649/1000], Loss: 0.0761\n",
      "Epoch [649/1000], Loss: 0.1435\n",
      "tensor(2.1508, grad_fn=<MeanBackward0>)\n",
      "649\n",
      "Epoch [650/1000], Loss: 0.1686\n",
      "Epoch [650/1000], Loss: 0.1478\n",
      "Epoch [650/1000], Loss: 0.1424\n",
      "Epoch [650/1000], Loss: 0.0940\n",
      "Epoch [650/1000], Loss: 0.0842\n",
      "Epoch [650/1000], Loss: 0.1768\n",
      "Epoch [650/1000], Loss: 0.2606\n",
      "Epoch [650/1000], Loss: 0.3214\n",
      "Epoch [650/1000], Loss: 0.3240\n",
      "Epoch [650/1000], Loss: 0.2762\n",
      "Epoch [650/1000], Loss: 0.1267\n",
      "tensor(2.1395, grad_fn=<MeanBackward0>)\n",
      "650\n",
      "Epoch [651/1000], Loss: 0.1978\n",
      "Epoch [651/1000], Loss: 0.3735\n",
      "Epoch [651/1000], Loss: 0.5031\n",
      "Epoch [651/1000], Loss: 0.6200\n",
      "Epoch [651/1000], Loss: 0.5624\n",
      "Epoch [651/1000], Loss: 0.4662\n",
      "Epoch [651/1000], Loss: 0.2555\n",
      "Epoch [651/1000], Loss: 0.1749\n",
      "Epoch [651/1000], Loss: 0.2839\n",
      "Epoch [651/1000], Loss: 0.4207\n",
      "Epoch [651/1000], Loss: 0.4791\n",
      "tensor(2.6021, grad_fn=<MeanBackward0>)\n",
      "651\n",
      "Epoch [652/1000], Loss: 0.4382\n",
      "Epoch [652/1000], Loss: 0.3247\n",
      "Epoch [652/1000], Loss: 0.1832\n",
      "Epoch [652/1000], Loss: 0.2552\n",
      "Epoch [652/1000], Loss: 0.3484\n",
      "Epoch [652/1000], Loss: 0.3987\n",
      "Epoch [652/1000], Loss: 0.3604\n",
      "Epoch [652/1000], Loss: 0.3234\n",
      "Epoch [652/1000], Loss: 0.2455\n",
      "Epoch [652/1000], Loss: 0.1915\n",
      "Epoch [652/1000], Loss: 0.2016\n",
      "tensor(2.5172, grad_fn=<MeanBackward0>)\n",
      "652\n",
      "Epoch [653/1000], Loss: 0.3221\n",
      "Epoch [653/1000], Loss: 0.3135\n",
      "Epoch [653/1000], Loss: 0.2167\n",
      "Epoch [653/1000], Loss: 0.1641\n",
      "Epoch [653/1000], Loss: 0.1966\n",
      "Epoch [653/1000], Loss: 0.2401\n",
      "Epoch [653/1000], Loss: 0.2388\n",
      "Epoch [653/1000], Loss: 0.1925\n",
      "Epoch [653/1000], Loss: 0.1093\n",
      "Epoch [653/1000], Loss: 0.1453\n",
      "Epoch [653/1000], Loss: 0.2090\n",
      "tensor(2.4945, grad_fn=<MeanBackward0>)\n",
      "653\n",
      "Epoch [654/1000], Loss: 0.2473\n",
      "Epoch [654/1000], Loss: 0.1687\n",
      "Epoch [654/1000], Loss: 0.1146\n",
      "Epoch [654/1000], Loss: 0.1947\n",
      "Epoch [654/1000], Loss: 0.2422\n",
      "Epoch [654/1000], Loss: 0.2336\n",
      "Epoch [654/1000], Loss: 0.1528\n",
      "Epoch [654/1000], Loss: 0.0818\n",
      "Epoch [654/1000], Loss: 0.0990\n",
      "Epoch [654/1000], Loss: 0.1597\n",
      "Epoch [654/1000], Loss: 0.1315\n",
      "tensor(2.3639, grad_fn=<MeanBackward0>)\n",
      "654\n",
      "Epoch [655/1000], Loss: 0.1075\n",
      "Epoch [655/1000], Loss: 0.1194\n",
      "Epoch [655/1000], Loss: 0.1762\n",
      "Epoch [655/1000], Loss: 0.1872\n",
      "Epoch [655/1000], Loss: 0.1589\n",
      "Epoch [655/1000], Loss: 0.0944\n",
      "Epoch [655/1000], Loss: 0.1051\n",
      "Epoch [655/1000], Loss: 0.1472\n",
      "Epoch [655/1000], Loss: 0.1085\n",
      "Epoch [655/1000], Loss: 0.0791\n",
      "Epoch [655/1000], Loss: 0.0911\n",
      "tensor(2.3142, grad_fn=<MeanBackward0>)\n",
      "655\n",
      "Epoch [656/1000], Loss: 0.0876\n",
      "Epoch [656/1000], Loss: 0.0816\n",
      "Epoch [656/1000], Loss: 0.0917\n",
      "Epoch [656/1000], Loss: 0.0868\n",
      "Epoch [656/1000], Loss: 0.0775\n",
      "Epoch [656/1000], Loss: 0.0754\n",
      "Epoch [656/1000], Loss: 0.0889\n",
      "Epoch [656/1000], Loss: 0.1186\n",
      "Epoch [656/1000], Loss: 0.0966\n",
      "Epoch [656/1000], Loss: 0.1094\n",
      "Epoch [656/1000], Loss: 0.0839\n",
      "tensor(2.3030, grad_fn=<MeanBackward0>)\n",
      "656\n",
      "Epoch [657/1000], Loss: 0.0871\n",
      "Epoch [657/1000], Loss: 0.1045\n",
      "Epoch [657/1000], Loss: 0.0974\n",
      "Epoch [657/1000], Loss: 0.0956\n",
      "Epoch [657/1000], Loss: 0.0828\n",
      "Epoch [657/1000], Loss: 0.0831\n",
      "Epoch [657/1000], Loss: 0.1012\n",
      "Epoch [657/1000], Loss: 0.1441\n",
      "Epoch [657/1000], Loss: 0.1512\n",
      "Epoch [657/1000], Loss: 0.1455\n",
      "Epoch [657/1000], Loss: 0.0762\n",
      "tensor(2.2662, grad_fn=<MeanBackward0>)\n",
      "657\n",
      "Epoch [658/1000], Loss: 0.0780\n",
      "Epoch [658/1000], Loss: 0.1257\n",
      "Epoch [658/1000], Loss: 0.1657\n",
      "Epoch [658/1000], Loss: 0.1954\n",
      "Epoch [658/1000], Loss: 0.1602\n",
      "Epoch [658/1000], Loss: 0.1272\n",
      "Epoch [658/1000], Loss: 0.1247\n",
      "Epoch [658/1000], Loss: 0.1786\n",
      "Epoch [658/1000], Loss: 0.1927\n",
      "Epoch [658/1000], Loss: 0.1738\n",
      "Epoch [658/1000], Loss: 0.0973\n",
      "tensor(2.2636, grad_fn=<MeanBackward0>)\n",
      "658\n",
      "Epoch [659/1000], Loss: 0.0641\n",
      "Epoch [659/1000], Loss: 0.1550\n",
      "Epoch [659/1000], Loss: 0.2578\n",
      "Epoch [659/1000], Loss: 0.3157\n",
      "Epoch [659/1000], Loss: 0.3140\n",
      "Epoch [659/1000], Loss: 0.2554\n",
      "Epoch [659/1000], Loss: 0.1475\n",
      "Epoch [659/1000], Loss: 0.0918\n",
      "Epoch [659/1000], Loss: 0.2535\n",
      "Epoch [659/1000], Loss: 0.3711\n",
      "Epoch [659/1000], Loss: 0.3684\n",
      "tensor(2.5609, grad_fn=<MeanBackward0>)\n",
      "659\n",
      "Epoch [660/1000], Loss: 0.3439\n",
      "Epoch [660/1000], Loss: 0.2483\n",
      "Epoch [660/1000], Loss: 0.1494\n",
      "Epoch [660/1000], Loss: 0.2722\n",
      "Epoch [660/1000], Loss: 0.3446\n",
      "Epoch [660/1000], Loss: 0.4411\n",
      "Epoch [660/1000], Loss: 0.4332\n",
      "Epoch [660/1000], Loss: 0.4756\n",
      "Epoch [660/1000], Loss: 0.4161\n",
      "Epoch [660/1000], Loss: 0.2456\n",
      "Epoch [660/1000], Loss: 0.1589\n",
      "tensor(2.5581, grad_fn=<MeanBackward0>)\n",
      "660\n",
      "Epoch [661/1000], Loss: 0.3124\n",
      "Epoch [661/1000], Loss: 0.4604\n",
      "Epoch [661/1000], Loss: 0.4867\n",
      "Epoch [661/1000], Loss: 0.3776\n",
      "Epoch [661/1000], Loss: 0.2137\n",
      "Epoch [661/1000], Loss: 0.1732\n",
      "Epoch [661/1000], Loss: 0.2860\n",
      "Epoch [661/1000], Loss: 0.4186\n",
      "Epoch [661/1000], Loss: 0.3994\n",
      "Epoch [661/1000], Loss: 0.2656\n",
      "Epoch [661/1000], Loss: 0.1072\n",
      "tensor(2.5069, grad_fn=<MeanBackward0>)\n",
      "661\n",
      "Epoch [662/1000], Loss: 0.2071\n",
      "Epoch [662/1000], Loss: 0.3470\n",
      "Epoch [662/1000], Loss: 0.3312\n",
      "Epoch [662/1000], Loss: 0.2938\n",
      "Epoch [662/1000], Loss: 0.1672\n",
      "Epoch [662/1000], Loss: 0.1653\n",
      "Epoch [662/1000], Loss: 0.2302\n",
      "Epoch [662/1000], Loss: 0.2609\n",
      "Epoch [662/1000], Loss: 0.2446\n",
      "Epoch [662/1000], Loss: 0.1315\n",
      "Epoch [662/1000], Loss: 0.0735\n",
      "tensor(2.4533, grad_fn=<MeanBackward0>)\n",
      "662\n",
      "Epoch [663/1000], Loss: 0.1463\n",
      "Epoch [663/1000], Loss: 0.1553\n",
      "Epoch [663/1000], Loss: 0.1026\n",
      "Epoch [663/1000], Loss: 0.1027\n",
      "Epoch [663/1000], Loss: 0.1475\n",
      "Epoch [663/1000], Loss: 0.1243\n",
      "Epoch [663/1000], Loss: 0.0839\n",
      "Epoch [663/1000], Loss: 0.0814\n",
      "Epoch [663/1000], Loss: 0.0828\n",
      "Epoch [663/1000], Loss: 0.0993\n",
      "Epoch [663/1000], Loss: 0.0698\n",
      "tensor(2.4079, grad_fn=<MeanBackward0>)\n",
      "663\n",
      "Epoch [664/1000], Loss: 0.0732\n",
      "Epoch [664/1000], Loss: 0.0756\n",
      "Epoch [664/1000], Loss: 0.0720\n",
      "Epoch [664/1000], Loss: 0.0690\n",
      "Epoch [664/1000], Loss: 0.0599\n",
      "Epoch [664/1000], Loss: 0.0782\n",
      "Epoch [664/1000], Loss: 0.0666\n",
      "Epoch [664/1000], Loss: 0.0639\n",
      "Epoch [664/1000], Loss: 0.0593\n",
      "Epoch [664/1000], Loss: 0.0631\n",
      "Epoch [664/1000], Loss: 0.0538\n",
      "tensor(2.4165, grad_fn=<MeanBackward0>)\n",
      "664\n",
      "Epoch [665/1000], Loss: 0.0577\n",
      "Epoch [665/1000], Loss: 0.0505\n",
      "Epoch [665/1000], Loss: 0.0639\n",
      "Epoch [665/1000], Loss: 0.0743\n",
      "Epoch [665/1000], Loss: 0.0604\n",
      "Epoch [665/1000], Loss: 0.0502\n",
      "Epoch [665/1000], Loss: 0.0633\n",
      "Epoch [665/1000], Loss: 0.0531\n",
      "Epoch [665/1000], Loss: 0.0499\n",
      "Epoch [665/1000], Loss: 0.0600\n",
      "Epoch [665/1000], Loss: 0.0559\n",
      "tensor(2.4431, grad_fn=<MeanBackward0>)\n",
      "665\n",
      "Epoch [666/1000], Loss: 0.0685\n",
      "Epoch [666/1000], Loss: 0.0769\n",
      "Epoch [666/1000], Loss: 0.0509\n",
      "Epoch [666/1000], Loss: 0.0704\n",
      "Epoch [666/1000], Loss: 0.0987\n",
      "Epoch [666/1000], Loss: 0.0910\n",
      "Epoch [666/1000], Loss: 0.0734\n",
      "Epoch [666/1000], Loss: 0.0590\n",
      "Epoch [666/1000], Loss: 0.0791\n",
      "Epoch [666/1000], Loss: 0.0705\n",
      "Epoch [666/1000], Loss: 0.0743\n",
      "tensor(2.3837, grad_fn=<MeanBackward0>)\n",
      "666\n",
      "Epoch [667/1000], Loss: 0.0656\n",
      "Epoch [667/1000], Loss: 0.0629\n",
      "Epoch [667/1000], Loss: 0.0541\n",
      "Epoch [667/1000], Loss: 0.0550\n",
      "Epoch [667/1000], Loss: 0.0780\n",
      "Epoch [667/1000], Loss: 0.0853\n",
      "Epoch [667/1000], Loss: 0.0687\n",
      "Epoch [667/1000], Loss: 0.0762\n",
      "Epoch [667/1000], Loss: 0.0655\n",
      "Epoch [667/1000], Loss: 0.0653\n",
      "Epoch [667/1000], Loss: 0.0506\n",
      "tensor(2.3632, grad_fn=<MeanBackward0>)\n",
      "667\n",
      "Epoch [668/1000], Loss: 0.0600\n",
      "Epoch [668/1000], Loss: 0.0634\n",
      "Epoch [668/1000], Loss: 0.0707\n",
      "Epoch [668/1000], Loss: 0.0655\n",
      "Epoch [668/1000], Loss: 0.0678\n",
      "Epoch [668/1000], Loss: 0.0757\n",
      "Epoch [668/1000], Loss: 0.0721\n",
      "Epoch [668/1000], Loss: 0.0805\n",
      "Epoch [668/1000], Loss: 0.0953\n",
      "Epoch [668/1000], Loss: 0.0767\n",
      "Epoch [668/1000], Loss: 0.0783\n",
      "tensor(2.3985, grad_fn=<MeanBackward0>)\n",
      "668\n",
      "Epoch [669/1000], Loss: 0.0417\n",
      "Epoch [669/1000], Loss: 0.0599\n",
      "Epoch [669/1000], Loss: 0.0699\n",
      "Epoch [669/1000], Loss: 0.0772\n",
      "Epoch [669/1000], Loss: 0.0674\n",
      "Epoch [669/1000], Loss: 0.0466\n",
      "Epoch [669/1000], Loss: 0.0474\n",
      "Epoch [669/1000], Loss: 0.0510\n",
      "Epoch [669/1000], Loss: 0.0998\n",
      "Epoch [669/1000], Loss: 0.0927\n",
      "Epoch [669/1000], Loss: 0.1020\n",
      "tensor(2.4527, grad_fn=<MeanBackward0>)\n",
      "669\n",
      "Epoch [670/1000], Loss: 0.0834\n",
      "Epoch [670/1000], Loss: 0.0555\n",
      "Epoch [670/1000], Loss: 0.0509\n",
      "Epoch [670/1000], Loss: 0.0893\n",
      "Epoch [670/1000], Loss: 0.0964\n",
      "Epoch [670/1000], Loss: 0.0958\n",
      "Epoch [670/1000], Loss: 0.0698\n",
      "Epoch [670/1000], Loss: 0.0637\n",
      "Epoch [670/1000], Loss: 0.0649\n",
      "Epoch [670/1000], Loss: 0.0636\n",
      "Epoch [670/1000], Loss: 0.0693\n",
      "tensor(2.4596, grad_fn=<MeanBackward0>)\n",
      "670\n",
      "Epoch [671/1000], Loss: 0.0771\n",
      "Epoch [671/1000], Loss: 0.0842\n",
      "Epoch [671/1000], Loss: 0.0896\n",
      "Epoch [671/1000], Loss: 0.0732\n",
      "Epoch [671/1000], Loss: 0.0655\n",
      "Epoch [671/1000], Loss: 0.0567\n",
      "Epoch [671/1000], Loss: 0.0534\n",
      "Epoch [671/1000], Loss: 0.0604\n",
      "Epoch [671/1000], Loss: 0.0829\n",
      "Epoch [671/1000], Loss: 0.0589\n",
      "Epoch [671/1000], Loss: 0.0519\n",
      "tensor(2.4607, grad_fn=<MeanBackward0>)\n",
      "671\n",
      "Epoch [672/1000], Loss: 0.0539\n",
      "Epoch [672/1000], Loss: 0.0611\n",
      "Epoch [672/1000], Loss: 0.0816\n",
      "Epoch [672/1000], Loss: 0.0844\n",
      "Epoch [672/1000], Loss: 0.0952\n",
      "Epoch [672/1000], Loss: 0.0726\n",
      "Epoch [672/1000], Loss: 0.0452\n",
      "Epoch [672/1000], Loss: 0.0407\n",
      "Epoch [672/1000], Loss: 0.0623\n",
      "Epoch [672/1000], Loss: 0.0777\n",
      "Epoch [672/1000], Loss: 0.0706\n",
      "tensor(2.4571, grad_fn=<MeanBackward0>)\n",
      "672\n",
      "Epoch [673/1000], Loss: 0.0772\n",
      "Epoch [673/1000], Loss: 0.0958\n",
      "Epoch [673/1000], Loss: 0.0663\n",
      "Epoch [673/1000], Loss: 0.0763\n",
      "Epoch [673/1000], Loss: 0.1036\n",
      "Epoch [673/1000], Loss: 0.1317\n",
      "Epoch [673/1000], Loss: 0.1153\n",
      "Epoch [673/1000], Loss: 0.1207\n",
      "Epoch [673/1000], Loss: 0.1097\n",
      "Epoch [673/1000], Loss: 0.0765\n",
      "Epoch [673/1000], Loss: 0.1323\n",
      "tensor(2.4987, grad_fn=<MeanBackward0>)\n",
      "673\n",
      "Epoch [674/1000], Loss: 0.1244\n",
      "Epoch [674/1000], Loss: 0.1175\n",
      "Epoch [674/1000], Loss: 0.0907\n",
      "Epoch [674/1000], Loss: 0.0777\n",
      "Epoch [674/1000], Loss: 0.0952\n",
      "Epoch [674/1000], Loss: 0.1062\n",
      "Epoch [674/1000], Loss: 0.1021\n",
      "Epoch [674/1000], Loss: 0.1076\n",
      "Epoch [674/1000], Loss: 0.1093\n",
      "Epoch [674/1000], Loss: 0.0852\n",
      "Epoch [674/1000], Loss: 0.1111\n",
      "tensor(2.5375, grad_fn=<MeanBackward0>)\n",
      "674\n",
      "Epoch [675/1000], Loss: 0.1093\n",
      "Epoch [675/1000], Loss: 0.0848\n",
      "Epoch [675/1000], Loss: 0.0566\n",
      "Epoch [675/1000], Loss: 0.0763\n",
      "Epoch [675/1000], Loss: 0.0902\n",
      "Epoch [675/1000], Loss: 0.0744\n",
      "Epoch [675/1000], Loss: 0.0509\n",
      "Epoch [675/1000], Loss: 0.0466\n",
      "Epoch [675/1000], Loss: 0.0567\n",
      "Epoch [675/1000], Loss: 0.0618\n",
      "Epoch [675/1000], Loss: 0.0646\n",
      "tensor(2.4916, grad_fn=<MeanBackward0>)\n",
      "675\n",
      "Epoch [676/1000], Loss: 0.0651\n",
      "Epoch [676/1000], Loss: 0.0730\n",
      "Epoch [676/1000], Loss: 0.0832\n",
      "Epoch [676/1000], Loss: 0.0488\n",
      "Epoch [676/1000], Loss: 0.0580\n",
      "Epoch [676/1000], Loss: 0.1015\n",
      "Epoch [676/1000], Loss: 0.0939\n",
      "Epoch [676/1000], Loss: 0.0764\n",
      "Epoch [676/1000], Loss: 0.0447\n",
      "Epoch [676/1000], Loss: 0.0800\n",
      "Epoch [676/1000], Loss: 0.0820\n",
      "tensor(2.5108, grad_fn=<MeanBackward0>)\n",
      "676\n",
      "Epoch [677/1000], Loss: 0.0647\n",
      "Epoch [677/1000], Loss: 0.0734\n",
      "Epoch [677/1000], Loss: 0.0838\n",
      "Epoch [677/1000], Loss: 0.0712\n",
      "Epoch [677/1000], Loss: 0.0653\n",
      "Epoch [677/1000], Loss: 0.1142\n",
      "Epoch [677/1000], Loss: 0.1624\n",
      "Epoch [677/1000], Loss: 0.2024\n",
      "Epoch [677/1000], Loss: 0.1883\n",
      "Epoch [677/1000], Loss: 0.0945\n",
      "Epoch [677/1000], Loss: 0.0892\n",
      "tensor(2.6228, grad_fn=<MeanBackward0>)\n",
      "677\n",
      "Epoch [678/1000], Loss: 0.2160\n",
      "Epoch [678/1000], Loss: 0.2900\n",
      "Epoch [678/1000], Loss: 0.3160\n",
      "Epoch [678/1000], Loss: 0.2751\n",
      "Epoch [678/1000], Loss: 0.1550\n",
      "Epoch [678/1000], Loss: 0.0960\n",
      "Epoch [678/1000], Loss: 0.2305\n",
      "Epoch [678/1000], Loss: 0.3871\n",
      "Epoch [678/1000], Loss: 0.4792\n",
      "Epoch [678/1000], Loss: 0.4947\n",
      "Epoch [678/1000], Loss: 0.4727\n",
      "tensor(2.2848, grad_fn=<MeanBackward0>)\n",
      "678\n",
      "Epoch [679/1000], Loss: 0.2934\n",
      "Epoch [679/1000], Loss: 0.0772\n",
      "Epoch [679/1000], Loss: 0.2604\n",
      "Epoch [679/1000], Loss: 0.4755\n",
      "Epoch [679/1000], Loss: 0.5579\n",
      "Epoch [679/1000], Loss: 0.5443\n",
      "Epoch [679/1000], Loss: 0.4257\n",
      "Epoch [679/1000], Loss: 0.2353\n",
      "Epoch [679/1000], Loss: 0.1913\n",
      "Epoch [679/1000], Loss: 0.2478\n",
      "Epoch [679/1000], Loss: 0.4279\n",
      "tensor(2.1533, grad_fn=<MeanBackward0>)\n",
      "679\n",
      "Epoch [680/1000], Loss: 0.4573\n",
      "Epoch [680/1000], Loss: 0.4257\n",
      "Epoch [680/1000], Loss: 0.3756\n",
      "Epoch [680/1000], Loss: 0.2139\n",
      "Epoch [680/1000], Loss: 0.2233\n",
      "Epoch [680/1000], Loss: 0.2987\n",
      "Epoch [680/1000], Loss: 0.3736\n",
      "Epoch [680/1000], Loss: 0.3395\n",
      "Epoch [680/1000], Loss: 0.3015\n",
      "Epoch [680/1000], Loss: 0.1773\n",
      "Epoch [680/1000], Loss: 0.1733\n",
      "tensor(2.2872, grad_fn=<MeanBackward0>)\n",
      "680\n",
      "Epoch [681/1000], Loss: 0.2271\n",
      "Epoch [681/1000], Loss: 0.2275\n",
      "Epoch [681/1000], Loss: 0.1582\n",
      "Epoch [681/1000], Loss: 0.0808\n",
      "Epoch [681/1000], Loss: 0.1430\n",
      "Epoch [681/1000], Loss: 0.2012\n",
      "Epoch [681/1000], Loss: 0.1396\n",
      "Epoch [681/1000], Loss: 0.0700\n",
      "Epoch [681/1000], Loss: 0.1263\n",
      "Epoch [681/1000], Loss: 0.1444\n",
      "Epoch [681/1000], Loss: 0.1599\n",
      "tensor(2.3929, grad_fn=<MeanBackward0>)\n",
      "681\n",
      "Epoch [682/1000], Loss: 0.0990\n",
      "Epoch [682/1000], Loss: 0.1113\n",
      "Epoch [682/1000], Loss: 0.1409\n",
      "Epoch [682/1000], Loss: 0.1382\n",
      "Epoch [682/1000], Loss: 0.0927\n",
      "Epoch [682/1000], Loss: 0.0989\n",
      "Epoch [682/1000], Loss: 0.0969\n",
      "Epoch [682/1000], Loss: 0.1030\n",
      "Epoch [682/1000], Loss: 0.0644\n",
      "Epoch [682/1000], Loss: 0.0569\n",
      "Epoch [682/1000], Loss: 0.0633\n",
      "tensor(2.4401, grad_fn=<MeanBackward0>)\n",
      "682\n",
      "Epoch [683/1000], Loss: 0.0759\n",
      "Epoch [683/1000], Loss: 0.0749\n",
      "Epoch [683/1000], Loss: 0.0834\n",
      "Epoch [683/1000], Loss: 0.0738\n",
      "Epoch [683/1000], Loss: 0.0709\n",
      "Epoch [683/1000], Loss: 0.0700\n",
      "Epoch [683/1000], Loss: 0.1128\n",
      "Epoch [683/1000], Loss: 0.1273\n",
      "Epoch [683/1000], Loss: 0.1146\n",
      "Epoch [683/1000], Loss: 0.0590\n",
      "Epoch [683/1000], Loss: 0.0746\n",
      "tensor(2.5148, grad_fn=<MeanBackward0>)\n",
      "683\n",
      "Epoch [684/1000], Loss: 0.1308\n",
      "Epoch [684/1000], Loss: 0.1106\n",
      "Epoch [684/1000], Loss: 0.0577\n",
      "Epoch [684/1000], Loss: 0.0626\n",
      "Epoch [684/1000], Loss: 0.0733\n",
      "Epoch [684/1000], Loss: 0.0417\n",
      "Epoch [684/1000], Loss: 0.0399\n",
      "Epoch [684/1000], Loss: 0.0551\n",
      "Epoch [684/1000], Loss: 0.0609\n",
      "Epoch [684/1000], Loss: 0.0762\n",
      "Epoch [684/1000], Loss: 0.0756\n",
      "tensor(2.4992, grad_fn=<MeanBackward0>)\n",
      "684\n",
      "Epoch [685/1000], Loss: 0.0812\n",
      "Epoch [685/1000], Loss: 0.1577\n",
      "Epoch [685/1000], Loss: 0.1740\n",
      "Epoch [685/1000], Loss: 0.1825\n",
      "Epoch [685/1000], Loss: 0.1191\n",
      "Epoch [685/1000], Loss: 0.0715\n",
      "Epoch [685/1000], Loss: 0.1158\n",
      "Epoch [685/1000], Loss: 0.1848\n",
      "Epoch [685/1000], Loss: 0.2138\n",
      "Epoch [685/1000], Loss: 0.1917\n",
      "Epoch [685/1000], Loss: 0.1664\n",
      "tensor(2.4698, grad_fn=<MeanBackward0>)\n",
      "685\n",
      "Epoch [686/1000], Loss: 0.1126\n",
      "Epoch [686/1000], Loss: 0.1473\n",
      "Epoch [686/1000], Loss: 0.2803\n",
      "Epoch [686/1000], Loss: 0.3282\n",
      "Epoch [686/1000], Loss: 0.3053\n",
      "Epoch [686/1000], Loss: 0.2265\n",
      "Epoch [686/1000], Loss: 0.1336\n",
      "Epoch [686/1000], Loss: 0.1387\n",
      "Epoch [686/1000], Loss: 0.3062\n",
      "Epoch [686/1000], Loss: 0.3526\n",
      "Epoch [686/1000], Loss: 0.4377\n",
      "tensor(2.1646, grad_fn=<MeanBackward0>)\n",
      "686\n",
      "Epoch [687/1000], Loss: 0.4500\n",
      "Epoch [687/1000], Loss: 0.3371\n",
      "Epoch [687/1000], Loss: 0.1858\n",
      "Epoch [687/1000], Loss: 0.1738\n",
      "Epoch [687/1000], Loss: 0.3562\n",
      "Epoch [687/1000], Loss: 0.4857\n",
      "Epoch [687/1000], Loss: 0.5480\n",
      "Epoch [687/1000], Loss: 0.5154\n",
      "Epoch [687/1000], Loss: 0.3968\n",
      "Epoch [687/1000], Loss: 0.2542\n",
      "Epoch [687/1000], Loss: 0.2215\n",
      "tensor(2.0938, grad_fn=<MeanBackward0>)\n",
      "687\n",
      "Epoch [688/1000], Loss: 0.4313\n",
      "Epoch [688/1000], Loss: 0.5280\n",
      "Epoch [688/1000], Loss: 0.5630\n",
      "Epoch [688/1000], Loss: 0.5275\n",
      "Epoch [688/1000], Loss: 0.3638\n",
      "Epoch [688/1000], Loss: 0.1931\n",
      "Epoch [688/1000], Loss: 0.1629\n",
      "Epoch [688/1000], Loss: 0.2720\n",
      "Epoch [688/1000], Loss: 0.3869\n",
      "Epoch [688/1000], Loss: 0.3443\n",
      "Epoch [688/1000], Loss: 0.1554\n",
      "tensor(2.3445, grad_fn=<MeanBackward0>)\n",
      "688\n",
      "Epoch [689/1000], Loss: 0.1511\n",
      "Epoch [689/1000], Loss: 0.2057\n",
      "Epoch [689/1000], Loss: 0.1903\n",
      "Epoch [689/1000], Loss: 0.1656\n",
      "Epoch [689/1000], Loss: 0.1154\n",
      "Epoch [689/1000], Loss: 0.1358\n",
      "Epoch [689/1000], Loss: 0.1325\n",
      "Epoch [689/1000], Loss: 0.0926\n",
      "Epoch [689/1000], Loss: 0.0733\n",
      "Epoch [689/1000], Loss: 0.1303\n",
      "Epoch [689/1000], Loss: 0.1288\n",
      "tensor(2.4094, grad_fn=<MeanBackward0>)\n",
      "689\n",
      "Epoch [690/1000], Loss: 0.0836\n",
      "Epoch [690/1000], Loss: 0.1363\n",
      "Epoch [690/1000], Loss: 0.1536\n",
      "Epoch [690/1000], Loss: 0.1379\n",
      "Epoch [690/1000], Loss: 0.0782\n",
      "Epoch [690/1000], Loss: 0.0772\n",
      "Epoch [690/1000], Loss: 0.1262\n",
      "Epoch [690/1000], Loss: 0.1208\n",
      "Epoch [690/1000], Loss: 0.0963\n",
      "Epoch [690/1000], Loss: 0.0987\n",
      "Epoch [690/1000], Loss: 0.1093\n",
      "tensor(2.4629, grad_fn=<MeanBackward0>)\n",
      "690\n",
      "Epoch [691/1000], Loss: 0.0982\n",
      "Epoch [691/1000], Loss: 0.0819\n",
      "Epoch [691/1000], Loss: 0.0795\n",
      "Epoch [691/1000], Loss: 0.0593\n",
      "Epoch [691/1000], Loss: 0.0595\n",
      "Epoch [691/1000], Loss: 0.0531\n",
      "Epoch [691/1000], Loss: 0.0760\n",
      "Epoch [691/1000], Loss: 0.0630\n",
      "Epoch [691/1000], Loss: 0.0447\n",
      "Epoch [691/1000], Loss: 0.0422\n",
      "Epoch [691/1000], Loss: 0.0489\n",
      "tensor(2.4622, grad_fn=<MeanBackward0>)\n",
      "691\n",
      "Epoch [692/1000], Loss: 0.0452\n",
      "Epoch [692/1000], Loss: 0.0433\n",
      "Epoch [692/1000], Loss: 0.0626\n",
      "Epoch [692/1000], Loss: 0.0564\n",
      "Epoch [692/1000], Loss: 0.0516\n",
      "Epoch [692/1000], Loss: 0.0488\n",
      "Epoch [692/1000], Loss: 0.0472\n",
      "Epoch [692/1000], Loss: 0.0411\n",
      "Epoch [692/1000], Loss: 0.0447\n",
      "Epoch [692/1000], Loss: 0.0525\n",
      "Epoch [692/1000], Loss: 0.0598\n",
      "tensor(2.4600, grad_fn=<MeanBackward0>)\n",
      "692\n",
      "Epoch [693/1000], Loss: 0.0503\n",
      "Epoch [693/1000], Loss: 0.0761\n",
      "Epoch [693/1000], Loss: 0.1112\n",
      "Epoch [693/1000], Loss: 0.1096\n",
      "Epoch [693/1000], Loss: 0.0642\n",
      "Epoch [693/1000], Loss: 0.0694\n",
      "Epoch [693/1000], Loss: 0.1082\n",
      "Epoch [693/1000], Loss: 0.1339\n",
      "Epoch [693/1000], Loss: 0.1323\n",
      "Epoch [693/1000], Loss: 0.0870\n",
      "Epoch [693/1000], Loss: 0.0557\n",
      "tensor(2.4881, grad_fn=<MeanBackward0>)\n",
      "693\n",
      "Epoch [694/1000], Loss: 0.0653\n",
      "Epoch [694/1000], Loss: 0.0775\n",
      "Epoch [694/1000], Loss: 0.1195\n",
      "Epoch [694/1000], Loss: 0.1070\n",
      "Epoch [694/1000], Loss: 0.1035\n",
      "Epoch [694/1000], Loss: 0.0776\n",
      "Epoch [694/1000], Loss: 0.0599\n",
      "Epoch [694/1000], Loss: 0.1106\n",
      "Epoch [694/1000], Loss: 0.1631\n",
      "Epoch [694/1000], Loss: 0.1332\n",
      "Epoch [694/1000], Loss: 0.1195\n",
      "tensor(2.4062, grad_fn=<MeanBackward0>)\n",
      "694\n",
      "Epoch [695/1000], Loss: 0.0901\n",
      "Epoch [695/1000], Loss: 0.1031\n",
      "Epoch [695/1000], Loss: 0.1424\n",
      "Epoch [695/1000], Loss: 0.1865\n",
      "Epoch [695/1000], Loss: 0.1765\n",
      "Epoch [695/1000], Loss: 0.1377\n",
      "Epoch [695/1000], Loss: 0.0744\n",
      "Epoch [695/1000], Loss: 0.1362\n",
      "Epoch [695/1000], Loss: 0.2752\n",
      "Epoch [695/1000], Loss: 0.3030\n",
      "Epoch [695/1000], Loss: 0.3397\n",
      "tensor(2.2490, grad_fn=<MeanBackward0>)\n",
      "695\n",
      "Epoch [696/1000], Loss: 0.3279\n",
      "Epoch [696/1000], Loss: 0.2273\n",
      "Epoch [696/1000], Loss: 0.0970\n",
      "Epoch [696/1000], Loss: 0.2297\n",
      "Epoch [696/1000], Loss: 0.3974\n",
      "Epoch [696/1000], Loss: 0.4965\n",
      "Epoch [696/1000], Loss: 0.5229\n",
      "Epoch [696/1000], Loss: 0.4631\n",
      "Epoch [696/1000], Loss: 0.2831\n",
      "Epoch [696/1000], Loss: 0.2077\n",
      "Epoch [696/1000], Loss: 0.2696\n",
      "tensor(2.0977, grad_fn=<MeanBackward0>)\n",
      "696\n",
      "Epoch [697/1000], Loss: 0.4948\n",
      "Epoch [697/1000], Loss: 0.6331\n",
      "Epoch [697/1000], Loss: 0.6805\n",
      "Epoch [697/1000], Loss: 0.6443\n",
      "Epoch [697/1000], Loss: 0.4541\n",
      "Epoch [697/1000], Loss: 0.2512\n",
      "Epoch [697/1000], Loss: 0.1873\n",
      "Epoch [697/1000], Loss: 0.3152\n",
      "Epoch [697/1000], Loss: 0.4505\n",
      "Epoch [697/1000], Loss: 0.4711\n",
      "Epoch [697/1000], Loss: 0.2612\n",
      "tensor(2.3921, grad_fn=<MeanBackward0>)\n",
      "697\n",
      "Epoch [698/1000], Loss: 0.1927\n",
      "Epoch [698/1000], Loss: 0.2264\n",
      "Epoch [698/1000], Loss: 0.2538\n",
      "Epoch [698/1000], Loss: 0.2550\n",
      "Epoch [698/1000], Loss: 0.1567\n",
      "Epoch [698/1000], Loss: 0.1324\n",
      "Epoch [698/1000], Loss: 0.1232\n",
      "Epoch [698/1000], Loss: 0.1156\n",
      "Epoch [698/1000], Loss: 0.0745\n",
      "Epoch [698/1000], Loss: 0.0907\n",
      "Epoch [698/1000], Loss: 0.1452\n",
      "tensor(2.3404, grad_fn=<MeanBackward0>)\n",
      "698\n",
      "Epoch [699/1000], Loss: 0.1200\n",
      "Epoch [699/1000], Loss: 0.1086\n",
      "Epoch [699/1000], Loss: 0.1372\n",
      "Epoch [699/1000], Loss: 0.1503\n",
      "Epoch [699/1000], Loss: 0.1684\n",
      "Epoch [699/1000], Loss: 0.1507\n",
      "Epoch [699/1000], Loss: 0.1281\n",
      "Epoch [699/1000], Loss: 0.0935\n",
      "Epoch [699/1000], Loss: 0.1144\n",
      "Epoch [699/1000], Loss: 0.1121\n",
      "Epoch [699/1000], Loss: 0.1221\n",
      "tensor(2.2991, grad_fn=<MeanBackward0>)\n",
      "699\n",
      "Epoch [700/1000], Loss: 0.1118\n",
      "Epoch [700/1000], Loss: 0.0820\n",
      "Epoch [700/1000], Loss: 0.1005\n",
      "Epoch [700/1000], Loss: 0.1094\n",
      "Epoch [700/1000], Loss: 0.1294\n",
      "Epoch [700/1000], Loss: 0.1176\n",
      "Epoch [700/1000], Loss: 0.1021\n",
      "Epoch [700/1000], Loss: 0.0919\n",
      "Epoch [700/1000], Loss: 0.1079\n",
      "Epoch [700/1000], Loss: 0.0806\n",
      "Epoch [700/1000], Loss: 0.1070\n",
      "tensor(2.3061, grad_fn=<MeanBackward0>)\n",
      "700\n",
      "Epoch [701/1000], Loss: 0.1316\n",
      "Epoch [701/1000], Loss: 0.1223\n",
      "Epoch [701/1000], Loss: 0.0722\n",
      "Epoch [701/1000], Loss: 0.1056\n",
      "Epoch [701/1000], Loss: 0.2238\n",
      "Epoch [701/1000], Loss: 0.2673\n",
      "Epoch [701/1000], Loss: 0.2442\n",
      "Epoch [701/1000], Loss: 0.1555\n",
      "Epoch [701/1000], Loss: 0.0609\n",
      "Epoch [701/1000], Loss: 0.1255\n",
      "Epoch [701/1000], Loss: 0.2006\n",
      "tensor(2.1768, grad_fn=<MeanBackward0>)\n",
      "701\n",
      "Epoch [702/1000], Loss: 0.2500\n",
      "Epoch [702/1000], Loss: 0.2455\n",
      "Epoch [702/1000], Loss: 0.2167\n",
      "Epoch [702/1000], Loss: 0.1357\n",
      "Epoch [702/1000], Loss: 0.1519\n",
      "Epoch [702/1000], Loss: 0.2651\n",
      "Epoch [702/1000], Loss: 0.3666\n",
      "Epoch [702/1000], Loss: 0.4017\n",
      "Epoch [702/1000], Loss: 0.4014\n",
      "Epoch [702/1000], Loss: 0.3182\n",
      "Epoch [702/1000], Loss: 0.1243\n",
      "tensor(2.1261, grad_fn=<MeanBackward0>)\n",
      "702\n",
      "Epoch [703/1000], Loss: 0.2479\n",
      "Epoch [703/1000], Loss: 0.4195\n",
      "Epoch [703/1000], Loss: 0.5607\n",
      "Epoch [703/1000], Loss: 0.6510\n",
      "Epoch [703/1000], Loss: 0.5552\n",
      "Epoch [703/1000], Loss: 0.4237\n",
      "Epoch [703/1000], Loss: 0.2072\n",
      "Epoch [703/1000], Loss: 0.0852\n",
      "Epoch [703/1000], Loss: 0.2963\n",
      "Epoch [703/1000], Loss: 0.4374\n",
      "Epoch [703/1000], Loss: 0.3962\n",
      "tensor(2.5597, grad_fn=<MeanBackward0>)\n",
      "703\n",
      "Epoch [704/1000], Loss: 0.3098\n",
      "Epoch [704/1000], Loss: 0.2147\n",
      "Epoch [704/1000], Loss: 0.1889\n",
      "Epoch [704/1000], Loss: 0.2384\n",
      "Epoch [704/1000], Loss: 0.2248\n",
      "Epoch [704/1000], Loss: 0.2612\n",
      "Epoch [704/1000], Loss: 0.2214\n",
      "Epoch [704/1000], Loss: 0.2083\n",
      "Epoch [704/1000], Loss: 0.1276\n",
      "Epoch [704/1000], Loss: 0.1166\n",
      "Epoch [704/1000], Loss: 0.1197\n",
      "tensor(2.4675, grad_fn=<MeanBackward0>)\n",
      "704\n",
      "Epoch [705/1000], Loss: 0.2048\n",
      "Epoch [705/1000], Loss: 0.1879\n",
      "Epoch [705/1000], Loss: 0.1242\n",
      "Epoch [705/1000], Loss: 0.1226\n",
      "Epoch [705/1000], Loss: 0.1170\n",
      "Epoch [705/1000], Loss: 0.1055\n",
      "Epoch [705/1000], Loss: 0.0957\n",
      "Epoch [705/1000], Loss: 0.0873\n",
      "Epoch [705/1000], Loss: 0.0880\n",
      "Epoch [705/1000], Loss: 0.0758\n",
      "Epoch [705/1000], Loss: 0.0793\n",
      "tensor(2.4043, grad_fn=<MeanBackward0>)\n",
      "705\n",
      "Epoch [706/1000], Loss: 0.0713\n",
      "Epoch [706/1000], Loss: 0.0713\n",
      "Epoch [706/1000], Loss: 0.0587\n",
      "Epoch [706/1000], Loss: 0.0687\n",
      "Epoch [706/1000], Loss: 0.0649\n",
      "Epoch [706/1000], Loss: 0.0765\n",
      "Epoch [706/1000], Loss: 0.0788\n",
      "Epoch [706/1000], Loss: 0.0646\n",
      "Epoch [706/1000], Loss: 0.0784\n",
      "Epoch [706/1000], Loss: 0.0768\n",
      "Epoch [706/1000], Loss: 0.0921\n",
      "tensor(2.3423, grad_fn=<MeanBackward0>)\n",
      "706\n",
      "Epoch [707/1000], Loss: 0.0747\n",
      "Epoch [707/1000], Loss: 0.0526\n",
      "Epoch [707/1000], Loss: 0.0583\n",
      "Epoch [707/1000], Loss: 0.0739\n",
      "Epoch [707/1000], Loss: 0.0690\n",
      "Epoch [707/1000], Loss: 0.0705\n",
      "Epoch [707/1000], Loss: 0.0549\n",
      "Epoch [707/1000], Loss: 0.0749\n",
      "Epoch [707/1000], Loss: 0.0965\n",
      "Epoch [707/1000], Loss: 0.0978\n",
      "Epoch [707/1000], Loss: 0.1003\n",
      "tensor(2.3568, grad_fn=<MeanBackward0>)\n",
      "707\n",
      "Epoch [708/1000], Loss: 0.0639\n",
      "Epoch [708/1000], Loss: 0.0542\n",
      "Epoch [708/1000], Loss: 0.0883\n",
      "Epoch [708/1000], Loss: 0.1024\n",
      "Epoch [708/1000], Loss: 0.1321\n",
      "Epoch [708/1000], Loss: 0.1262\n",
      "Epoch [708/1000], Loss: 0.1223\n",
      "Epoch [708/1000], Loss: 0.0658\n",
      "Epoch [708/1000], Loss: 0.0661\n",
      "Epoch [708/1000], Loss: 0.0836\n",
      "Epoch [708/1000], Loss: 0.0900\n",
      "tensor(2.3087, grad_fn=<MeanBackward0>)\n",
      "708\n",
      "Epoch [709/1000], Loss: 0.1032\n",
      "Epoch [709/1000], Loss: 0.1053\n",
      "Epoch [709/1000], Loss: 0.1198\n",
      "Epoch [709/1000], Loss: 0.0859\n",
      "Epoch [709/1000], Loss: 0.0826\n",
      "Epoch [709/1000], Loss: 0.0986\n",
      "Epoch [709/1000], Loss: 0.1180\n",
      "Epoch [709/1000], Loss: 0.1146\n",
      "Epoch [709/1000], Loss: 0.1183\n",
      "Epoch [709/1000], Loss: 0.1291\n",
      "Epoch [709/1000], Loss: 0.1272\n",
      "tensor(2.2348, grad_fn=<MeanBackward0>)\n",
      "709\n",
      "Epoch [710/1000], Loss: 0.2147\n",
      "Epoch [710/1000], Loss: 0.2260\n",
      "Epoch [710/1000], Loss: 0.1630\n",
      "Epoch [710/1000], Loss: 0.0657\n",
      "Epoch [710/1000], Loss: 0.1209\n",
      "Epoch [710/1000], Loss: 0.2054\n",
      "Epoch [710/1000], Loss: 0.2274\n",
      "Epoch [710/1000], Loss: 0.2161\n",
      "Epoch [710/1000], Loss: 0.2009\n",
      "Epoch [710/1000], Loss: 0.1354\n",
      "Epoch [710/1000], Loss: 0.1252\n",
      "tensor(2.1305, grad_fn=<MeanBackward0>)\n",
      "710\n",
      "Epoch [711/1000], Loss: 0.2531\n",
      "Epoch [711/1000], Loss: 0.3006\n",
      "Epoch [711/1000], Loss: 0.3518\n",
      "Epoch [711/1000], Loss: 0.3294\n",
      "Epoch [711/1000], Loss: 0.2117\n",
      "Epoch [711/1000], Loss: 0.1037\n",
      "Epoch [711/1000], Loss: 0.1891\n",
      "Epoch [711/1000], Loss: 0.3410\n",
      "Epoch [711/1000], Loss: 0.4542\n",
      "Epoch [711/1000], Loss: 0.4341\n",
      "Epoch [711/1000], Loss: 0.2758\n",
      "tensor(2.3781, grad_fn=<MeanBackward0>)\n",
      "711\n",
      "Epoch [712/1000], Loss: 0.1016\n",
      "Epoch [712/1000], Loss: 0.1821\n",
      "Epoch [712/1000], Loss: 0.3157\n",
      "Epoch [712/1000], Loss: 0.3810\n",
      "Epoch [712/1000], Loss: 0.3334\n",
      "Epoch [712/1000], Loss: 0.2801\n",
      "Epoch [712/1000], Loss: 0.1838\n",
      "Epoch [712/1000], Loss: 0.1306\n",
      "Epoch [712/1000], Loss: 0.1833\n",
      "Epoch [712/1000], Loss: 0.2434\n",
      "Epoch [712/1000], Loss: 0.2495\n",
      "tensor(2.4969, grad_fn=<MeanBackward0>)\n",
      "712\n",
      "Epoch [713/1000], Loss: 0.2082\n",
      "Epoch [713/1000], Loss: 0.1519\n",
      "Epoch [713/1000], Loss: 0.1313\n",
      "Epoch [713/1000], Loss: 0.1761\n",
      "Epoch [713/1000], Loss: 0.1980\n",
      "Epoch [713/1000], Loss: 0.1855\n",
      "Epoch [713/1000], Loss: 0.1486\n",
      "Epoch [713/1000], Loss: 0.0939\n",
      "Epoch [713/1000], Loss: 0.1078\n",
      "Epoch [713/1000], Loss: 0.1603\n",
      "Epoch [713/1000], Loss: 0.1684\n",
      "tensor(2.4653, grad_fn=<MeanBackward0>)\n",
      "713\n",
      "Epoch [714/1000], Loss: 0.1220\n",
      "Epoch [714/1000], Loss: 0.1030\n",
      "Epoch [714/1000], Loss: 0.1086\n",
      "Epoch [714/1000], Loss: 0.1514\n",
      "Epoch [714/1000], Loss: 0.1503\n",
      "Epoch [714/1000], Loss: 0.1406\n",
      "Epoch [714/1000], Loss: 0.1084\n",
      "Epoch [714/1000], Loss: 0.0777\n",
      "Epoch [714/1000], Loss: 0.1252\n",
      "Epoch [714/1000], Loss: 0.1355\n",
      "Epoch [714/1000], Loss: 0.0897\n",
      "tensor(2.3801, grad_fn=<MeanBackward0>)\n",
      "714\n",
      "Epoch [715/1000], Loss: 0.0573\n",
      "Epoch [715/1000], Loss: 0.0864\n",
      "Epoch [715/1000], Loss: 0.1116\n",
      "Epoch [715/1000], Loss: 0.1129\n",
      "Epoch [715/1000], Loss: 0.0905\n",
      "Epoch [715/1000], Loss: 0.0938\n",
      "Epoch [715/1000], Loss: 0.0746\n",
      "Epoch [715/1000], Loss: 0.0446\n",
      "Epoch [715/1000], Loss: 0.0597\n",
      "Epoch [715/1000], Loss: 0.0562\n",
      "Epoch [715/1000], Loss: 0.0581\n",
      "tensor(2.3811, grad_fn=<MeanBackward0>)\n",
      "715\n",
      "Epoch [716/1000], Loss: 0.0664\n",
      "Epoch [716/1000], Loss: 0.0645\n",
      "Epoch [716/1000], Loss: 0.0680\n",
      "Epoch [716/1000], Loss: 0.0892\n",
      "Epoch [716/1000], Loss: 0.0900\n",
      "Epoch [716/1000], Loss: 0.0618\n",
      "Epoch [716/1000], Loss: 0.0667\n",
      "Epoch [716/1000], Loss: 0.1177\n",
      "Epoch [716/1000], Loss: 0.0925\n",
      "Epoch [716/1000], Loss: 0.0762\n",
      "Epoch [716/1000], Loss: 0.0874\n",
      "tensor(2.3698, grad_fn=<MeanBackward0>)\n",
      "716\n",
      "Epoch [717/1000], Loss: 0.0802\n",
      "Epoch [717/1000], Loss: 0.0604\n",
      "Epoch [717/1000], Loss: 0.0574\n",
      "Epoch [717/1000], Loss: 0.0655\n",
      "Epoch [717/1000], Loss: 0.0700\n",
      "Epoch [717/1000], Loss: 0.0841\n",
      "Epoch [717/1000], Loss: 0.0672\n",
      "Epoch [717/1000], Loss: 0.0609\n",
      "Epoch [717/1000], Loss: 0.0682\n",
      "Epoch [717/1000], Loss: 0.0673\n",
      "Epoch [717/1000], Loss: 0.0684\n",
      "tensor(2.4110, grad_fn=<MeanBackward0>)\n",
      "717\n",
      "Epoch [718/1000], Loss: 0.0704\n",
      "Epoch [718/1000], Loss: 0.0606\n",
      "Epoch [718/1000], Loss: 0.0670\n",
      "Epoch [718/1000], Loss: 0.0560\n",
      "Epoch [718/1000], Loss: 0.0567\n",
      "Epoch [718/1000], Loss: 0.0680\n",
      "Epoch [718/1000], Loss: 0.0661\n",
      "Epoch [718/1000], Loss: 0.0693\n",
      "Epoch [718/1000], Loss: 0.0711\n",
      "Epoch [718/1000], Loss: 0.0795\n",
      "Epoch [718/1000], Loss: 0.0751\n",
      "tensor(2.4176, grad_fn=<MeanBackward0>)\n",
      "718\n",
      "Epoch [719/1000], Loss: 0.0506\n",
      "Epoch [719/1000], Loss: 0.0695\n",
      "Epoch [719/1000], Loss: 0.0845\n",
      "Epoch [719/1000], Loss: 0.0956\n",
      "Epoch [719/1000], Loss: 0.0894\n",
      "Epoch [719/1000], Loss: 0.0753\n",
      "Epoch [719/1000], Loss: 0.0384\n",
      "Epoch [719/1000], Loss: 0.0476\n",
      "Epoch [719/1000], Loss: 0.0667\n",
      "Epoch [719/1000], Loss: 0.0791\n",
      "Epoch [719/1000], Loss: 0.0823\n",
      "tensor(2.4162, grad_fn=<MeanBackward0>)\n",
      "719\n",
      "Epoch [720/1000], Loss: 0.0697\n",
      "Epoch [720/1000], Loss: 0.0415\n",
      "Epoch [720/1000], Loss: 0.0704\n",
      "Epoch [720/1000], Loss: 0.0914\n",
      "Epoch [720/1000], Loss: 0.1005\n",
      "Epoch [720/1000], Loss: 0.0927\n",
      "Epoch [720/1000], Loss: 0.0825\n",
      "Epoch [720/1000], Loss: 0.0746\n",
      "Epoch [720/1000], Loss: 0.0756\n",
      "Epoch [720/1000], Loss: 0.0690\n",
      "Epoch [720/1000], Loss: 0.1230\n",
      "tensor(2.4617, grad_fn=<MeanBackward0>)\n",
      "720\n",
      "Epoch [721/1000], Loss: 0.1835\n",
      "Epoch [721/1000], Loss: 0.2223\n",
      "Epoch [721/1000], Loss: 0.1629\n",
      "Epoch [721/1000], Loss: 0.1006\n",
      "Epoch [721/1000], Loss: 0.0957\n",
      "Epoch [721/1000], Loss: 0.1167\n",
      "Epoch [721/1000], Loss: 0.1014\n",
      "Epoch [721/1000], Loss: 0.1424\n",
      "Epoch [721/1000], Loss: 0.1570\n",
      "Epoch [721/1000], Loss: 0.1331\n",
      "Epoch [721/1000], Loss: 0.1516\n",
      "tensor(2.5781, grad_fn=<MeanBackward0>)\n",
      "721\n",
      "Epoch [722/1000], Loss: 0.2144\n",
      "Epoch [722/1000], Loss: 0.2219\n",
      "Epoch [722/1000], Loss: 0.1657\n",
      "Epoch [722/1000], Loss: 0.0692\n",
      "Epoch [722/1000], Loss: 0.1236\n",
      "Epoch [722/1000], Loss: 0.1965\n",
      "Epoch [722/1000], Loss: 0.2223\n",
      "Epoch [722/1000], Loss: 0.2218\n",
      "Epoch [722/1000], Loss: 0.1623\n",
      "Epoch [722/1000], Loss: 0.0947\n",
      "Epoch [722/1000], Loss: 0.1003\n",
      "tensor(2.5863, grad_fn=<MeanBackward0>)\n",
      "722\n",
      "Epoch [723/1000], Loss: 0.2301\n",
      "Epoch [723/1000], Loss: 0.2847\n",
      "Epoch [723/1000], Loss: 0.2812\n",
      "Epoch [723/1000], Loss: 0.2042\n",
      "Epoch [723/1000], Loss: 0.0888\n",
      "Epoch [723/1000], Loss: 0.1378\n",
      "Epoch [723/1000], Loss: 0.2272\n",
      "Epoch [723/1000], Loss: 0.3499\n",
      "Epoch [723/1000], Loss: 0.4042\n",
      "Epoch [723/1000], Loss: 0.4179\n",
      "Epoch [723/1000], Loss: 0.3446\n",
      "tensor(2.3750, grad_fn=<MeanBackward0>)\n",
      "723\n",
      "Epoch [724/1000], Loss: 0.1706\n",
      "Epoch [724/1000], Loss: 0.1302\n",
      "Epoch [724/1000], Loss: 0.3260\n",
      "Epoch [724/1000], Loss: 0.4061\n",
      "Epoch [724/1000], Loss: 0.4299\n",
      "Epoch [724/1000], Loss: 0.3352\n",
      "Epoch [724/1000], Loss: 0.2149\n",
      "Epoch [724/1000], Loss: 0.1606\n",
      "Epoch [724/1000], Loss: 0.2642\n",
      "Epoch [724/1000], Loss: 0.3165\n",
      "Epoch [724/1000], Loss: 0.3626\n",
      "tensor(2.2169, grad_fn=<MeanBackward0>)\n",
      "724\n",
      "Epoch [725/1000], Loss: 0.3198\n",
      "Epoch [725/1000], Loss: 0.2335\n",
      "Epoch [725/1000], Loss: 0.1858\n",
      "Epoch [725/1000], Loss: 0.1660\n",
      "Epoch [725/1000], Loss: 0.2668\n",
      "Epoch [725/1000], Loss: 0.3072\n",
      "Epoch [725/1000], Loss: 0.3190\n",
      "Epoch [725/1000], Loss: 0.2658\n",
      "Epoch [725/1000], Loss: 0.1740\n",
      "Epoch [725/1000], Loss: 0.1021\n",
      "Epoch [725/1000], Loss: 0.2031\n",
      "tensor(2.1798, grad_fn=<MeanBackward0>)\n",
      "725\n",
      "Epoch [726/1000], Loss: 0.3103\n",
      "Epoch [726/1000], Loss: 0.3466\n",
      "Epoch [726/1000], Loss: 0.2895\n",
      "Epoch [726/1000], Loss: 0.2450\n",
      "Epoch [726/1000], Loss: 0.1453\n",
      "Epoch [726/1000], Loss: 0.1659\n",
      "Epoch [726/1000], Loss: 0.2719\n",
      "Epoch [726/1000], Loss: 0.3098\n",
      "Epoch [726/1000], Loss: 0.2724\n",
      "Epoch [726/1000], Loss: 0.2058\n",
      "Epoch [726/1000], Loss: 0.1109\n",
      "tensor(2.2210, grad_fn=<MeanBackward0>)\n",
      "726\n",
      "Epoch [727/1000], Loss: 0.1846\n",
      "Epoch [727/1000], Loss: 0.2458\n",
      "Epoch [727/1000], Loss: 0.2876\n",
      "Epoch [727/1000], Loss: 0.3144\n",
      "Epoch [727/1000], Loss: 0.1966\n",
      "Epoch [727/1000], Loss: 0.1128\n",
      "Epoch [727/1000], Loss: 0.1302\n",
      "Epoch [727/1000], Loss: 0.2190\n",
      "Epoch [727/1000], Loss: 0.2780\n",
      "Epoch [727/1000], Loss: 0.2223\n",
      "Epoch [727/1000], Loss: 0.0894\n",
      "tensor(2.3325, grad_fn=<MeanBackward0>)\n",
      "727\n",
      "Epoch [728/1000], Loss: 0.0805\n",
      "Epoch [728/1000], Loss: 0.1503\n",
      "Epoch [728/1000], Loss: 0.1435\n",
      "Epoch [728/1000], Loss: 0.1332\n",
      "Epoch [728/1000], Loss: 0.0986\n",
      "Epoch [728/1000], Loss: 0.0958\n",
      "Epoch [728/1000], Loss: 0.0833\n",
      "Epoch [728/1000], Loss: 0.0621\n",
      "Epoch [728/1000], Loss: 0.0608\n",
      "Epoch [728/1000], Loss: 0.0865\n",
      "Epoch [728/1000], Loss: 0.0492\n",
      "tensor(2.3427, grad_fn=<MeanBackward0>)\n",
      "728\n",
      "Epoch [729/1000], Loss: 0.0575\n",
      "Epoch [729/1000], Loss: 0.0569\n",
      "Epoch [729/1000], Loss: 0.0541\n",
      "Epoch [729/1000], Loss: 0.0570\n",
      "Epoch [729/1000], Loss: 0.0545\n",
      "Epoch [729/1000], Loss: 0.0488\n",
      "Epoch [729/1000], Loss: 0.0628\n",
      "Epoch [729/1000], Loss: 0.0581\n",
      "Epoch [729/1000], Loss: 0.0517\n",
      "Epoch [729/1000], Loss: 0.0776\n",
      "Epoch [729/1000], Loss: 0.0839\n",
      "tensor(2.3799, grad_fn=<MeanBackward0>)\n",
      "729\n",
      "Epoch [730/1000], Loss: 0.0434\n",
      "Epoch [730/1000], Loss: 0.0678\n",
      "Epoch [730/1000], Loss: 0.1008\n",
      "Epoch [730/1000], Loss: 0.0787\n",
      "Epoch [730/1000], Loss: 0.0782\n",
      "Epoch [730/1000], Loss: 0.0584\n",
      "Epoch [730/1000], Loss: 0.0369\n",
      "Epoch [730/1000], Loss: 0.0419\n",
      "Epoch [730/1000], Loss: 0.0643\n",
      "Epoch [730/1000], Loss: 0.0644\n",
      "Epoch [730/1000], Loss: 0.1184\n",
      "tensor(2.4522, grad_fn=<MeanBackward0>)\n",
      "730\n",
      "Epoch [731/1000], Loss: 0.1138\n",
      "Epoch [731/1000], Loss: 0.0767\n",
      "Epoch [731/1000], Loss: 0.0561\n",
      "Epoch [731/1000], Loss: 0.1424\n",
      "Epoch [731/1000], Loss: 0.1485\n",
      "Epoch [731/1000], Loss: 0.1308\n",
      "Epoch [731/1000], Loss: 0.0941\n",
      "Epoch [731/1000], Loss: 0.0992\n",
      "Epoch [731/1000], Loss: 0.1346\n",
      "Epoch [731/1000], Loss: 0.0850\n",
      "Epoch [731/1000], Loss: 0.0614\n",
      "tensor(2.4340, grad_fn=<MeanBackward0>)\n",
      "731\n",
      "Epoch [732/1000], Loss: 0.0532\n",
      "Epoch [732/1000], Loss: 0.0670\n",
      "Epoch [732/1000], Loss: 0.0695\n",
      "Epoch [732/1000], Loss: 0.0806\n",
      "Epoch [732/1000], Loss: 0.0958\n",
      "Epoch [732/1000], Loss: 0.0932\n",
      "Epoch [732/1000], Loss: 0.0576\n",
      "Epoch [732/1000], Loss: 0.0384\n",
      "Epoch [732/1000], Loss: 0.0564\n",
      "Epoch [732/1000], Loss: 0.0421\n",
      "Epoch [732/1000], Loss: 0.0470\n",
      "tensor(2.4313, grad_fn=<MeanBackward0>)\n",
      "732\n",
      "Epoch [733/1000], Loss: 0.0616\n",
      "Epoch [733/1000], Loss: 0.0561\n",
      "Epoch [733/1000], Loss: 0.0461\n",
      "Epoch [733/1000], Loss: 0.0919\n",
      "Epoch [733/1000], Loss: 0.1163\n",
      "Epoch [733/1000], Loss: 0.1223\n",
      "Epoch [733/1000], Loss: 0.0750\n",
      "Epoch [733/1000], Loss: 0.0468\n",
      "Epoch [733/1000], Loss: 0.0691\n",
      "Epoch [733/1000], Loss: 0.0715\n",
      "Epoch [733/1000], Loss: 0.0558\n",
      "tensor(2.4519, grad_fn=<MeanBackward0>)\n",
      "733\n",
      "Epoch [734/1000], Loss: 0.0543\n",
      "Epoch [734/1000], Loss: 0.0681\n",
      "Epoch [734/1000], Loss: 0.0825\n",
      "Epoch [734/1000], Loss: 0.0778\n",
      "Epoch [734/1000], Loss: 0.0723\n",
      "Epoch [734/1000], Loss: 0.1064\n",
      "Epoch [734/1000], Loss: 0.1066\n",
      "Epoch [734/1000], Loss: 0.1219\n",
      "Epoch [734/1000], Loss: 0.0865\n",
      "Epoch [734/1000], Loss: 0.0717\n",
      "Epoch [734/1000], Loss: 0.1519\n",
      "tensor(2.5614, grad_fn=<MeanBackward0>)\n",
      "734\n",
      "Epoch [735/1000], Loss: 0.2229\n",
      "Epoch [735/1000], Loss: 0.2258\n",
      "Epoch [735/1000], Loss: 0.1916\n",
      "Epoch [735/1000], Loss: 0.1041\n",
      "Epoch [735/1000], Loss: 0.1362\n",
      "Epoch [735/1000], Loss: 0.2340\n",
      "Epoch [735/1000], Loss: 0.2983\n",
      "Epoch [735/1000], Loss: 0.3429\n",
      "Epoch [735/1000], Loss: 0.3570\n",
      "Epoch [735/1000], Loss: 0.3326\n",
      "Epoch [735/1000], Loss: 0.1929\n",
      "tensor(2.5586, grad_fn=<MeanBackward0>)\n",
      "735\n",
      "Epoch [736/1000], Loss: 0.1406\n",
      "Epoch [736/1000], Loss: 0.3383\n",
      "Epoch [736/1000], Loss: 0.5391\n",
      "Epoch [736/1000], Loss: 0.5693\n",
      "Epoch [736/1000], Loss: 0.4966\n",
      "Epoch [736/1000], Loss: 0.3215\n",
      "Epoch [736/1000], Loss: 0.1312\n",
      "Epoch [736/1000], Loss: 0.2544\n",
      "Epoch [736/1000], Loss: 0.4386\n",
      "Epoch [736/1000], Loss: 0.5105\n",
      "Epoch [736/1000], Loss: 0.5404\n",
      "tensor(2.1282, grad_fn=<MeanBackward0>)\n",
      "736\n",
      "Epoch [737/1000], Loss: 0.4537\n",
      "Epoch [737/1000], Loss: 0.3075\n",
      "Epoch [737/1000], Loss: 0.2209\n",
      "Epoch [737/1000], Loss: 0.1949\n",
      "Epoch [737/1000], Loss: 0.2979\n",
      "Epoch [737/1000], Loss: 0.3110\n",
      "Epoch [737/1000], Loss: 0.3741\n",
      "Epoch [737/1000], Loss: 0.3070\n",
      "Epoch [737/1000], Loss: 0.2338\n",
      "Epoch [737/1000], Loss: 0.1767\n",
      "Epoch [737/1000], Loss: 0.1566\n",
      "tensor(2.2753, grad_fn=<MeanBackward0>)\n",
      "737\n",
      "Epoch [738/1000], Loss: 0.2204\n",
      "Epoch [738/1000], Loss: 0.2509\n",
      "Epoch [738/1000], Loss: 0.1979\n",
      "Epoch [738/1000], Loss: 0.1606\n",
      "Epoch [738/1000], Loss: 0.1268\n",
      "Epoch [738/1000], Loss: 0.1369\n",
      "Epoch [738/1000], Loss: 0.1848\n",
      "Epoch [738/1000], Loss: 0.1464\n",
      "Epoch [738/1000], Loss: 0.0993\n",
      "Epoch [738/1000], Loss: 0.1037\n",
      "Epoch [738/1000], Loss: 0.1254\n",
      "tensor(2.3553, grad_fn=<MeanBackward0>)\n",
      "738\n",
      "Epoch [739/1000], Loss: 0.1268\n",
      "Epoch [739/1000], Loss: 0.1020\n",
      "Epoch [739/1000], Loss: 0.1099\n",
      "Epoch [739/1000], Loss: 0.1081\n",
      "Epoch [739/1000], Loss: 0.0859\n",
      "Epoch [739/1000], Loss: 0.0878\n",
      "Epoch [739/1000], Loss: 0.0721\n",
      "Epoch [739/1000], Loss: 0.0664\n",
      "Epoch [739/1000], Loss: 0.0581\n",
      "Epoch [739/1000], Loss: 0.0605\n",
      "Epoch [739/1000], Loss: 0.0613\n",
      "tensor(2.4229, grad_fn=<MeanBackward0>)\n",
      "739\n",
      "Epoch [740/1000], Loss: 0.0509\n",
      "Epoch [740/1000], Loss: 0.0504\n",
      "Epoch [740/1000], Loss: 0.0549\n",
      "Epoch [740/1000], Loss: 0.0537\n",
      "Epoch [740/1000], Loss: 0.0690\n",
      "Epoch [740/1000], Loss: 0.0864\n",
      "Epoch [740/1000], Loss: 0.0790\n",
      "Epoch [740/1000], Loss: 0.0814\n",
      "Epoch [740/1000], Loss: 0.0757\n",
      "Epoch [740/1000], Loss: 0.0588\n",
      "Epoch [740/1000], Loss: 0.0948\n",
      "tensor(2.4714, grad_fn=<MeanBackward0>)\n",
      "740\n",
      "Epoch [741/1000], Loss: 0.1228\n",
      "Epoch [741/1000], Loss: 0.1175\n",
      "Epoch [741/1000], Loss: 0.1063\n",
      "Epoch [741/1000], Loss: 0.0955\n",
      "Epoch [741/1000], Loss: 0.0854\n",
      "Epoch [741/1000], Loss: 0.1096\n",
      "Epoch [741/1000], Loss: 0.1104\n",
      "Epoch [741/1000], Loss: 0.0957\n",
      "Epoch [741/1000], Loss: 0.1104\n",
      "Epoch [741/1000], Loss: 0.0942\n",
      "Epoch [741/1000], Loss: 0.1178\n",
      "tensor(2.5635, grad_fn=<MeanBackward0>)\n",
      "741\n",
      "Epoch [742/1000], Loss: 0.1399\n",
      "Epoch [742/1000], Loss: 0.1307\n",
      "Epoch [742/1000], Loss: 0.1220\n",
      "Epoch [742/1000], Loss: 0.0642\n",
      "Epoch [742/1000], Loss: 0.0797\n",
      "Epoch [742/1000], Loss: 0.1447\n",
      "Epoch [742/1000], Loss: 0.1645\n",
      "Epoch [742/1000], Loss: 0.1992\n",
      "Epoch [742/1000], Loss: 0.1659\n",
      "Epoch [742/1000], Loss: 0.1275\n",
      "Epoch [742/1000], Loss: 0.0751\n",
      "tensor(2.5827, grad_fn=<MeanBackward0>)\n",
      "742\n",
      "Epoch [743/1000], Loss: 0.1962\n",
      "Epoch [743/1000], Loss: 0.3007\n",
      "Epoch [743/1000], Loss: 0.3778\n",
      "Epoch [743/1000], Loss: 0.3503\n",
      "Epoch [743/1000], Loss: 0.2665\n",
      "Epoch [743/1000], Loss: 0.1012\n",
      "Epoch [743/1000], Loss: 0.1500\n",
      "Epoch [743/1000], Loss: 0.3032\n",
      "Epoch [743/1000], Loss: 0.4461\n",
      "Epoch [743/1000], Loss: 0.4827\n",
      "Epoch [743/1000], Loss: 0.4780\n",
      "tensor(2.2468, grad_fn=<MeanBackward0>)\n",
      "743\n",
      "Epoch [744/1000], Loss: 0.3651\n",
      "Epoch [744/1000], Loss: 0.1812\n",
      "Epoch [744/1000], Loss: 0.1319\n",
      "Epoch [744/1000], Loss: 0.2973\n",
      "Epoch [744/1000], Loss: 0.3904\n",
      "Epoch [744/1000], Loss: 0.3649\n",
      "Epoch [744/1000], Loss: 0.3199\n",
      "Epoch [744/1000], Loss: 0.1748\n",
      "Epoch [744/1000], Loss: 0.2135\n",
      "Epoch [744/1000], Loss: 0.2252\n",
      "Epoch [744/1000], Loss: 0.3009\n",
      "tensor(2.2085, grad_fn=<MeanBackward0>)\n",
      "744\n",
      "Epoch [745/1000], Loss: 0.3307\n",
      "Epoch [745/1000], Loss: 0.2840\n",
      "Epoch [745/1000], Loss: 0.2434\n",
      "Epoch [745/1000], Loss: 0.1395\n",
      "Epoch [745/1000], Loss: 0.1798\n",
      "Epoch [745/1000], Loss: 0.2342\n",
      "Epoch [745/1000], Loss: 0.3240\n",
      "Epoch [745/1000], Loss: 0.2834\n",
      "Epoch [745/1000], Loss: 0.2243\n",
      "Epoch [745/1000], Loss: 0.1380\n",
      "Epoch [745/1000], Loss: 0.1301\n",
      "tensor(2.2162, grad_fn=<MeanBackward0>)\n",
      "745\n",
      "Epoch [746/1000], Loss: 0.2794\n",
      "Epoch [746/1000], Loss: 0.3247\n",
      "Epoch [746/1000], Loss: 0.3032\n",
      "Epoch [746/1000], Loss: 0.2635\n",
      "Epoch [746/1000], Loss: 0.1638\n",
      "Epoch [746/1000], Loss: 0.1342\n",
      "Epoch [746/1000], Loss: 0.2230\n",
      "Epoch [746/1000], Loss: 0.2544\n",
      "Epoch [746/1000], Loss: 0.2279\n",
      "Epoch [746/1000], Loss: 0.1692\n",
      "Epoch [746/1000], Loss: 0.0879\n",
      "tensor(2.2479, grad_fn=<MeanBackward0>)\n",
      "746\n",
      "Epoch [747/1000], Loss: 0.1688\n",
      "Epoch [747/1000], Loss: 0.1970\n",
      "Epoch [747/1000], Loss: 0.2147\n",
      "Epoch [747/1000], Loss: 0.1891\n",
      "Epoch [747/1000], Loss: 0.1064\n",
      "Epoch [747/1000], Loss: 0.0968\n",
      "Epoch [747/1000], Loss: 0.1418\n",
      "Epoch [747/1000], Loss: 0.1695\n",
      "Epoch [747/1000], Loss: 0.1674\n",
      "Epoch [747/1000], Loss: 0.1239\n",
      "Epoch [747/1000], Loss: 0.0795\n",
      "tensor(2.3063, grad_fn=<MeanBackward0>)\n",
      "747\n",
      "Epoch [748/1000], Loss: 0.1323\n",
      "Epoch [748/1000], Loss: 0.1369\n",
      "Epoch [748/1000], Loss: 0.1198\n",
      "Epoch [748/1000], Loss: 0.1079\n",
      "Epoch [748/1000], Loss: 0.0807\n",
      "Epoch [748/1000], Loss: 0.0705\n",
      "Epoch [748/1000], Loss: 0.0898\n",
      "Epoch [748/1000], Loss: 0.0958\n",
      "Epoch [748/1000], Loss: 0.1246\n",
      "Epoch [748/1000], Loss: 0.1060\n",
      "Epoch [748/1000], Loss: 0.0690\n",
      "tensor(2.3428, grad_fn=<MeanBackward0>)\n",
      "748\n",
      "Epoch [749/1000], Loss: 0.0587\n",
      "Epoch [749/1000], Loss: 0.0621\n",
      "Epoch [749/1000], Loss: 0.0789\n",
      "Epoch [749/1000], Loss: 0.0850\n",
      "Epoch [749/1000], Loss: 0.0983\n",
      "Epoch [749/1000], Loss: 0.0933\n",
      "Epoch [749/1000], Loss: 0.0773\n",
      "Epoch [749/1000], Loss: 0.0757\n",
      "Epoch [749/1000], Loss: 0.0681\n",
      "Epoch [749/1000], Loss: 0.0769\n",
      "Epoch [749/1000], Loss: 0.0911\n",
      "tensor(2.4054, grad_fn=<MeanBackward0>)\n",
      "749\n",
      "Epoch [750/1000], Loss: 0.0810\n",
      "Epoch [750/1000], Loss: 0.0585\n",
      "Epoch [750/1000], Loss: 0.0707\n",
      "Epoch [750/1000], Loss: 0.0528\n",
      "Epoch [750/1000], Loss: 0.0691\n",
      "Epoch [750/1000], Loss: 0.0729\n",
      "Epoch [750/1000], Loss: 0.0681\n",
      "Epoch [750/1000], Loss: 0.0563\n",
      "Epoch [750/1000], Loss: 0.0504\n",
      "Epoch [750/1000], Loss: 0.0515\n",
      "Epoch [750/1000], Loss: 0.0761\n",
      "tensor(2.3897, grad_fn=<MeanBackward0>)\n",
      "750\n",
      "Epoch [751/1000], Loss: 0.0835\n",
      "Epoch [751/1000], Loss: 0.0808\n",
      "Epoch [751/1000], Loss: 0.1025\n",
      "Epoch [751/1000], Loss: 0.1096\n",
      "Epoch [751/1000], Loss: 0.1078\n",
      "Epoch [751/1000], Loss: 0.0726\n",
      "Epoch [751/1000], Loss: 0.0563\n",
      "Epoch [751/1000], Loss: 0.0685\n",
      "Epoch [751/1000], Loss: 0.0858\n",
      "Epoch [751/1000], Loss: 0.0928\n",
      "Epoch [751/1000], Loss: 0.1135\n",
      "tensor(2.4643, grad_fn=<MeanBackward0>)\n",
      "751\n",
      "Epoch [752/1000], Loss: 0.0971\n",
      "Epoch [752/1000], Loss: 0.1218\n",
      "Epoch [752/1000], Loss: 0.0766\n",
      "Epoch [752/1000], Loss: 0.0702\n",
      "Epoch [752/1000], Loss: 0.1416\n",
      "Epoch [752/1000], Loss: 0.1970\n",
      "Epoch [752/1000], Loss: 0.2071\n",
      "Epoch [752/1000], Loss: 0.1488\n",
      "Epoch [752/1000], Loss: 0.0836\n",
      "Epoch [752/1000], Loss: 0.0973\n",
      "Epoch [752/1000], Loss: 0.1399\n",
      "tensor(2.5705, grad_fn=<MeanBackward0>)\n",
      "752\n",
      "Epoch [753/1000], Loss: 0.1986\n",
      "Epoch [753/1000], Loss: 0.2128\n",
      "Epoch [753/1000], Loss: 0.1992\n",
      "Epoch [753/1000], Loss: 0.1307\n",
      "Epoch [753/1000], Loss: 0.0932\n",
      "Epoch [753/1000], Loss: 0.2087\n",
      "Epoch [753/1000], Loss: 0.2667\n",
      "Epoch [753/1000], Loss: 0.3324\n",
      "Epoch [753/1000], Loss: 0.3476\n",
      "Epoch [753/1000], Loss: 0.3335\n",
      "Epoch [753/1000], Loss: 0.2223\n",
      "tensor(2.4786, grad_fn=<MeanBackward0>)\n",
      "753\n",
      "Epoch [754/1000], Loss: 0.0970\n",
      "Epoch [754/1000], Loss: 0.2711\n",
      "Epoch [754/1000], Loss: 0.4197\n",
      "Epoch [754/1000], Loss: 0.4900\n",
      "Epoch [754/1000], Loss: 0.4259\n",
      "Epoch [754/1000], Loss: 0.2739\n",
      "Epoch [754/1000], Loss: 0.0873\n",
      "Epoch [754/1000], Loss: 0.1974\n",
      "Epoch [754/1000], Loss: 0.3256\n",
      "Epoch [754/1000], Loss: 0.3977\n",
      "Epoch [754/1000], Loss: 0.4168\n",
      "tensor(2.1876, grad_fn=<MeanBackward0>)\n",
      "754\n",
      "Epoch [755/1000], Loss: 0.3242\n",
      "Epoch [755/1000], Loss: 0.1857\n",
      "Epoch [755/1000], Loss: 0.1125\n",
      "Epoch [755/1000], Loss: 0.2030\n",
      "Epoch [755/1000], Loss: 0.2731\n",
      "Epoch [755/1000], Loss: 0.2438\n",
      "Epoch [755/1000], Loss: 0.2179\n",
      "Epoch [755/1000], Loss: 0.1516\n",
      "Epoch [755/1000], Loss: 0.1435\n",
      "Epoch [755/1000], Loss: 0.1537\n",
      "Epoch [755/1000], Loss: 0.1767\n",
      "tensor(2.2534, grad_fn=<MeanBackward0>)\n",
      "755\n",
      "Epoch [756/1000], Loss: 0.2156\n",
      "Epoch [756/1000], Loss: 0.1651\n",
      "Epoch [756/1000], Loss: 0.1184\n",
      "Epoch [756/1000], Loss: 0.1238\n",
      "Epoch [756/1000], Loss: 0.1758\n",
      "Epoch [756/1000], Loss: 0.1641\n",
      "Epoch [756/1000], Loss: 0.1528\n",
      "Epoch [756/1000], Loss: 0.1117\n",
      "Epoch [756/1000], Loss: 0.1129\n",
      "Epoch [756/1000], Loss: 0.1079\n",
      "Epoch [756/1000], Loss: 0.0978\n",
      "tensor(2.3034, grad_fn=<MeanBackward0>)\n",
      "756\n",
      "Epoch [757/1000], Loss: 0.1347\n",
      "Epoch [757/1000], Loss: 0.1360\n",
      "Epoch [757/1000], Loss: 0.1078\n",
      "Epoch [757/1000], Loss: 0.0698\n",
      "Epoch [757/1000], Loss: 0.1160\n",
      "Epoch [757/1000], Loss: 0.1152\n",
      "Epoch [757/1000], Loss: 0.1141\n",
      "Epoch [757/1000], Loss: 0.0644\n",
      "Epoch [757/1000], Loss: 0.0671\n",
      "Epoch [757/1000], Loss: 0.0940\n",
      "Epoch [757/1000], Loss: 0.0961\n",
      "tensor(2.3254, grad_fn=<MeanBackward0>)\n",
      "757\n",
      "Epoch [758/1000], Loss: 0.0935\n",
      "Epoch [758/1000], Loss: 0.0944\n",
      "Epoch [758/1000], Loss: 0.0772\n",
      "Epoch [758/1000], Loss: 0.0617\n",
      "Epoch [758/1000], Loss: 0.0696\n",
      "Epoch [758/1000], Loss: 0.0548\n",
      "Epoch [758/1000], Loss: 0.0725\n",
      "Epoch [758/1000], Loss: 0.0797\n",
      "Epoch [758/1000], Loss: 0.0628\n",
      "Epoch [758/1000], Loss: 0.0776\n",
      "Epoch [758/1000], Loss: 0.1054\n",
      "tensor(2.3049, grad_fn=<MeanBackward0>)\n",
      "758\n",
      "Epoch [759/1000], Loss: 0.0917\n",
      "Epoch [759/1000], Loss: 0.0589\n",
      "Epoch [759/1000], Loss: 0.0572\n",
      "Epoch [759/1000], Loss: 0.0435\n",
      "Epoch [759/1000], Loss: 0.0604\n",
      "Epoch [759/1000], Loss: 0.0684\n",
      "Epoch [759/1000], Loss: 0.0554\n",
      "Epoch [759/1000], Loss: 0.0638\n",
      "Epoch [759/1000], Loss: 0.0983\n",
      "Epoch [759/1000], Loss: 0.0498\n",
      "Epoch [759/1000], Loss: 0.0772\n",
      "tensor(2.2772, grad_fn=<MeanBackward0>)\n",
      "759\n",
      "Epoch [760/1000], Loss: 0.1244\n",
      "Epoch [760/1000], Loss: 0.0942\n",
      "Epoch [760/1000], Loss: 0.0508\n",
      "Epoch [760/1000], Loss: 0.0893\n",
      "Epoch [760/1000], Loss: 0.0722\n",
      "Epoch [760/1000], Loss: 0.0584\n",
      "Epoch [760/1000], Loss: 0.0718\n",
      "Epoch [760/1000], Loss: 0.0689\n",
      "Epoch [760/1000], Loss: 0.1017\n",
      "Epoch [760/1000], Loss: 0.1639\n",
      "Epoch [760/1000], Loss: 0.1517\n",
      "tensor(2.3214, grad_fn=<MeanBackward0>)\n",
      "760\n",
      "Epoch [761/1000], Loss: 0.0603\n",
      "Epoch [761/1000], Loss: 0.1511\n",
      "Epoch [761/1000], Loss: 0.1815\n",
      "Epoch [761/1000], Loss: 0.1774\n",
      "Epoch [761/1000], Loss: 0.0847\n",
      "Epoch [761/1000], Loss: 0.0762\n",
      "Epoch [761/1000], Loss: 0.1055\n",
      "Epoch [761/1000], Loss: 0.0986\n",
      "Epoch [761/1000], Loss: 0.0706\n",
      "Epoch [761/1000], Loss: 0.0704\n",
      "Epoch [761/1000], Loss: 0.0500\n",
      "tensor(2.2660, grad_fn=<MeanBackward0>)\n",
      "761\n",
      "Epoch [762/1000], Loss: 0.0804\n",
      "Epoch [762/1000], Loss: 0.1035\n",
      "Epoch [762/1000], Loss: 0.1210\n",
      "Epoch [762/1000], Loss: 0.1007\n",
      "Epoch [762/1000], Loss: 0.0729\n",
      "Epoch [762/1000], Loss: 0.0817\n",
      "Epoch [762/1000], Loss: 0.0830\n",
      "Epoch [762/1000], Loss: 0.0665\n",
      "Epoch [762/1000], Loss: 0.0645\n",
      "Epoch [762/1000], Loss: 0.0636\n",
      "Epoch [762/1000], Loss: 0.0588\n",
      "tensor(2.2556, grad_fn=<MeanBackward0>)\n",
      "762\n",
      "Epoch [763/1000], Loss: 0.0700\n",
      "Epoch [763/1000], Loss: 0.0946\n",
      "Epoch [763/1000], Loss: 0.0961\n",
      "Epoch [763/1000], Loss: 0.0903\n",
      "Epoch [763/1000], Loss: 0.0603\n",
      "Epoch [763/1000], Loss: 0.0685\n",
      "Epoch [763/1000], Loss: 0.0719\n",
      "Epoch [763/1000], Loss: 0.0680\n",
      "Epoch [763/1000], Loss: 0.0916\n",
      "Epoch [763/1000], Loss: 0.1149\n",
      "Epoch [763/1000], Loss: 0.0631\n",
      "tensor(2.2730, grad_fn=<MeanBackward0>)\n",
      "763\n",
      "Epoch [764/1000], Loss: 0.0792\n",
      "Epoch [764/1000], Loss: 0.1246\n",
      "Epoch [764/1000], Loss: 0.1894\n",
      "Epoch [764/1000], Loss: 0.2170\n",
      "Epoch [764/1000], Loss: 0.1638\n",
      "Epoch [764/1000], Loss: 0.1053\n",
      "Epoch [764/1000], Loss: 0.0809\n",
      "Epoch [764/1000], Loss: 0.1898\n",
      "Epoch [764/1000], Loss: 0.2456\n",
      "Epoch [764/1000], Loss: 0.2161\n",
      "Epoch [764/1000], Loss: 0.1593\n",
      "tensor(2.3890, grad_fn=<MeanBackward0>)\n",
      "764\n",
      "Epoch [765/1000], Loss: 0.0732\n",
      "Epoch [765/1000], Loss: 0.1065\n",
      "Epoch [765/1000], Loss: 0.2014\n",
      "Epoch [765/1000], Loss: 0.2422\n",
      "Epoch [765/1000], Loss: 0.2312\n",
      "Epoch [765/1000], Loss: 0.2151\n",
      "Epoch [765/1000], Loss: 0.1656\n",
      "Epoch [765/1000], Loss: 0.1134\n",
      "Epoch [765/1000], Loss: 0.1412\n",
      "Epoch [765/1000], Loss: 0.2608\n",
      "Epoch [765/1000], Loss: 0.2878\n",
      "tensor(2.5662, grad_fn=<MeanBackward0>)\n",
      "765\n",
      "Epoch [766/1000], Loss: 0.3221\n",
      "Epoch [766/1000], Loss: 0.2843\n",
      "Epoch [766/1000], Loss: 0.1483\n",
      "Epoch [766/1000], Loss: 0.1304\n",
      "Epoch [766/1000], Loss: 0.2104\n",
      "Epoch [766/1000], Loss: 0.3012\n",
      "Epoch [766/1000], Loss: 0.3841\n",
      "Epoch [766/1000], Loss: 0.4138\n",
      "Epoch [766/1000], Loss: 0.3664\n",
      "Epoch [766/1000], Loss: 0.2574\n",
      "Epoch [766/1000], Loss: 0.1628\n",
      "tensor(2.5871, grad_fn=<MeanBackward0>)\n",
      "766\n",
      "Epoch [767/1000], Loss: 0.2559\n",
      "Epoch [767/1000], Loss: 0.4533\n",
      "Epoch [767/1000], Loss: 0.5613\n",
      "Epoch [767/1000], Loss: 0.5627\n",
      "Epoch [767/1000], Loss: 0.4669\n",
      "Epoch [767/1000], Loss: 0.2786\n",
      "Epoch [767/1000], Loss: 0.1707\n",
      "Epoch [767/1000], Loss: 0.3045\n",
      "Epoch [767/1000], Loss: 0.4786\n",
      "Epoch [767/1000], Loss: 0.5266\n",
      "Epoch [767/1000], Loss: 0.5584\n",
      "tensor(2.1696, grad_fn=<MeanBackward0>)\n",
      "767\n",
      "Epoch [768/1000], Loss: 0.3485\n",
      "Epoch [768/1000], Loss: 0.1660\n",
      "Epoch [768/1000], Loss: 0.1310\n",
      "Epoch [768/1000], Loss: 0.2423\n",
      "Epoch [768/1000], Loss: 0.2893\n",
      "Epoch [768/1000], Loss: 0.2160\n",
      "Epoch [768/1000], Loss: 0.1560\n",
      "Epoch [768/1000], Loss: 0.1196\n",
      "Epoch [768/1000], Loss: 0.1430\n",
      "Epoch [768/1000], Loss: 0.1113\n",
      "Epoch [768/1000], Loss: 0.0840\n",
      "tensor(2.3602, grad_fn=<MeanBackward0>)\n",
      "768\n",
      "Epoch [769/1000], Loss: 0.1115\n",
      "Epoch [769/1000], Loss: 0.1061\n",
      "Epoch [769/1000], Loss: 0.0928\n",
      "Epoch [769/1000], Loss: 0.0950\n",
      "Epoch [769/1000], Loss: 0.0802\n",
      "Epoch [769/1000], Loss: 0.0697\n",
      "Epoch [769/1000], Loss: 0.0807\n",
      "Epoch [769/1000], Loss: 0.0972\n",
      "Epoch [769/1000], Loss: 0.0773\n",
      "Epoch [769/1000], Loss: 0.0671\n",
      "Epoch [769/1000], Loss: 0.0634\n",
      "tensor(2.4048, grad_fn=<MeanBackward0>)\n",
      "769\n",
      "Epoch [770/1000], Loss: 0.0665\n",
      "Epoch [770/1000], Loss: 0.0654\n",
      "Epoch [770/1000], Loss: 0.0602\n",
      "Epoch [770/1000], Loss: 0.0571\n",
      "Epoch [770/1000], Loss: 0.0490\n",
      "Epoch [770/1000], Loss: 0.0673\n",
      "Epoch [770/1000], Loss: 0.0496\n",
      "Epoch [770/1000], Loss: 0.0658\n",
      "Epoch [770/1000], Loss: 0.0797\n",
      "Epoch [770/1000], Loss: 0.0534\n",
      "Epoch [770/1000], Loss: 0.0408\n",
      "tensor(2.3666, grad_fn=<MeanBackward0>)\n",
      "770\n",
      "Epoch [771/1000], Loss: 0.0380\n",
      "Epoch [771/1000], Loss: 0.0506\n",
      "Epoch [771/1000], Loss: 0.0579\n",
      "Epoch [771/1000], Loss: 0.0537\n",
      "Epoch [771/1000], Loss: 0.0391\n",
      "Epoch [771/1000], Loss: 0.0477\n",
      "Epoch [771/1000], Loss: 0.0452\n",
      "Epoch [771/1000], Loss: 0.0728\n",
      "Epoch [771/1000], Loss: 0.0692\n",
      "Epoch [771/1000], Loss: 0.0610\n",
      "Epoch [771/1000], Loss: 0.0332\n",
      "tensor(2.3278, grad_fn=<MeanBackward0>)\n",
      "771\n",
      "Epoch [772/1000], Loss: 0.0568\n",
      "Epoch [772/1000], Loss: 0.0417\n",
      "Epoch [772/1000], Loss: 0.0411\n",
      "Epoch [772/1000], Loss: 0.0460\n",
      "Epoch [772/1000], Loss: 0.0472\n",
      "Epoch [772/1000], Loss: 0.0748\n",
      "Epoch [772/1000], Loss: 0.0605\n",
      "Epoch [772/1000], Loss: 0.0498\n",
      "Epoch [772/1000], Loss: 0.1063\n",
      "Epoch [772/1000], Loss: 0.1210\n",
      "Epoch [772/1000], Loss: 0.1003\n",
      "tensor(2.4019, grad_fn=<MeanBackward0>)\n",
      "772\n",
      "Epoch [773/1000], Loss: 0.0535\n",
      "Epoch [773/1000], Loss: 0.0649\n",
      "Epoch [773/1000], Loss: 0.0769\n",
      "Epoch [773/1000], Loss: 0.0647\n",
      "Epoch [773/1000], Loss: 0.0528\n",
      "Epoch [773/1000], Loss: 0.0541\n",
      "Epoch [773/1000], Loss: 0.0807\n",
      "Epoch [773/1000], Loss: 0.0828\n",
      "Epoch [773/1000], Loss: 0.0664\n",
      "Epoch [773/1000], Loss: 0.1471\n",
      "Epoch [773/1000], Loss: 0.2261\n",
      "tensor(2.5169, grad_fn=<MeanBackward0>)\n",
      "773\n",
      "Epoch [774/1000], Loss: 0.2129\n",
      "Epoch [774/1000], Loss: 0.1488\n",
      "Epoch [774/1000], Loss: 0.0829\n",
      "Epoch [774/1000], Loss: 0.1357\n",
      "Epoch [774/1000], Loss: 0.2042\n",
      "Epoch [774/1000], Loss: 0.2120\n",
      "Epoch [774/1000], Loss: 0.2191\n",
      "Epoch [774/1000], Loss: 0.1990\n",
      "Epoch [774/1000], Loss: 0.1933\n",
      "Epoch [774/1000], Loss: 0.1134\n",
      "Epoch [774/1000], Loss: 0.1603\n",
      "tensor(2.6544, grad_fn=<MeanBackward0>)\n",
      "774\n",
      "Epoch [775/1000], Loss: 0.3040\n",
      "Epoch [775/1000], Loss: 0.3775\n",
      "Epoch [775/1000], Loss: 0.3922\n",
      "Epoch [775/1000], Loss: 0.3334\n",
      "Epoch [775/1000], Loss: 0.1932\n",
      "Epoch [775/1000], Loss: 0.2009\n",
      "Epoch [775/1000], Loss: 0.3413\n",
      "Epoch [775/1000], Loss: 0.5026\n",
      "Epoch [775/1000], Loss: 0.6208\n",
      "Epoch [775/1000], Loss: 0.6153\n",
      "Epoch [775/1000], Loss: 0.5726\n",
      "tensor(2.2276, grad_fn=<MeanBackward0>)\n",
      "775\n",
      "Epoch [776/1000], Loss: 0.3160\n",
      "Epoch [776/1000], Loss: 0.1757\n",
      "Epoch [776/1000], Loss: 0.2485\n",
      "Epoch [776/1000], Loss: 0.3848\n",
      "Epoch [776/1000], Loss: 0.4445\n",
      "Epoch [776/1000], Loss: 0.3660\n",
      "Epoch [776/1000], Loss: 0.3033\n",
      "Epoch [776/1000], Loss: 0.2007\n",
      "Epoch [776/1000], Loss: 0.2272\n",
      "Epoch [776/1000], Loss: 0.2547\n",
      "Epoch [776/1000], Loss: 0.1989\n",
      "tensor(2.3141, grad_fn=<MeanBackward0>)\n",
      "776\n",
      "Epoch [777/1000], Loss: 0.1793\n",
      "Epoch [777/1000], Loss: 0.1599\n",
      "Epoch [777/1000], Loss: 0.1841\n",
      "Epoch [777/1000], Loss: 0.1629\n",
      "Epoch [777/1000], Loss: 0.1775\n",
      "Epoch [777/1000], Loss: 0.1882\n",
      "Epoch [777/1000], Loss: 0.1720\n",
      "Epoch [777/1000], Loss: 0.1431\n",
      "Epoch [777/1000], Loss: 0.1204\n",
      "Epoch [777/1000], Loss: 0.1106\n",
      "Epoch [777/1000], Loss: 0.0972\n",
      "tensor(2.3800, grad_fn=<MeanBackward0>)\n",
      "777\n",
      "Epoch [778/1000], Loss: 0.1096\n",
      "Epoch [778/1000], Loss: 0.1038\n",
      "Epoch [778/1000], Loss: 0.1097\n",
      "Epoch [778/1000], Loss: 0.1057\n",
      "Epoch [778/1000], Loss: 0.0976\n",
      "Epoch [778/1000], Loss: 0.0926\n",
      "Epoch [778/1000], Loss: 0.1014\n",
      "Epoch [778/1000], Loss: 0.1230\n",
      "Epoch [778/1000], Loss: 0.1494\n",
      "Epoch [778/1000], Loss: 0.1213\n",
      "Epoch [778/1000], Loss: 0.0921\n",
      "tensor(2.4991, grad_fn=<MeanBackward0>)\n",
      "778\n",
      "Epoch [779/1000], Loss: 0.1541\n",
      "Epoch [779/1000], Loss: 0.2023\n",
      "Epoch [779/1000], Loss: 0.1732\n",
      "Epoch [779/1000], Loss: 0.1169\n",
      "Epoch [779/1000], Loss: 0.0805\n",
      "Epoch [779/1000], Loss: 0.1116\n",
      "Epoch [779/1000], Loss: 0.1378\n",
      "Epoch [779/1000], Loss: 0.1644\n",
      "Epoch [779/1000], Loss: 0.1716\n",
      "Epoch [779/1000], Loss: 0.1584\n",
      "Epoch [779/1000], Loss: 0.1106\n",
      "tensor(2.5504, grad_fn=<MeanBackward0>)\n",
      "779\n",
      "Epoch [780/1000], Loss: 0.1705\n",
      "Epoch [780/1000], Loss: 0.2336\n",
      "Epoch [780/1000], Loss: 0.2807\n",
      "Epoch [780/1000], Loss: 0.2339\n",
      "Epoch [780/1000], Loss: 0.1523\n",
      "Epoch [780/1000], Loss: 0.1050\n",
      "Epoch [780/1000], Loss: 0.2143\n",
      "Epoch [780/1000], Loss: 0.2982\n",
      "Epoch [780/1000], Loss: 0.3694\n",
      "Epoch [780/1000], Loss: 0.3859\n",
      "Epoch [780/1000], Loss: 0.3163\n",
      "tensor(2.3669, grad_fn=<MeanBackward0>)\n",
      "780\n",
      "Epoch [781/1000], Loss: 0.1217\n",
      "Epoch [781/1000], Loss: 0.1637\n",
      "Epoch [781/1000], Loss: 0.3209\n",
      "Epoch [781/1000], Loss: 0.4027\n",
      "Epoch [781/1000], Loss: 0.4056\n",
      "Epoch [781/1000], Loss: 0.3448\n",
      "Epoch [781/1000], Loss: 0.2268\n",
      "Epoch [781/1000], Loss: 0.1345\n",
      "Epoch [781/1000], Loss: 0.2361\n",
      "Epoch [781/1000], Loss: 0.2995\n",
      "Epoch [781/1000], Loss: 0.3347\n",
      "tensor(2.2145, grad_fn=<MeanBackward0>)\n",
      "781\n",
      "Epoch [782/1000], Loss: 0.2607\n",
      "Epoch [782/1000], Loss: 0.1997\n",
      "Epoch [782/1000], Loss: 0.1790\n",
      "Epoch [782/1000], Loss: 0.1377\n",
      "Epoch [782/1000], Loss: 0.2060\n",
      "Epoch [782/1000], Loss: 0.2222\n",
      "Epoch [782/1000], Loss: 0.2363\n",
      "Epoch [782/1000], Loss: 0.1842\n",
      "Epoch [782/1000], Loss: 0.1195\n",
      "Epoch [782/1000], Loss: 0.1249\n",
      "Epoch [782/1000], Loss: 0.1263\n",
      "tensor(2.2521, grad_fn=<MeanBackward0>)\n",
      "782\n",
      "Epoch [783/1000], Loss: 0.1689\n",
      "Epoch [783/1000], Loss: 0.2040\n",
      "Epoch [783/1000], Loss: 0.1384\n",
      "Epoch [783/1000], Loss: 0.1017\n",
      "Epoch [783/1000], Loss: 0.1327\n",
      "Epoch [783/1000], Loss: 0.1667\n",
      "Epoch [783/1000], Loss: 0.1907\n",
      "Epoch [783/1000], Loss: 0.1770\n",
      "Epoch [783/1000], Loss: 0.1260\n",
      "Epoch [783/1000], Loss: 0.1070\n",
      "Epoch [783/1000], Loss: 0.1109\n",
      "tensor(2.2254, grad_fn=<MeanBackward0>)\n",
      "783\n",
      "Epoch [784/1000], Loss: 0.1465\n",
      "Epoch [784/1000], Loss: 0.1741\n",
      "Epoch [784/1000], Loss: 0.1810\n",
      "Epoch [784/1000], Loss: 0.1525\n",
      "Epoch [784/1000], Loss: 0.0852\n",
      "Epoch [784/1000], Loss: 0.1150\n",
      "Epoch [784/1000], Loss: 0.1625\n",
      "Epoch [784/1000], Loss: 0.1873\n",
      "Epoch [784/1000], Loss: 0.1505\n",
      "Epoch [784/1000], Loss: 0.1158\n",
      "Epoch [784/1000], Loss: 0.0680\n",
      "tensor(2.2270, grad_fn=<MeanBackward0>)\n",
      "784\n",
      "Epoch [785/1000], Loss: 0.1328\n",
      "Epoch [785/1000], Loss: 0.2023\n",
      "Epoch [785/1000], Loss: 0.2193\n",
      "Epoch [785/1000], Loss: 0.2331\n",
      "Epoch [785/1000], Loss: 0.1551\n",
      "Epoch [785/1000], Loss: 0.0899\n",
      "Epoch [785/1000], Loss: 0.1248\n",
      "Epoch [785/1000], Loss: 0.2166\n",
      "Epoch [785/1000], Loss: 0.2579\n",
      "Epoch [785/1000], Loss: 0.2271\n",
      "Epoch [785/1000], Loss: 0.1389\n",
      "tensor(2.3293, grad_fn=<MeanBackward0>)\n",
      "785\n",
      "Epoch [786/1000], Loss: 0.0948\n",
      "Epoch [786/1000], Loss: 0.1413\n",
      "Epoch [786/1000], Loss: 0.2064\n",
      "Epoch [786/1000], Loss: 0.2455\n",
      "Epoch [786/1000], Loss: 0.2110\n",
      "Epoch [786/1000], Loss: 0.1950\n",
      "Epoch [786/1000], Loss: 0.1392\n",
      "Epoch [786/1000], Loss: 0.0913\n",
      "Epoch [786/1000], Loss: 0.1507\n",
      "Epoch [786/1000], Loss: 0.2472\n",
      "Epoch [786/1000], Loss: 0.2623\n",
      "tensor(2.4992, grad_fn=<MeanBackward0>)\n",
      "786\n",
      "Epoch [787/1000], Loss: 0.2140\n",
      "Epoch [787/1000], Loss: 0.0960\n",
      "Epoch [787/1000], Loss: 0.0936\n",
      "Epoch [787/1000], Loss: 0.1722\n",
      "Epoch [787/1000], Loss: 0.2098\n",
      "Epoch [787/1000], Loss: 0.2086\n",
      "Epoch [787/1000], Loss: 0.2095\n",
      "Epoch [787/1000], Loss: 0.1400\n",
      "Epoch [787/1000], Loss: 0.1099\n",
      "Epoch [787/1000], Loss: 0.1268\n",
      "Epoch [787/1000], Loss: 0.1883\n",
      "tensor(2.5945, grad_fn=<MeanBackward0>)\n",
      "787\n",
      "Epoch [788/1000], Loss: 0.2670\n",
      "Epoch [788/1000], Loss: 0.2783\n",
      "Epoch [788/1000], Loss: 0.2440\n",
      "Epoch [788/1000], Loss: 0.1817\n",
      "Epoch [788/1000], Loss: 0.1464\n",
      "Epoch [788/1000], Loss: 0.2130\n",
      "Epoch [788/1000], Loss: 0.3048\n",
      "Epoch [788/1000], Loss: 0.3685\n",
      "Epoch [788/1000], Loss: 0.4293\n",
      "Epoch [788/1000], Loss: 0.3963\n",
      "Epoch [788/1000], Loss: 0.2871\n",
      "tensor(2.4356, grad_fn=<MeanBackward0>)\n",
      "788\n",
      "Epoch [789/1000], Loss: 0.1393\n",
      "Epoch [789/1000], Loss: 0.2496\n",
      "Epoch [789/1000], Loss: 0.4412\n",
      "Epoch [789/1000], Loss: 0.5020\n",
      "Epoch [789/1000], Loss: 0.4734\n",
      "Epoch [789/1000], Loss: 0.3326\n",
      "Epoch [789/1000], Loss: 0.1571\n",
      "Epoch [789/1000], Loss: 0.1801\n",
      "Epoch [789/1000], Loss: 0.3029\n",
      "Epoch [789/1000], Loss: 0.3258\n",
      "Epoch [789/1000], Loss: 0.3044\n",
      "tensor(2.2232, grad_fn=<MeanBackward0>)\n",
      "789\n",
      "Epoch [790/1000], Loss: 0.1853\n",
      "Epoch [790/1000], Loss: 0.0887\n",
      "Epoch [790/1000], Loss: 0.1217\n",
      "Epoch [790/1000], Loss: 0.1736\n",
      "Epoch [790/1000], Loss: 0.1611\n",
      "Epoch [790/1000], Loss: 0.1349\n",
      "Epoch [790/1000], Loss: 0.0971\n",
      "Epoch [790/1000], Loss: 0.1009\n",
      "Epoch [790/1000], Loss: 0.1143\n",
      "Epoch [790/1000], Loss: 0.1022\n",
      "Epoch [790/1000], Loss: 0.0763\n",
      "tensor(2.4315, grad_fn=<MeanBackward0>)\n",
      "790\n",
      "Epoch [791/1000], Loss: 0.0972\n",
      "Epoch [791/1000], Loss: 0.0968\n",
      "Epoch [791/1000], Loss: 0.0960\n",
      "Epoch [791/1000], Loss: 0.0831\n",
      "Epoch [791/1000], Loss: 0.0639\n",
      "Epoch [791/1000], Loss: 0.0661\n",
      "Epoch [791/1000], Loss: 0.0660\n",
      "Epoch [791/1000], Loss: 0.0683\n",
      "Epoch [791/1000], Loss: 0.0942\n",
      "Epoch [791/1000], Loss: 0.0980\n",
      "Epoch [791/1000], Loss: 0.0847\n",
      "tensor(2.4413, grad_fn=<MeanBackward0>)\n",
      "791\n",
      "Epoch [792/1000], Loss: 0.0712\n",
      "Epoch [792/1000], Loss: 0.0806\n",
      "Epoch [792/1000], Loss: 0.0878\n",
      "Epoch [792/1000], Loss: 0.0729\n",
      "Epoch [792/1000], Loss: 0.0907\n",
      "Epoch [792/1000], Loss: 0.0890\n",
      "Epoch [792/1000], Loss: 0.0888\n",
      "Epoch [792/1000], Loss: 0.0639\n",
      "Epoch [792/1000], Loss: 0.0557\n",
      "Epoch [792/1000], Loss: 0.0676\n",
      "Epoch [792/1000], Loss: 0.0799\n",
      "tensor(2.4254, grad_fn=<MeanBackward0>)\n",
      "792\n",
      "Epoch [793/1000], Loss: 0.0799\n",
      "Epoch [793/1000], Loss: 0.0820\n",
      "Epoch [793/1000], Loss: 0.0819\n",
      "Epoch [793/1000], Loss: 0.0636\n",
      "Epoch [793/1000], Loss: 0.0739\n",
      "Epoch [793/1000], Loss: 0.0775\n",
      "Epoch [793/1000], Loss: 0.0745\n",
      "Epoch [793/1000], Loss: 0.0539\n",
      "Epoch [793/1000], Loss: 0.0667\n",
      "Epoch [793/1000], Loss: 0.0504\n",
      "Epoch [793/1000], Loss: 0.0424\n",
      "tensor(2.4236, grad_fn=<MeanBackward0>)\n",
      "793\n",
      "Epoch [794/1000], Loss: 0.0544\n",
      "Epoch [794/1000], Loss: 0.0627\n",
      "Epoch [794/1000], Loss: 0.0578\n",
      "Epoch [794/1000], Loss: 0.0757\n",
      "Epoch [794/1000], Loss: 0.0491\n",
      "Epoch [794/1000], Loss: 0.0517\n",
      "Epoch [794/1000], Loss: 0.0774\n",
      "Epoch [794/1000], Loss: 0.0794\n",
      "Epoch [794/1000], Loss: 0.0963\n",
      "Epoch [794/1000], Loss: 0.0896\n",
      "Epoch [794/1000], Loss: 0.0693\n",
      "tensor(2.4756, grad_fn=<MeanBackward0>)\n",
      "794\n",
      "Epoch [795/1000], Loss: 0.0729\n",
      "Epoch [795/1000], Loss: 0.0903\n",
      "Epoch [795/1000], Loss: 0.0894\n",
      "Epoch [795/1000], Loss: 0.1145\n",
      "Epoch [795/1000], Loss: 0.1195\n",
      "Epoch [795/1000], Loss: 0.1068\n",
      "Epoch [795/1000], Loss: 0.0922\n",
      "Epoch [795/1000], Loss: 0.1152\n",
      "Epoch [795/1000], Loss: 0.1590\n",
      "Epoch [795/1000], Loss: 0.1731\n",
      "Epoch [795/1000], Loss: 0.1478\n",
      "tensor(2.4908, grad_fn=<MeanBackward0>)\n",
      "795\n",
      "Epoch [796/1000], Loss: 0.1170\n",
      "Epoch [796/1000], Loss: 0.1640\n",
      "Epoch [796/1000], Loss: 0.2809\n",
      "Epoch [796/1000], Loss: 0.3198\n",
      "Epoch [796/1000], Loss: 0.2752\n",
      "Epoch [796/1000], Loss: 0.1852\n",
      "Epoch [796/1000], Loss: 0.0640\n",
      "Epoch [796/1000], Loss: 0.1754\n",
      "Epoch [796/1000], Loss: 0.3123\n",
      "Epoch [796/1000], Loss: 0.3644\n",
      "Epoch [796/1000], Loss: 0.3920\n",
      "tensor(2.1746, grad_fn=<MeanBackward0>)\n",
      "796\n",
      "Epoch [797/1000], Loss: 0.3194\n",
      "Epoch [797/1000], Loss: 0.1822\n",
      "Epoch [797/1000], Loss: 0.1155\n",
      "Epoch [797/1000], Loss: 0.2136\n",
      "Epoch [797/1000], Loss: 0.3281\n",
      "Epoch [797/1000], Loss: 0.3596\n",
      "Epoch [797/1000], Loss: 0.3642\n",
      "Epoch [797/1000], Loss: 0.2918\n",
      "Epoch [797/1000], Loss: 0.1913\n",
      "Epoch [797/1000], Loss: 0.1386\n",
      "Epoch [797/1000], Loss: 0.2042\n",
      "tensor(2.1658, grad_fn=<MeanBackward0>)\n",
      "797\n",
      "Epoch [798/1000], Loss: 0.3096\n",
      "Epoch [798/1000], Loss: 0.3623\n",
      "Epoch [798/1000], Loss: 0.3753\n",
      "Epoch [798/1000], Loss: 0.3195\n",
      "Epoch [798/1000], Loss: 0.2035\n",
      "Epoch [798/1000], Loss: 0.1090\n",
      "Epoch [798/1000], Loss: 0.2162\n",
      "Epoch [798/1000], Loss: 0.3786\n",
      "Epoch [798/1000], Loss: 0.4377\n",
      "Epoch [798/1000], Loss: 0.4461\n",
      "Epoch [798/1000], Loss: 0.3304\n",
      "tensor(2.3633, grad_fn=<MeanBackward0>)\n",
      "798\n",
      "Epoch [799/1000], Loss: 0.1928\n",
      "Epoch [799/1000], Loss: 0.1803\n",
      "Epoch [799/1000], Loss: 0.2858\n",
      "Epoch [799/1000], Loss: 0.3619\n",
      "Epoch [799/1000], Loss: 0.3256\n",
      "Epoch [799/1000], Loss: 0.2798\n",
      "Epoch [799/1000], Loss: 0.1657\n",
      "Epoch [799/1000], Loss: 0.1291\n",
      "Epoch [799/1000], Loss: 0.1636\n",
      "Epoch [799/1000], Loss: 0.1736\n",
      "Epoch [799/1000], Loss: 0.1160\n",
      "tensor(2.4357, grad_fn=<MeanBackward0>)\n",
      "799\n",
      "Epoch [800/1000], Loss: 0.1042\n",
      "Epoch [800/1000], Loss: 0.1112\n",
      "Epoch [800/1000], Loss: 0.0929\n",
      "Epoch [800/1000], Loss: 0.0789\n",
      "Epoch [800/1000], Loss: 0.0867\n",
      "Epoch [800/1000], Loss: 0.0984\n",
      "Epoch [800/1000], Loss: 0.0849\n",
      "Epoch [800/1000], Loss: 0.0951\n",
      "Epoch [800/1000], Loss: 0.1102\n",
      "Epoch [800/1000], Loss: 0.1141\n",
      "Epoch [800/1000], Loss: 0.0699\n",
      "tensor(2.4514, grad_fn=<MeanBackward0>)\n",
      "800\n",
      "Epoch [801/1000], Loss: 0.1027\n",
      "Epoch [801/1000], Loss: 0.1510\n",
      "Epoch [801/1000], Loss: 0.1012\n",
      "Epoch [801/1000], Loss: 0.0749\n",
      "Epoch [801/1000], Loss: 0.0721\n",
      "Epoch [801/1000], Loss: 0.0745\n",
      "Epoch [801/1000], Loss: 0.0741\n",
      "Epoch [801/1000], Loss: 0.0665\n",
      "Epoch [801/1000], Loss: 0.0652\n",
      "Epoch [801/1000], Loss: 0.0803\n",
      "Epoch [801/1000], Loss: 0.0694\n",
      "tensor(2.4519, grad_fn=<MeanBackward0>)\n",
      "801\n",
      "Epoch [802/1000], Loss: 0.0621\n",
      "Epoch [802/1000], Loss: 0.0942\n",
      "Epoch [802/1000], Loss: 0.0882\n",
      "Epoch [802/1000], Loss: 0.0944\n",
      "Epoch [802/1000], Loss: 0.0874\n",
      "Epoch [802/1000], Loss: 0.0757\n",
      "Epoch [802/1000], Loss: 0.1071\n",
      "Epoch [802/1000], Loss: 0.1147\n",
      "Epoch [802/1000], Loss: 0.1179\n",
      "Epoch [802/1000], Loss: 0.1285\n",
      "Epoch [802/1000], Loss: 0.1094\n",
      "tensor(2.4959, grad_fn=<MeanBackward0>)\n",
      "802\n",
      "Epoch [803/1000], Loss: 0.1117\n",
      "Epoch [803/1000], Loss: 0.1783\n",
      "Epoch [803/1000], Loss: 0.2250\n",
      "Epoch [803/1000], Loss: 0.1910\n",
      "Epoch [803/1000], Loss: 0.1235\n",
      "Epoch [803/1000], Loss: 0.0720\n",
      "Epoch [803/1000], Loss: 0.1461\n",
      "Epoch [803/1000], Loss: 0.2258\n",
      "Epoch [803/1000], Loss: 0.2670\n",
      "Epoch [803/1000], Loss: 0.2929\n",
      "Epoch [803/1000], Loss: 0.2360\n",
      "tensor(2.3761, grad_fn=<MeanBackward0>)\n",
      "803\n",
      "Epoch [804/1000], Loss: 0.1069\n",
      "Epoch [804/1000], Loss: 0.1181\n",
      "Epoch [804/1000], Loss: 0.2666\n",
      "Epoch [804/1000], Loss: 0.3524\n",
      "Epoch [804/1000], Loss: 0.3716\n",
      "Epoch [804/1000], Loss: 0.3295\n",
      "Epoch [804/1000], Loss: 0.2242\n",
      "Epoch [804/1000], Loss: 0.0929\n",
      "Epoch [804/1000], Loss: 0.1992\n",
      "Epoch [804/1000], Loss: 0.2873\n",
      "Epoch [804/1000], Loss: 0.3537\n",
      "tensor(2.1378, grad_fn=<MeanBackward0>)\n",
      "804\n",
      "Epoch [805/1000], Loss: 0.3608\n",
      "Epoch [805/1000], Loss: 0.3026\n",
      "Epoch [805/1000], Loss: 0.2151\n",
      "Epoch [805/1000], Loss: 0.1070\n",
      "Epoch [805/1000], Loss: 0.1944\n",
      "Epoch [805/1000], Loss: 0.2859\n",
      "Epoch [805/1000], Loss: 0.3656\n",
      "Epoch [805/1000], Loss: 0.3286\n",
      "Epoch [805/1000], Loss: 0.2935\n",
      "Epoch [805/1000], Loss: 0.2555\n",
      "Epoch [805/1000], Loss: 0.1834\n",
      "tensor(2.1973, grad_fn=<MeanBackward0>)\n",
      "805\n",
      "Epoch [806/1000], Loss: 0.2565\n",
      "Epoch [806/1000], Loss: 0.3815\n",
      "Epoch [806/1000], Loss: 0.4817\n",
      "Epoch [806/1000], Loss: 0.5325\n",
      "Epoch [806/1000], Loss: 0.4488\n",
      "Epoch [806/1000], Loss: 0.3530\n",
      "Epoch [806/1000], Loss: 0.1578\n",
      "Epoch [806/1000], Loss: 0.1682\n",
      "Epoch [806/1000], Loss: 0.3311\n",
      "Epoch [806/1000], Loss: 0.3778\n",
      "Epoch [806/1000], Loss: 0.2888\n",
      "tensor(2.4649, grad_fn=<MeanBackward0>)\n",
      "806\n",
      "Epoch [807/1000], Loss: 0.1884\n",
      "Epoch [807/1000], Loss: 0.1430\n",
      "Epoch [807/1000], Loss: 0.1623\n",
      "Epoch [807/1000], Loss: 0.1882\n",
      "Epoch [807/1000], Loss: 0.1244\n",
      "Epoch [807/1000], Loss: 0.0974\n",
      "Epoch [807/1000], Loss: 0.0908\n",
      "Epoch [807/1000], Loss: 0.0900\n",
      "Epoch [807/1000], Loss: 0.0630\n",
      "Epoch [807/1000], Loss: 0.0965\n",
      "Epoch [807/1000], Loss: 0.1173\n",
      "tensor(2.3926, grad_fn=<MeanBackward0>)\n",
      "807\n",
      "Epoch [808/1000], Loss: 0.0769\n",
      "Epoch [808/1000], Loss: 0.0783\n",
      "Epoch [808/1000], Loss: 0.1434\n",
      "Epoch [808/1000], Loss: 0.1315\n",
      "Epoch [808/1000], Loss: 0.0923\n",
      "Epoch [808/1000], Loss: 0.0549\n",
      "Epoch [808/1000], Loss: 0.1080\n",
      "Epoch [808/1000], Loss: 0.1348\n",
      "Epoch [808/1000], Loss: 0.0987\n",
      "Epoch [808/1000], Loss: 0.0554\n",
      "Epoch [808/1000], Loss: 0.0665\n",
      "tensor(2.4337, grad_fn=<MeanBackward0>)\n",
      "808\n",
      "Epoch [809/1000], Loss: 0.0836\n",
      "Epoch [809/1000], Loss: 0.0480\n",
      "Epoch [809/1000], Loss: 0.0655\n",
      "Epoch [809/1000], Loss: 0.0777\n",
      "Epoch [809/1000], Loss: 0.0753\n",
      "Epoch [809/1000], Loss: 0.1233\n",
      "Epoch [809/1000], Loss: 0.0996\n",
      "Epoch [809/1000], Loss: 0.0832\n",
      "Epoch [809/1000], Loss: 0.1425\n",
      "Epoch [809/1000], Loss: 0.1739\n",
      "Epoch [809/1000], Loss: 0.1492\n",
      "tensor(2.3839, grad_fn=<MeanBackward0>)\n",
      "809\n",
      "Epoch [810/1000], Loss: 0.0539\n",
      "Epoch [810/1000], Loss: 0.1057\n",
      "Epoch [810/1000], Loss: 0.1815\n",
      "Epoch [810/1000], Loss: 0.1734\n",
      "Epoch [810/1000], Loss: 0.1346\n",
      "Epoch [810/1000], Loss: 0.0946\n",
      "Epoch [810/1000], Loss: 0.0677\n",
      "Epoch [810/1000], Loss: 0.0909\n",
      "Epoch [810/1000], Loss: 0.1226\n",
      "Epoch [810/1000], Loss: 0.1275\n",
      "Epoch [810/1000], Loss: 0.0994\n",
      "tensor(2.3835, grad_fn=<MeanBackward0>)\n",
      "810\n",
      "Epoch [811/1000], Loss: 0.0562\n",
      "Epoch [811/1000], Loss: 0.0744\n",
      "Epoch [811/1000], Loss: 0.1041\n",
      "Epoch [811/1000], Loss: 0.1018\n",
      "Epoch [811/1000], Loss: 0.0754\n",
      "Epoch [811/1000], Loss: 0.0684\n",
      "Epoch [811/1000], Loss: 0.0575\n",
      "Epoch [811/1000], Loss: 0.0827\n",
      "Epoch [811/1000], Loss: 0.1357\n",
      "Epoch [811/1000], Loss: 0.1769\n",
      "Epoch [811/1000], Loss: 0.1904\n",
      "tensor(2.3093, grad_fn=<MeanBackward0>)\n",
      "811\n",
      "Epoch [812/1000], Loss: 0.1469\n",
      "Epoch [812/1000], Loss: 0.0701\n",
      "Epoch [812/1000], Loss: 0.1290\n",
      "Epoch [812/1000], Loss: 0.2178\n",
      "Epoch [812/1000], Loss: 0.2659\n",
      "Epoch [812/1000], Loss: 0.2792\n",
      "Epoch [812/1000], Loss: 0.2407\n",
      "Epoch [812/1000], Loss: 0.1486\n",
      "Epoch [812/1000], Loss: 0.1067\n",
      "Epoch [812/1000], Loss: 0.1751\n",
      "Epoch [812/1000], Loss: 0.2824\n",
      "tensor(2.1448, grad_fn=<MeanBackward0>)\n",
      "812\n",
      "Epoch [813/1000], Loss: 0.3360\n",
      "Epoch [813/1000], Loss: 0.3529\n",
      "Epoch [813/1000], Loss: 0.3310\n",
      "Epoch [813/1000], Loss: 0.2327\n",
      "Epoch [813/1000], Loss: 0.1554\n",
      "Epoch [813/1000], Loss: 0.1911\n",
      "Epoch [813/1000], Loss: 0.3514\n",
      "Epoch [813/1000], Loss: 0.4376\n",
      "Epoch [813/1000], Loss: 0.4990\n",
      "Epoch [813/1000], Loss: 0.5111\n",
      "Epoch [813/1000], Loss: 0.3612\n",
      "tensor(2.2911, grad_fn=<MeanBackward0>)\n",
      "813\n",
      "Epoch [814/1000], Loss: 0.1973\n",
      "Epoch [814/1000], Loss: 0.2976\n",
      "Epoch [814/1000], Loss: 0.4971\n",
      "Epoch [814/1000], Loss: 0.6253\n",
      "Epoch [814/1000], Loss: 0.6216\n",
      "Epoch [814/1000], Loss: 0.6085\n",
      "Epoch [814/1000], Loss: 0.3702\n",
      "Epoch [814/1000], Loss: 0.2284\n",
      "Epoch [814/1000], Loss: 0.1375\n",
      "Epoch [814/1000], Loss: 0.3167\n",
      "Epoch [814/1000], Loss: 0.3297\n",
      "tensor(2.5326, grad_fn=<MeanBackward0>)\n",
      "814\n",
      "Epoch [815/1000], Loss: 0.2671\n",
      "Epoch [815/1000], Loss: 0.1689\n",
      "Epoch [815/1000], Loss: 0.1478\n",
      "Epoch [815/1000], Loss: 0.1821\n",
      "Epoch [815/1000], Loss: 0.1754\n",
      "Epoch [815/1000], Loss: 0.1480\n",
      "Epoch [815/1000], Loss: 0.1066\n",
      "Epoch [815/1000], Loss: 0.0898\n",
      "Epoch [815/1000], Loss: 0.0825\n",
      "Epoch [815/1000], Loss: 0.1241\n",
      "Epoch [815/1000], Loss: 0.1532\n",
      "tensor(2.3700, grad_fn=<MeanBackward0>)\n",
      "815\n",
      "Epoch [816/1000], Loss: 0.1114\n",
      "Epoch [816/1000], Loss: 0.0978\n",
      "Epoch [816/1000], Loss: 0.1489\n",
      "Epoch [816/1000], Loss: 0.1941\n",
      "Epoch [816/1000], Loss: 0.1821\n",
      "Epoch [816/1000], Loss: 0.1478\n",
      "Epoch [816/1000], Loss: 0.0817\n",
      "Epoch [816/1000], Loss: 0.0935\n",
      "Epoch [816/1000], Loss: 0.1436\n",
      "Epoch [816/1000], Loss: 0.1945\n",
      "Epoch [816/1000], Loss: 0.1885\n",
      "tensor(2.2602, grad_fn=<MeanBackward0>)\n",
      "816\n",
      "Epoch [817/1000], Loss: 0.1296\n",
      "Epoch [817/1000], Loss: 0.0739\n",
      "Epoch [817/1000], Loss: 0.1047\n",
      "Epoch [817/1000], Loss: 0.1226\n",
      "Epoch [817/1000], Loss: 0.1203\n",
      "Epoch [817/1000], Loss: 0.1125\n",
      "Epoch [817/1000], Loss: 0.0899\n",
      "Epoch [817/1000], Loss: 0.0798\n",
      "Epoch [817/1000], Loss: 0.0786\n",
      "Epoch [817/1000], Loss: 0.0954\n",
      "Epoch [817/1000], Loss: 0.1162\n",
      "tensor(2.2889, grad_fn=<MeanBackward0>)\n",
      "817\n",
      "Epoch [818/1000], Loss: 0.1229\n",
      "Epoch [818/1000], Loss: 0.0962\n",
      "Epoch [818/1000], Loss: 0.0680\n",
      "Epoch [818/1000], Loss: 0.0907\n",
      "Epoch [818/1000], Loss: 0.1459\n",
      "Epoch [818/1000], Loss: 0.1688\n",
      "Epoch [818/1000], Loss: 0.1681\n",
      "Epoch [818/1000], Loss: 0.1133\n",
      "Epoch [818/1000], Loss: 0.0852\n",
      "Epoch [818/1000], Loss: 0.1009\n",
      "Epoch [818/1000], Loss: 0.1725\n",
      "tensor(2.1884, grad_fn=<MeanBackward0>)\n",
      "818\n",
      "Epoch [819/1000], Loss: 0.2153\n",
      "Epoch [819/1000], Loss: 0.2340\n",
      "Epoch [819/1000], Loss: 0.2334\n",
      "Epoch [819/1000], Loss: 0.1608\n",
      "Epoch [819/1000], Loss: 0.1152\n",
      "Epoch [819/1000], Loss: 0.1498\n",
      "Epoch [819/1000], Loss: 0.2955\n",
      "Epoch [819/1000], Loss: 0.3824\n",
      "Epoch [819/1000], Loss: 0.4400\n",
      "Epoch [819/1000], Loss: 0.4511\n",
      "Epoch [819/1000], Loss: 0.3351\n",
      "tensor(2.2924, grad_fn=<MeanBackward0>)\n",
      "819\n",
      "Epoch [820/1000], Loss: 0.1760\n",
      "Epoch [820/1000], Loss: 0.2639\n",
      "Epoch [820/1000], Loss: 0.4710\n",
      "Epoch [820/1000], Loss: 0.6165\n",
      "Epoch [820/1000], Loss: 0.6404\n",
      "Epoch [820/1000], Loss: 0.6527\n",
      "Epoch [820/1000], Loss: 0.4422\n",
      "Epoch [820/1000], Loss: 0.2882\n",
      "Epoch [820/1000], Loss: 0.1415\n",
      "Epoch [820/1000], Loss: 0.2999\n",
      "Epoch [820/1000], Loss: 0.3974\n",
      "tensor(2.5556, grad_fn=<MeanBackward0>)\n",
      "820\n",
      "Epoch [821/1000], Loss: 0.3630\n",
      "Epoch [821/1000], Loss: 0.2637\n",
      "Epoch [821/1000], Loss: 0.1498\n",
      "Epoch [821/1000], Loss: 0.1330\n",
      "Epoch [821/1000], Loss: 0.1755\n",
      "Epoch [821/1000], Loss: 0.1655\n",
      "Epoch [821/1000], Loss: 0.1118\n",
      "Epoch [821/1000], Loss: 0.1019\n",
      "Epoch [821/1000], Loss: 0.0909\n",
      "Epoch [821/1000], Loss: 0.1171\n",
      "Epoch [821/1000], Loss: 0.1161\n",
      "tensor(2.3826, grad_fn=<MeanBackward0>)\n",
      "821\n",
      "Epoch [822/1000], Loss: 0.1147\n",
      "Epoch [822/1000], Loss: 0.1261\n",
      "Epoch [822/1000], Loss: 0.1192\n",
      "Epoch [822/1000], Loss: 0.1275\n",
      "Epoch [822/1000], Loss: 0.1158\n",
      "Epoch [822/1000], Loss: 0.1226\n",
      "Epoch [822/1000], Loss: 0.0986\n",
      "Epoch [822/1000], Loss: 0.1012\n",
      "Epoch [822/1000], Loss: 0.0962\n",
      "Epoch [822/1000], Loss: 0.1374\n",
      "Epoch [822/1000], Loss: 0.1191\n",
      "tensor(2.3210, grad_fn=<MeanBackward0>)\n",
      "822\n",
      "Epoch [823/1000], Loss: 0.0661\n",
      "Epoch [823/1000], Loss: 0.0718\n",
      "Epoch [823/1000], Loss: 0.0966\n",
      "Epoch [823/1000], Loss: 0.0906\n",
      "Epoch [823/1000], Loss: 0.0867\n",
      "Epoch [823/1000], Loss: 0.0826\n",
      "Epoch [823/1000], Loss: 0.0804\n",
      "Epoch [823/1000], Loss: 0.0910\n",
      "Epoch [823/1000], Loss: 0.1006\n",
      "Epoch [823/1000], Loss: 0.0735\n",
      "Epoch [823/1000], Loss: 0.0961\n",
      "tensor(2.2277, grad_fn=<MeanBackward0>)\n",
      "823\n",
      "Epoch [824/1000], Loss: 0.1421\n",
      "Epoch [824/1000], Loss: 0.1752\n",
      "Epoch [824/1000], Loss: 0.1440\n",
      "Epoch [824/1000], Loss: 0.0893\n",
      "Epoch [824/1000], Loss: 0.1102\n",
      "Epoch [824/1000], Loss: 0.1572\n",
      "Epoch [824/1000], Loss: 0.2335\n",
      "Epoch [824/1000], Loss: 0.2301\n",
      "Epoch [824/1000], Loss: 0.2272\n",
      "Epoch [824/1000], Loss: 0.2137\n",
      "Epoch [824/1000], Loss: 0.1315\n",
      "tensor(2.1938, grad_fn=<MeanBackward0>)\n",
      "824\n",
      "Epoch [825/1000], Loss: 0.1474\n",
      "Epoch [825/1000], Loss: 0.2679\n",
      "Epoch [825/1000], Loss: 0.4189\n",
      "Epoch [825/1000], Loss: 0.5088\n",
      "Epoch [825/1000], Loss: 0.5360\n",
      "Epoch [825/1000], Loss: 0.5203\n",
      "Epoch [825/1000], Loss: 0.3250\n",
      "Epoch [825/1000], Loss: 0.1722\n",
      "Epoch [825/1000], Loss: 0.2014\n",
      "Epoch [825/1000], Loss: 0.4294\n",
      "Epoch [825/1000], Loss: 0.4892\n",
      "tensor(2.5767, grad_fn=<MeanBackward0>)\n",
      "825\n",
      "Epoch [826/1000], Loss: 0.4431\n",
      "Epoch [826/1000], Loss: 0.3173\n",
      "Epoch [826/1000], Loss: 0.1905\n",
      "Epoch [826/1000], Loss: 0.2082\n",
      "Epoch [826/1000], Loss: 0.2318\n",
      "Epoch [826/1000], Loss: 0.2843\n",
      "Epoch [826/1000], Loss: 0.2089\n",
      "Epoch [826/1000], Loss: 0.1608\n",
      "Epoch [826/1000], Loss: 0.1101\n",
      "Epoch [826/1000], Loss: 0.1020\n",
      "Epoch [826/1000], Loss: 0.1148\n",
      "tensor(2.4330, grad_fn=<MeanBackward0>)\n",
      "826\n",
      "Epoch [827/1000], Loss: 0.1361\n",
      "Epoch [827/1000], Loss: 0.1401\n",
      "Epoch [827/1000], Loss: 0.1181\n",
      "Epoch [827/1000], Loss: 0.1321\n",
      "Epoch [827/1000], Loss: 0.1209\n",
      "Epoch [827/1000], Loss: 0.1319\n",
      "Epoch [827/1000], Loss: 0.0986\n",
      "Epoch [827/1000], Loss: 0.0872\n",
      "Epoch [827/1000], Loss: 0.1046\n",
      "Epoch [827/1000], Loss: 0.1430\n",
      "Epoch [827/1000], Loss: 0.1241\n",
      "tensor(2.3614, grad_fn=<MeanBackward0>)\n",
      "827\n",
      "Epoch [828/1000], Loss: 0.0847\n",
      "Epoch [828/1000], Loss: 0.0868\n",
      "Epoch [828/1000], Loss: 0.1207\n",
      "Epoch [828/1000], Loss: 0.1175\n",
      "Epoch [828/1000], Loss: 0.0922\n",
      "Epoch [828/1000], Loss: 0.0866\n",
      "Epoch [828/1000], Loss: 0.0769\n",
      "Epoch [828/1000], Loss: 0.0582\n",
      "Epoch [828/1000], Loss: 0.0797\n",
      "Epoch [828/1000], Loss: 0.0841\n",
      "Epoch [828/1000], Loss: 0.0635\n",
      "tensor(2.3349, grad_fn=<MeanBackward0>)\n",
      "828\n",
      "Epoch [829/1000], Loss: 0.0658\n",
      "Epoch [829/1000], Loss: 0.0582\n",
      "Epoch [829/1000], Loss: 0.0699\n",
      "Epoch [829/1000], Loss: 0.0912\n",
      "Epoch [829/1000], Loss: 0.0783\n",
      "Epoch [829/1000], Loss: 0.1052\n",
      "Epoch [829/1000], Loss: 0.0869\n",
      "Epoch [829/1000], Loss: 0.0535\n",
      "Epoch [829/1000], Loss: 0.0964\n",
      "Epoch [829/1000], Loss: 0.1420\n",
      "Epoch [829/1000], Loss: 0.1244\n",
      "tensor(2.2908, grad_fn=<MeanBackward0>)\n",
      "829\n",
      "Epoch [830/1000], Loss: 0.0822\n",
      "Epoch [830/1000], Loss: 0.0592\n",
      "Epoch [830/1000], Loss: 0.0797\n",
      "Epoch [830/1000], Loss: 0.0889\n",
      "Epoch [830/1000], Loss: 0.0722\n",
      "Epoch [830/1000], Loss: 0.0602\n",
      "Epoch [830/1000], Loss: 0.0489\n",
      "Epoch [830/1000], Loss: 0.0470\n",
      "Epoch [830/1000], Loss: 0.0565\n",
      "Epoch [830/1000], Loss: 0.0644\n",
      "Epoch [830/1000], Loss: 0.0801\n",
      "tensor(2.2932, grad_fn=<MeanBackward0>)\n",
      "830\n",
      "Epoch [831/1000], Loss: 0.0799\n",
      "Epoch [831/1000], Loss: 0.0852\n",
      "Epoch [831/1000], Loss: 0.0631\n",
      "Epoch [831/1000], Loss: 0.0700\n",
      "Epoch [831/1000], Loss: 0.1077\n",
      "Epoch [831/1000], Loss: 0.1349\n",
      "Epoch [831/1000], Loss: 0.1303\n",
      "Epoch [831/1000], Loss: 0.0745\n",
      "Epoch [831/1000], Loss: 0.0642\n",
      "Epoch [831/1000], Loss: 0.1014\n",
      "Epoch [831/1000], Loss: 0.1203\n",
      "tensor(2.1885, grad_fn=<MeanBackward0>)\n",
      "831\n",
      "Epoch [832/1000], Loss: 0.1472\n",
      "Epoch [832/1000], Loss: 0.1427\n",
      "Epoch [832/1000], Loss: 0.1329\n",
      "Epoch [832/1000], Loss: 0.0935\n",
      "Epoch [832/1000], Loss: 0.1124\n",
      "Epoch [832/1000], Loss: 0.1570\n",
      "Epoch [832/1000], Loss: 0.2341\n",
      "Epoch [832/1000], Loss: 0.2431\n",
      "Epoch [832/1000], Loss: 0.2567\n",
      "Epoch [832/1000], Loss: 0.2299\n",
      "Epoch [832/1000], Loss: 0.1434\n",
      "tensor(2.1448, grad_fn=<MeanBackward0>)\n",
      "832\n",
      "Epoch [833/1000], Loss: 0.1569\n",
      "Epoch [833/1000], Loss: 0.3187\n",
      "Epoch [833/1000], Loss: 0.4821\n",
      "Epoch [833/1000], Loss: 0.6311\n",
      "Epoch [833/1000], Loss: 0.6447\n",
      "Epoch [833/1000], Loss: 0.6316\n",
      "Epoch [833/1000], Loss: 0.3933\n",
      "Epoch [833/1000], Loss: 0.2306\n",
      "Epoch [833/1000], Loss: 0.1573\n",
      "Epoch [833/1000], Loss: 0.4145\n",
      "Epoch [833/1000], Loss: 0.5211\n",
      "tensor(2.6228, grad_fn=<MeanBackward0>)\n",
      "833\n",
      "Epoch [834/1000], Loss: 0.4897\n",
      "Epoch [834/1000], Loss: 0.3764\n",
      "Epoch [834/1000], Loss: 0.2257\n",
      "Epoch [834/1000], Loss: 0.2025\n",
      "Epoch [834/1000], Loss: 0.2279\n",
      "Epoch [834/1000], Loss: 0.2857\n",
      "Epoch [834/1000], Loss: 0.2219\n",
      "Epoch [834/1000], Loss: 0.1711\n",
      "Epoch [834/1000], Loss: 0.1297\n",
      "Epoch [834/1000], Loss: 0.1439\n",
      "Epoch [834/1000], Loss: 0.1132\n",
      "tensor(2.3733, grad_fn=<MeanBackward0>)\n",
      "834\n",
      "Epoch [835/1000], Loss: 0.1221\n",
      "Epoch [835/1000], Loss: 0.1342\n",
      "Epoch [835/1000], Loss: 0.1143\n",
      "Epoch [835/1000], Loss: 0.1220\n",
      "Epoch [835/1000], Loss: 0.1051\n",
      "Epoch [835/1000], Loss: 0.1289\n",
      "Epoch [835/1000], Loss: 0.1003\n",
      "Epoch [835/1000], Loss: 0.1050\n",
      "Epoch [835/1000], Loss: 0.0997\n",
      "Epoch [835/1000], Loss: 0.1319\n",
      "Epoch [835/1000], Loss: 0.1168\n",
      "tensor(2.3015, grad_fn=<MeanBackward0>)\n",
      "835\n",
      "Epoch [836/1000], Loss: 0.0835\n",
      "Epoch [836/1000], Loss: 0.0753\n",
      "Epoch [836/1000], Loss: 0.0694\n",
      "Epoch [836/1000], Loss: 0.0762\n",
      "Epoch [836/1000], Loss: 0.0748\n",
      "Epoch [836/1000], Loss: 0.0754\n",
      "Epoch [836/1000], Loss: 0.0844\n",
      "Epoch [836/1000], Loss: 0.0683\n",
      "Epoch [836/1000], Loss: 0.0834\n",
      "Epoch [836/1000], Loss: 0.0997\n",
      "Epoch [836/1000], Loss: 0.0761\n",
      "tensor(2.2557, grad_fn=<MeanBackward0>)\n",
      "836\n",
      "Epoch [837/1000], Loss: 0.0908\n",
      "Epoch [837/1000], Loss: 0.1176\n",
      "Epoch [837/1000], Loss: 0.1357\n",
      "Epoch [837/1000], Loss: 0.0939\n",
      "Epoch [837/1000], Loss: 0.0605\n",
      "Epoch [837/1000], Loss: 0.1460\n",
      "Epoch [837/1000], Loss: 0.2187\n",
      "Epoch [837/1000], Loss: 0.2207\n",
      "Epoch [837/1000], Loss: 0.1730\n",
      "Epoch [837/1000], Loss: 0.1101\n",
      "Epoch [837/1000], Loss: 0.0788\n",
      "tensor(2.1702, grad_fn=<MeanBackward0>)\n",
      "837\n",
      "Epoch [838/1000], Loss: 0.1422\n",
      "Epoch [838/1000], Loss: 0.2401\n",
      "Epoch [838/1000], Loss: 0.3294\n",
      "Epoch [838/1000], Loss: 0.3843\n",
      "Epoch [838/1000], Loss: 0.3891\n",
      "Epoch [838/1000], Loss: 0.3444\n",
      "Epoch [838/1000], Loss: 0.1851\n",
      "Epoch [838/1000], Loss: 0.1567\n",
      "Epoch [838/1000], Loss: 0.3284\n",
      "Epoch [838/1000], Loss: 0.5431\n",
      "Epoch [838/1000], Loss: 0.6165\n",
      "tensor(2.6032, grad_fn=<MeanBackward0>)\n",
      "838\n",
      "Epoch [839/1000], Loss: 0.5666\n",
      "Epoch [839/1000], Loss: 0.4249\n",
      "Epoch [839/1000], Loss: 0.1728\n",
      "Epoch [839/1000], Loss: 0.1875\n",
      "Epoch [839/1000], Loss: 0.3219\n",
      "Epoch [839/1000], Loss: 0.4667\n",
      "Epoch [839/1000], Loss: 0.3968\n",
      "Epoch [839/1000], Loss: 0.4076\n",
      "Epoch [839/1000], Loss: 0.3291\n",
      "Epoch [839/1000], Loss: 0.2293\n",
      "Epoch [839/1000], Loss: 0.2000\n",
      "tensor(2.4617, grad_fn=<MeanBackward0>)\n",
      "839\n",
      "Epoch [840/1000], Loss: 0.1974\n",
      "Epoch [840/1000], Loss: 0.2236\n",
      "Epoch [840/1000], Loss: 0.2238\n",
      "Epoch [840/1000], Loss: 0.1706\n",
      "Epoch [840/1000], Loss: 0.1271\n",
      "Epoch [840/1000], Loss: 0.1322\n",
      "Epoch [840/1000], Loss: 0.1290\n",
      "Epoch [840/1000], Loss: 0.1290\n",
      "Epoch [840/1000], Loss: 0.1172\n",
      "Epoch [840/1000], Loss: 0.0983\n",
      "Epoch [840/1000], Loss: 0.1005\n",
      "tensor(2.4321, grad_fn=<MeanBackward0>)\n",
      "840\n",
      "Epoch [841/1000], Loss: 0.1203\n",
      "Epoch [841/1000], Loss: 0.1076\n",
      "Epoch [841/1000], Loss: 0.1292\n",
      "Epoch [841/1000], Loss: 0.1069\n",
      "Epoch [841/1000], Loss: 0.0947\n",
      "Epoch [841/1000], Loss: 0.0921\n",
      "Epoch [841/1000], Loss: 0.0806\n",
      "Epoch [841/1000], Loss: 0.0834\n",
      "Epoch [841/1000], Loss: 0.1054\n",
      "Epoch [841/1000], Loss: 0.1400\n",
      "Epoch [841/1000], Loss: 0.1313\n",
      "tensor(2.2702, grad_fn=<MeanBackward0>)\n",
      "841\n",
      "Epoch [842/1000], Loss: 0.0931\n",
      "Epoch [842/1000], Loss: 0.0877\n",
      "Epoch [842/1000], Loss: 0.0866\n",
      "Epoch [842/1000], Loss: 0.1014\n",
      "Epoch [842/1000], Loss: 0.1123\n",
      "Epoch [842/1000], Loss: 0.1238\n",
      "Epoch [842/1000], Loss: 0.1578\n",
      "Epoch [842/1000], Loss: 0.1348\n",
      "Epoch [842/1000], Loss: 0.0937\n",
      "Epoch [842/1000], Loss: 0.0679\n",
      "Epoch [842/1000], Loss: 0.0832\n",
      "tensor(2.1802, grad_fn=<MeanBackward0>)\n",
      "842\n",
      "Epoch [843/1000], Loss: 0.1149\n",
      "Epoch [843/1000], Loss: 0.1706\n",
      "Epoch [843/1000], Loss: 0.2099\n",
      "Epoch [843/1000], Loss: 0.1865\n",
      "Epoch [843/1000], Loss: 0.1269\n",
      "Epoch [843/1000], Loss: 0.1069\n",
      "Epoch [843/1000], Loss: 0.1689\n",
      "Epoch [843/1000], Loss: 0.2564\n",
      "Epoch [843/1000], Loss: 0.3440\n",
      "Epoch [843/1000], Loss: 0.4207\n",
      "Epoch [843/1000], Loss: 0.3823\n",
      "tensor(2.3974, grad_fn=<MeanBackward0>)\n",
      "843\n",
      "Epoch [844/1000], Loss: 0.2586\n",
      "Epoch [844/1000], Loss: 0.1799\n",
      "Epoch [844/1000], Loss: 0.2740\n",
      "Epoch [844/1000], Loss: 0.4364\n",
      "Epoch [844/1000], Loss: 0.5322\n",
      "Epoch [844/1000], Loss: 0.6422\n",
      "Epoch [844/1000], Loss: 0.5042\n",
      "Epoch [844/1000], Loss: 0.4731\n",
      "Epoch [844/1000], Loss: 0.3267\n",
      "Epoch [844/1000], Loss: 0.1752\n",
      "Epoch [844/1000], Loss: 0.2227\n",
      "tensor(2.5313, grad_fn=<MeanBackward0>)\n",
      "844\n",
      "Epoch [845/1000], Loss: 0.2888\n",
      "Epoch [845/1000], Loss: 0.3168\n",
      "Epoch [845/1000], Loss: 0.2438\n",
      "Epoch [845/1000], Loss: 0.1704\n",
      "Epoch [845/1000], Loss: 0.1399\n",
      "Epoch [845/1000], Loss: 0.1772\n",
      "Epoch [845/1000], Loss: 0.1820\n",
      "Epoch [845/1000], Loss: 0.1778\n",
      "Epoch [845/1000], Loss: 0.1681\n",
      "Epoch [845/1000], Loss: 0.1598\n",
      "Epoch [845/1000], Loss: 0.1346\n",
      "tensor(2.4386, grad_fn=<MeanBackward0>)\n",
      "845\n",
      "Epoch [846/1000], Loss: 0.1482\n",
      "Epoch [846/1000], Loss: 0.1578\n",
      "Epoch [846/1000], Loss: 0.1845\n",
      "Epoch [846/1000], Loss: 0.1707\n",
      "Epoch [846/1000], Loss: 0.1318\n",
      "Epoch [846/1000], Loss: 0.0944\n",
      "Epoch [846/1000], Loss: 0.1035\n",
      "Epoch [846/1000], Loss: 0.1222\n",
      "Epoch [846/1000], Loss: 0.1069\n",
      "Epoch [846/1000], Loss: 0.1028\n",
      "Epoch [846/1000], Loss: 0.0824\n",
      "tensor(2.3703, grad_fn=<MeanBackward0>)\n",
      "846\n",
      "Epoch [847/1000], Loss: 0.0613\n",
      "Epoch [847/1000], Loss: 0.0549\n",
      "Epoch [847/1000], Loss: 0.0608\n",
      "Epoch [847/1000], Loss: 0.0707\n",
      "Epoch [847/1000], Loss: 0.0682\n",
      "Epoch [847/1000], Loss: 0.0785\n",
      "Epoch [847/1000], Loss: 0.0765\n",
      "Epoch [847/1000], Loss: 0.0603\n",
      "Epoch [847/1000], Loss: 0.0542\n",
      "Epoch [847/1000], Loss: 0.0913\n",
      "Epoch [847/1000], Loss: 0.0973\n",
      "tensor(2.2783, grad_fn=<MeanBackward0>)\n",
      "847\n",
      "Epoch [848/1000], Loss: 0.0713\n",
      "Epoch [848/1000], Loss: 0.0492\n",
      "Epoch [848/1000], Loss: 0.0807\n",
      "Epoch [848/1000], Loss: 0.0633\n",
      "Epoch [848/1000], Loss: 0.0541\n",
      "Epoch [848/1000], Loss: 0.0554\n",
      "Epoch [848/1000], Loss: 0.0614\n",
      "Epoch [848/1000], Loss: 0.0663\n",
      "Epoch [848/1000], Loss: 0.0491\n",
      "Epoch [848/1000], Loss: 0.0599\n",
      "Epoch [848/1000], Loss: 0.0834\n",
      "tensor(2.2923, grad_fn=<MeanBackward0>)\n",
      "848\n",
      "Epoch [849/1000], Loss: 0.0826\n",
      "Epoch [849/1000], Loss: 0.0758\n",
      "Epoch [849/1000], Loss: 0.0842\n",
      "Epoch [849/1000], Loss: 0.0594\n",
      "Epoch [849/1000], Loss: 0.0594\n",
      "Epoch [849/1000], Loss: 0.0621\n",
      "Epoch [849/1000], Loss: 0.0872\n",
      "Epoch [849/1000], Loss: 0.0729\n",
      "Epoch [849/1000], Loss: 0.0524\n",
      "Epoch [849/1000], Loss: 0.0623\n",
      "Epoch [849/1000], Loss: 0.0631\n",
      "tensor(2.2170, grad_fn=<MeanBackward0>)\n",
      "849\n",
      "Epoch [850/1000], Loss: 0.1116\n",
      "Epoch [850/1000], Loss: 0.1189\n",
      "Epoch [850/1000], Loss: 0.1167\n",
      "Epoch [850/1000], Loss: 0.1063\n",
      "Epoch [850/1000], Loss: 0.1149\n",
      "Epoch [850/1000], Loss: 0.1315\n",
      "Epoch [850/1000], Loss: 0.1531\n",
      "Epoch [850/1000], Loss: 0.1368\n",
      "Epoch [850/1000], Loss: 0.1342\n",
      "Epoch [850/1000], Loss: 0.1166\n",
      "Epoch [850/1000], Loss: 0.1026\n",
      "tensor(2.1951, grad_fn=<MeanBackward0>)\n",
      "850\n",
      "Epoch [851/1000], Loss: 0.1635\n",
      "Epoch [851/1000], Loss: 0.2616\n",
      "Epoch [851/1000], Loss: 0.3350\n",
      "Epoch [851/1000], Loss: 0.4091\n",
      "Epoch [851/1000], Loss: 0.3683\n",
      "Epoch [851/1000], Loss: 0.3131\n",
      "Epoch [851/1000], Loss: 0.1471\n",
      "Epoch [851/1000], Loss: 0.1410\n",
      "Epoch [851/1000], Loss: 0.3469\n",
      "Epoch [851/1000], Loss: 0.4869\n",
      "Epoch [851/1000], Loss: 0.5720\n",
      "tensor(2.5795, grad_fn=<MeanBackward0>)\n",
      "851\n",
      "Epoch [852/1000], Loss: 0.4825\n",
      "Epoch [852/1000], Loss: 0.3375\n",
      "Epoch [852/1000], Loss: 0.1457\n",
      "Epoch [852/1000], Loss: 0.1943\n",
      "Epoch [852/1000], Loss: 0.2880\n",
      "Epoch [852/1000], Loss: 0.4076\n",
      "Epoch [852/1000], Loss: 0.3535\n",
      "Epoch [852/1000], Loss: 0.3680\n",
      "Epoch [852/1000], Loss: 0.2813\n",
      "Epoch [852/1000], Loss: 0.1878\n",
      "Epoch [852/1000], Loss: 0.1620\n",
      "tensor(2.4712, grad_fn=<MeanBackward0>)\n",
      "852\n",
      "Epoch [853/1000], Loss: 0.1898\n",
      "Epoch [853/1000], Loss: 0.2025\n",
      "Epoch [853/1000], Loss: 0.1804\n",
      "Epoch [853/1000], Loss: 0.1502\n",
      "Epoch [853/1000], Loss: 0.1243\n",
      "Epoch [853/1000], Loss: 0.1267\n",
      "Epoch [853/1000], Loss: 0.1253\n",
      "Epoch [853/1000], Loss: 0.1342\n",
      "Epoch [853/1000], Loss: 0.1186\n",
      "Epoch [853/1000], Loss: 0.1133\n",
      "Epoch [853/1000], Loss: 0.0914\n",
      "tensor(2.4024, grad_fn=<MeanBackward0>)\n",
      "853\n",
      "Epoch [854/1000], Loss: 0.0891\n",
      "Epoch [854/1000], Loss: 0.0830\n",
      "Epoch [854/1000], Loss: 0.1039\n",
      "Epoch [854/1000], Loss: 0.0960\n",
      "Epoch [854/1000], Loss: 0.0747\n",
      "Epoch [854/1000], Loss: 0.0750\n",
      "Epoch [854/1000], Loss: 0.0678\n",
      "Epoch [854/1000], Loss: 0.0793\n",
      "Epoch [854/1000], Loss: 0.0734\n",
      "Epoch [854/1000], Loss: 0.0823\n",
      "Epoch [854/1000], Loss: 0.0490\n",
      "tensor(2.3117, grad_fn=<MeanBackward0>)\n",
      "854\n",
      "Epoch [855/1000], Loss: 0.0518\n",
      "Epoch [855/1000], Loss: 0.0688\n",
      "Epoch [855/1000], Loss: 0.0719\n",
      "Epoch [855/1000], Loss: 0.0740\n",
      "Epoch [855/1000], Loss: 0.0781\n",
      "Epoch [855/1000], Loss: 0.0829\n",
      "Epoch [855/1000], Loss: 0.0597\n",
      "Epoch [855/1000], Loss: 0.0547\n",
      "Epoch [855/1000], Loss: 0.0602\n",
      "Epoch [855/1000], Loss: 0.0642\n",
      "Epoch [855/1000], Loss: 0.0682\n",
      "tensor(2.2726, grad_fn=<MeanBackward0>)\n",
      "855\n",
      "Epoch [856/1000], Loss: 0.0688\n",
      "Epoch [856/1000], Loss: 0.1040\n",
      "Epoch [856/1000], Loss: 0.1011\n",
      "Epoch [856/1000], Loss: 0.0521\n",
      "Epoch [856/1000], Loss: 0.0870\n",
      "Epoch [856/1000], Loss: 0.1412\n",
      "Epoch [856/1000], Loss: 0.1761\n",
      "Epoch [856/1000], Loss: 0.1148\n",
      "Epoch [856/1000], Loss: 0.0723\n",
      "Epoch [856/1000], Loss: 0.0759\n",
      "Epoch [856/1000], Loss: 0.0766\n",
      "tensor(2.2390, grad_fn=<MeanBackward0>)\n",
      "856\n",
      "Epoch [857/1000], Loss: 0.0993\n",
      "Epoch [857/1000], Loss: 0.1419\n",
      "Epoch [857/1000], Loss: 0.1935\n",
      "Epoch [857/1000], Loss: 0.1920\n",
      "Epoch [857/1000], Loss: 0.1758\n",
      "Epoch [857/1000], Loss: 0.1320\n",
      "Epoch [857/1000], Loss: 0.1127\n",
      "Epoch [857/1000], Loss: 0.1940\n",
      "Epoch [857/1000], Loss: 0.3050\n",
      "Epoch [857/1000], Loss: 0.3360\n",
      "Epoch [857/1000], Loss: 0.3203\n",
      "tensor(2.3648, grad_fn=<MeanBackward0>)\n",
      "857\n",
      "Epoch [858/1000], Loss: 0.1779\n",
      "Epoch [858/1000], Loss: 0.1278\n",
      "Epoch [858/1000], Loss: 0.2523\n",
      "Epoch [858/1000], Loss: 0.4150\n",
      "Epoch [858/1000], Loss: 0.5014\n",
      "Epoch [858/1000], Loss: 0.5927\n",
      "Epoch [858/1000], Loss: 0.4596\n",
      "Epoch [858/1000], Loss: 0.4273\n",
      "Epoch [858/1000], Loss: 0.2823\n",
      "Epoch [858/1000], Loss: 0.1532\n",
      "Epoch [858/1000], Loss: 0.2516\n",
      "tensor(2.5480, grad_fn=<MeanBackward0>)\n",
      "858\n",
      "Epoch [859/1000], Loss: 0.3114\n",
      "Epoch [859/1000], Loss: 0.3206\n",
      "Epoch [859/1000], Loss: 0.2351\n",
      "Epoch [859/1000], Loss: 0.1370\n",
      "Epoch [859/1000], Loss: 0.1528\n",
      "Epoch [859/1000], Loss: 0.1931\n",
      "Epoch [859/1000], Loss: 0.1849\n",
      "Epoch [859/1000], Loss: 0.1669\n",
      "Epoch [859/1000], Loss: 0.1649\n",
      "Epoch [859/1000], Loss: 0.1603\n",
      "Epoch [859/1000], Loss: 0.1266\n",
      "tensor(2.4261, grad_fn=<MeanBackward0>)\n",
      "859\n",
      "Epoch [860/1000], Loss: 0.1406\n",
      "Epoch [860/1000], Loss: 0.1543\n",
      "Epoch [860/1000], Loss: 0.1785\n",
      "Epoch [860/1000], Loss: 0.1746\n",
      "Epoch [860/1000], Loss: 0.1358\n",
      "Epoch [860/1000], Loss: 0.0972\n",
      "Epoch [860/1000], Loss: 0.1145\n",
      "Epoch [860/1000], Loss: 0.1345\n",
      "Epoch [860/1000], Loss: 0.1290\n",
      "Epoch [860/1000], Loss: 0.1159\n",
      "Epoch [860/1000], Loss: 0.0866\n",
      "tensor(2.3797, grad_fn=<MeanBackward0>)\n",
      "860\n",
      "Epoch [861/1000], Loss: 0.0652\n",
      "Epoch [861/1000], Loss: 0.0805\n",
      "Epoch [861/1000], Loss: 0.0638\n",
      "Epoch [861/1000], Loss: 0.0701\n",
      "Epoch [861/1000], Loss: 0.0640\n",
      "Epoch [861/1000], Loss: 0.0856\n",
      "Epoch [861/1000], Loss: 0.0895\n",
      "Epoch [861/1000], Loss: 0.0799\n",
      "Epoch [861/1000], Loss: 0.0539\n",
      "Epoch [861/1000], Loss: 0.0839\n",
      "Epoch [861/1000], Loss: 0.1102\n",
      "tensor(2.2630, grad_fn=<MeanBackward0>)\n",
      "861\n",
      "Epoch [862/1000], Loss: 0.0895\n",
      "Epoch [862/1000], Loss: 0.0609\n",
      "Epoch [862/1000], Loss: 0.0974\n",
      "Epoch [862/1000], Loss: 0.1005\n",
      "Epoch [862/1000], Loss: 0.0707\n",
      "Epoch [862/1000], Loss: 0.0615\n",
      "Epoch [862/1000], Loss: 0.0540\n",
      "Epoch [862/1000], Loss: 0.0433\n",
      "Epoch [862/1000], Loss: 0.0390\n",
      "Epoch [862/1000], Loss: 0.0574\n",
      "Epoch [862/1000], Loss: 0.0652\n",
      "tensor(2.2820, grad_fn=<MeanBackward0>)\n",
      "862\n",
      "Epoch [863/1000], Loss: 0.0747\n",
      "Epoch [863/1000], Loss: 0.0583\n",
      "Epoch [863/1000], Loss: 0.0694\n",
      "Epoch [863/1000], Loss: 0.0880\n",
      "Epoch [863/1000], Loss: 0.0747\n",
      "Epoch [863/1000], Loss: 0.0628\n",
      "Epoch [863/1000], Loss: 0.0560\n",
      "Epoch [863/1000], Loss: 0.0473\n",
      "Epoch [863/1000], Loss: 0.0398\n",
      "Epoch [863/1000], Loss: 0.0593\n",
      "Epoch [863/1000], Loss: 0.0572\n",
      "tensor(2.2344, grad_fn=<MeanBackward0>)\n",
      "863\n",
      "Epoch [864/1000], Loss: 0.0933\n",
      "Epoch [864/1000], Loss: 0.1177\n",
      "Epoch [864/1000], Loss: 0.0950\n",
      "Epoch [864/1000], Loss: 0.0780\n",
      "Epoch [864/1000], Loss: 0.1170\n",
      "Epoch [864/1000], Loss: 0.1612\n",
      "Epoch [864/1000], Loss: 0.1558\n",
      "Epoch [864/1000], Loss: 0.1082\n",
      "Epoch [864/1000], Loss: 0.0986\n",
      "Epoch [864/1000], Loss: 0.0881\n",
      "Epoch [864/1000], Loss: 0.0980\n",
      "tensor(2.2003, grad_fn=<MeanBackward0>)\n",
      "864\n",
      "Epoch [865/1000], Loss: 0.1372\n",
      "Epoch [865/1000], Loss: 0.1852\n",
      "Epoch [865/1000], Loss: 0.2241\n",
      "Epoch [865/1000], Loss: 0.2500\n",
      "Epoch [865/1000], Loss: 0.1875\n",
      "Epoch [865/1000], Loss: 0.1090\n",
      "Epoch [865/1000], Loss: 0.1619\n",
      "Epoch [865/1000], Loss: 0.2571\n",
      "Epoch [865/1000], Loss: 0.3797\n",
      "Epoch [865/1000], Loss: 0.4276\n",
      "Epoch [865/1000], Loss: 0.4125\n",
      "tensor(2.3732, grad_fn=<MeanBackward0>)\n",
      "865\n",
      "Epoch [866/1000], Loss: 0.2823\n",
      "Epoch [866/1000], Loss: 0.1361\n",
      "Epoch [866/1000], Loss: 0.2223\n",
      "Epoch [866/1000], Loss: 0.4556\n",
      "Epoch [866/1000], Loss: 0.5718\n",
      "Epoch [866/1000], Loss: 0.6628\n",
      "Epoch [866/1000], Loss: 0.5611\n",
      "Epoch [866/1000], Loss: 0.5001\n",
      "Epoch [866/1000], Loss: 0.3134\n",
      "Epoch [866/1000], Loss: 0.1698\n",
      "Epoch [866/1000], Loss: 0.2387\n",
      "tensor(2.5232, grad_fn=<MeanBackward0>)\n",
      "866\n",
      "Epoch [867/1000], Loss: 0.3107\n",
      "Epoch [867/1000], Loss: 0.3117\n",
      "Epoch [867/1000], Loss: 0.2910\n",
      "Epoch [867/1000], Loss: 0.1916\n",
      "Epoch [867/1000], Loss: 0.1610\n",
      "Epoch [867/1000], Loss: 0.1858\n",
      "Epoch [867/1000], Loss: 0.1940\n",
      "Epoch [867/1000], Loss: 0.1774\n",
      "Epoch [867/1000], Loss: 0.1826\n",
      "Epoch [867/1000], Loss: 0.1823\n",
      "Epoch [867/1000], Loss: 0.1714\n",
      "tensor(2.3102, grad_fn=<MeanBackward0>)\n",
      "867\n",
      "Epoch [868/1000], Loss: 0.1406\n",
      "Epoch [868/1000], Loss: 0.1442\n",
      "Epoch [868/1000], Loss: 0.1466\n",
      "Epoch [868/1000], Loss: 0.1979\n",
      "Epoch [868/1000], Loss: 0.1774\n",
      "Epoch [868/1000], Loss: 0.1429\n",
      "Epoch [868/1000], Loss: 0.0724\n",
      "Epoch [868/1000], Loss: 0.1144\n",
      "Epoch [868/1000], Loss: 0.1450\n",
      "Epoch [868/1000], Loss: 0.1589\n",
      "Epoch [868/1000], Loss: 0.1208\n",
      "tensor(2.3149, grad_fn=<MeanBackward0>)\n",
      "868\n",
      "Epoch [869/1000], Loss: 0.0566\n",
      "Epoch [869/1000], Loss: 0.0683\n",
      "Epoch [869/1000], Loss: 0.1179\n",
      "Epoch [869/1000], Loss: 0.0764\n",
      "Epoch [869/1000], Loss: 0.0627\n",
      "Epoch [869/1000], Loss: 0.0638\n",
      "Epoch [869/1000], Loss: 0.0581\n",
      "Epoch [869/1000], Loss: 0.0579\n",
      "Epoch [869/1000], Loss: 0.0660\n",
      "Epoch [869/1000], Loss: 0.0788\n",
      "Epoch [869/1000], Loss: 0.1162\n",
      "tensor(2.2397, grad_fn=<MeanBackward0>)\n",
      "869\n",
      "Epoch [870/1000], Loss: 0.1226\n",
      "Epoch [870/1000], Loss: 0.1118\n",
      "Epoch [870/1000], Loss: 0.0747\n",
      "Epoch [870/1000], Loss: 0.0948\n",
      "Epoch [870/1000], Loss: 0.1185\n",
      "Epoch [870/1000], Loss: 0.1330\n",
      "Epoch [870/1000], Loss: 0.1252\n",
      "Epoch [870/1000], Loss: 0.1031\n",
      "Epoch [870/1000], Loss: 0.0941\n",
      "Epoch [870/1000], Loss: 0.0501\n",
      "Epoch [870/1000], Loss: 0.0873\n",
      "tensor(2.1857, grad_fn=<MeanBackward0>)\n",
      "870\n",
      "Epoch [871/1000], Loss: 0.1524\n",
      "Epoch [871/1000], Loss: 0.1993\n",
      "Epoch [871/1000], Loss: 0.1827\n",
      "Epoch [871/1000], Loss: 0.1538\n",
      "Epoch [871/1000], Loss: 0.1108\n",
      "Epoch [871/1000], Loss: 0.1159\n",
      "Epoch [871/1000], Loss: 0.2013\n",
      "Epoch [871/1000], Loss: 0.2401\n",
      "Epoch [871/1000], Loss: 0.2724\n",
      "Epoch [871/1000], Loss: 0.2949\n",
      "Epoch [871/1000], Loss: 0.2145\n",
      "tensor(2.2246, grad_fn=<MeanBackward0>)\n",
      "871\n",
      "Epoch [872/1000], Loss: 0.1437\n",
      "Epoch [872/1000], Loss: 0.2284\n",
      "Epoch [872/1000], Loss: 0.3817\n",
      "Epoch [872/1000], Loss: 0.5542\n",
      "Epoch [872/1000], Loss: 0.6074\n",
      "Epoch [872/1000], Loss: 0.6432\n",
      "Epoch [872/1000], Loss: 0.4630\n",
      "Epoch [872/1000], Loss: 0.3464\n",
      "Epoch [872/1000], Loss: 0.1620\n",
      "Epoch [872/1000], Loss: 0.3326\n",
      "Epoch [872/1000], Loss: 0.4762\n",
      "tensor(2.5876, grad_fn=<MeanBackward0>)\n",
      "872\n",
      "Epoch [873/1000], Loss: 0.5333\n",
      "Epoch [873/1000], Loss: 0.4962\n",
      "Epoch [873/1000], Loss: 0.3445\n",
      "Epoch [873/1000], Loss: 0.1875\n",
      "Epoch [873/1000], Loss: 0.2142\n",
      "Epoch [873/1000], Loss: 0.3039\n",
      "Epoch [873/1000], Loss: 0.2970\n",
      "Epoch [873/1000], Loss: 0.2734\n",
      "Epoch [873/1000], Loss: 0.1966\n",
      "Epoch [873/1000], Loss: 0.1616\n",
      "Epoch [873/1000], Loss: 0.1275\n",
      "tensor(2.3556, grad_fn=<MeanBackward0>)\n",
      "873\n",
      "Epoch [874/1000], Loss: 0.1408\n",
      "Epoch [874/1000], Loss: 0.1510\n",
      "Epoch [874/1000], Loss: 0.1357\n",
      "Epoch [874/1000], Loss: 0.1437\n",
      "Epoch [874/1000], Loss: 0.1435\n",
      "Epoch [874/1000], Loss: 0.1692\n",
      "Epoch [874/1000], Loss: 0.1135\n",
      "Epoch [874/1000], Loss: 0.0973\n",
      "Epoch [874/1000], Loss: 0.1187\n",
      "Epoch [874/1000], Loss: 0.1580\n",
      "Epoch [874/1000], Loss: 0.1468\n",
      "tensor(2.2951, grad_fn=<MeanBackward0>)\n",
      "874\n",
      "Epoch [875/1000], Loss: 0.0979\n",
      "Epoch [875/1000], Loss: 0.0821\n",
      "Epoch [875/1000], Loss: 0.1385\n",
      "Epoch [875/1000], Loss: 0.1476\n",
      "Epoch [875/1000], Loss: 0.1202\n",
      "Epoch [875/1000], Loss: 0.0924\n",
      "Epoch [875/1000], Loss: 0.0765\n",
      "Epoch [875/1000], Loss: 0.0572\n",
      "Epoch [875/1000], Loss: 0.0667\n",
      "Epoch [875/1000], Loss: 0.0738\n",
      "Epoch [875/1000], Loss: 0.0752\n",
      "tensor(2.2748, grad_fn=<MeanBackward0>)\n",
      "875\n",
      "Epoch [876/1000], Loss: 0.0872\n",
      "Epoch [876/1000], Loss: 0.0962\n",
      "Epoch [876/1000], Loss: 0.0774\n",
      "Epoch [876/1000], Loss: 0.0659\n",
      "Epoch [876/1000], Loss: 0.0952\n",
      "Epoch [876/1000], Loss: 0.1526\n",
      "Epoch [876/1000], Loss: 0.1668\n",
      "Epoch [876/1000], Loss: 0.1332\n",
      "Epoch [876/1000], Loss: 0.1095\n",
      "Epoch [876/1000], Loss: 0.0948\n",
      "Epoch [876/1000], Loss: 0.1422\n",
      "tensor(2.1048, grad_fn=<MeanBackward0>)\n",
      "876\n",
      "Epoch [877/1000], Loss: 0.1981\n",
      "Epoch [877/1000], Loss: 0.2226\n",
      "Epoch [877/1000], Loss: 0.2137\n",
      "Epoch [877/1000], Loss: 0.1667\n",
      "Epoch [877/1000], Loss: 0.1154\n",
      "Epoch [877/1000], Loss: 0.1404\n",
      "Epoch [877/1000], Loss: 0.2632\n",
      "Epoch [877/1000], Loss: 0.2717\n",
      "Epoch [877/1000], Loss: 0.3524\n",
      "Epoch [877/1000], Loss: 0.4089\n",
      "Epoch [877/1000], Loss: 0.3336\n",
      "tensor(2.3241, grad_fn=<MeanBackward0>)\n",
      "877\n",
      "Epoch [878/1000], Loss: 0.2148\n",
      "Epoch [878/1000], Loss: 0.1892\n",
      "Epoch [878/1000], Loss: 0.3058\n",
      "Epoch [878/1000], Loss: 0.5027\n",
      "Epoch [878/1000], Loss: 0.6117\n",
      "Epoch [878/1000], Loss: 0.7169\n",
      "Epoch [878/1000], Loss: 0.5777\n",
      "Epoch [878/1000], Loss: 0.5306\n",
      "Epoch [878/1000], Loss: 0.3711\n",
      "Epoch [878/1000], Loss: 0.2120\n",
      "Epoch [878/1000], Loss: 0.2836\n",
      "tensor(2.5148, grad_fn=<MeanBackward0>)\n",
      "878\n",
      "Epoch [879/1000], Loss: 0.3353\n",
      "Epoch [879/1000], Loss: 0.3174\n",
      "Epoch [879/1000], Loss: 0.2489\n",
      "Epoch [879/1000], Loss: 0.1482\n",
      "Epoch [879/1000], Loss: 0.1366\n",
      "Epoch [879/1000], Loss: 0.1907\n",
      "Epoch [879/1000], Loss: 0.1530\n",
      "Epoch [879/1000], Loss: 0.1066\n",
      "Epoch [879/1000], Loss: 0.1046\n",
      "Epoch [879/1000], Loss: 0.1395\n",
      "Epoch [879/1000], Loss: 0.0871\n",
      "tensor(2.3411, grad_fn=<MeanBackward0>)\n",
      "879\n",
      "Epoch [880/1000], Loss: 0.0881\n",
      "Epoch [880/1000], Loss: 0.1062\n",
      "Epoch [880/1000], Loss: 0.1078\n",
      "Epoch [880/1000], Loss: 0.1119\n",
      "Epoch [880/1000], Loss: 0.0927\n",
      "Epoch [880/1000], Loss: 0.1238\n",
      "Epoch [880/1000], Loss: 0.1308\n",
      "Epoch [880/1000], Loss: 0.0873\n",
      "Epoch [880/1000], Loss: 0.0725\n",
      "Epoch [880/1000], Loss: 0.1033\n",
      "Epoch [880/1000], Loss: 0.1331\n",
      "tensor(2.2150, grad_fn=<MeanBackward0>)\n",
      "880\n",
      "Epoch [881/1000], Loss: 0.1044\n",
      "Epoch [881/1000], Loss: 0.0921\n",
      "Epoch [881/1000], Loss: 0.0931\n",
      "Epoch [881/1000], Loss: 0.0970\n",
      "Epoch [881/1000], Loss: 0.0784\n",
      "Epoch [881/1000], Loss: 0.0960\n",
      "Epoch [881/1000], Loss: 0.1481\n",
      "Epoch [881/1000], Loss: 0.1652\n",
      "Epoch [881/1000], Loss: 0.2181\n",
      "Epoch [881/1000], Loss: 0.1966\n",
      "Epoch [881/1000], Loss: 0.1540\n",
      "tensor(2.2445, grad_fn=<MeanBackward0>)\n",
      "881\n",
      "Epoch [882/1000], Loss: 0.0983\n",
      "Epoch [882/1000], Loss: 0.1664\n",
      "Epoch [882/1000], Loss: 0.2596\n",
      "Epoch [882/1000], Loss: 0.3838\n",
      "Epoch [882/1000], Loss: 0.4298\n",
      "Epoch [882/1000], Loss: 0.4639\n",
      "Epoch [882/1000], Loss: 0.3320\n",
      "Epoch [882/1000], Loss: 0.2504\n",
      "Epoch [882/1000], Loss: 0.1515\n",
      "Epoch [882/1000], Loss: 0.3528\n",
      "Epoch [882/1000], Loss: 0.5193\n",
      "tensor(2.6265, grad_fn=<MeanBackward0>)\n",
      "882\n",
      "Epoch [883/1000], Loss: 0.5704\n",
      "Epoch [883/1000], Loss: 0.5269\n",
      "Epoch [883/1000], Loss: 0.3434\n",
      "Epoch [883/1000], Loss: 0.1319\n",
      "Epoch [883/1000], Loss: 0.1820\n",
      "Epoch [883/1000], Loss: 0.3416\n",
      "Epoch [883/1000], Loss: 0.3418\n",
      "Epoch [883/1000], Loss: 0.3574\n",
      "Epoch [883/1000], Loss: 0.2980\n",
      "Epoch [883/1000], Loss: 0.2385\n",
      "Epoch [883/1000], Loss: 0.1780\n",
      "tensor(2.3598, grad_fn=<MeanBackward0>)\n",
      "883\n",
      "Epoch [884/1000], Loss: 0.1649\n",
      "Epoch [884/1000], Loss: 0.2072\n",
      "Epoch [884/1000], Loss: 0.2064\n",
      "Epoch [884/1000], Loss: 0.2125\n",
      "Epoch [884/1000], Loss: 0.1894\n",
      "Epoch [884/1000], Loss: 0.1578\n",
      "Epoch [884/1000], Loss: 0.1089\n",
      "Epoch [884/1000], Loss: 0.1151\n",
      "Epoch [884/1000], Loss: 0.1519\n",
      "Epoch [884/1000], Loss: 0.1638\n",
      "Epoch [884/1000], Loss: 0.1828\n",
      "tensor(2.3027, grad_fn=<MeanBackward0>)\n",
      "884\n",
      "Epoch [885/1000], Loss: 0.1134\n",
      "Epoch [885/1000], Loss: 0.0856\n",
      "Epoch [885/1000], Loss: 0.1437\n",
      "Epoch [885/1000], Loss: 0.1412\n",
      "Epoch [885/1000], Loss: 0.1419\n",
      "Epoch [885/1000], Loss: 0.1105\n",
      "Epoch [885/1000], Loss: 0.0759\n",
      "Epoch [885/1000], Loss: 0.0696\n",
      "Epoch [885/1000], Loss: 0.0588\n",
      "Epoch [885/1000], Loss: 0.0665\n",
      "Epoch [885/1000], Loss: 0.0714\n",
      "tensor(2.2655, grad_fn=<MeanBackward0>)\n",
      "885\n",
      "Epoch [886/1000], Loss: 0.0636\n",
      "Epoch [886/1000], Loss: 0.0625\n",
      "Epoch [886/1000], Loss: 0.0694\n",
      "Epoch [886/1000], Loss: 0.0642\n",
      "Epoch [886/1000], Loss: 0.0890\n",
      "Epoch [886/1000], Loss: 0.1044\n",
      "Epoch [886/1000], Loss: 0.1081\n",
      "Epoch [886/1000], Loss: 0.0781\n",
      "Epoch [886/1000], Loss: 0.0608\n",
      "Epoch [886/1000], Loss: 0.0726\n",
      "Epoch [886/1000], Loss: 0.1058\n",
      "tensor(2.1768, grad_fn=<MeanBackward0>)\n",
      "886\n",
      "Epoch [887/1000], Loss: 0.1096\n",
      "Epoch [887/1000], Loss: 0.1139\n",
      "Epoch [887/1000], Loss: 0.1291\n",
      "Epoch [887/1000], Loss: 0.1075\n",
      "Epoch [887/1000], Loss: 0.0694\n",
      "Epoch [887/1000], Loss: 0.0951\n",
      "Epoch [887/1000], Loss: 0.1698\n",
      "Epoch [887/1000], Loss: 0.1983\n",
      "Epoch [887/1000], Loss: 0.2173\n",
      "Epoch [887/1000], Loss: 0.1811\n",
      "Epoch [887/1000], Loss: 0.1516\n",
      "tensor(2.2591, grad_fn=<MeanBackward0>)\n",
      "887\n",
      "Epoch [888/1000], Loss: 0.1316\n",
      "Epoch [888/1000], Loss: 0.1778\n",
      "Epoch [888/1000], Loss: 0.2861\n",
      "Epoch [888/1000], Loss: 0.4254\n",
      "Epoch [888/1000], Loss: 0.5032\n",
      "Epoch [888/1000], Loss: 0.5562\n",
      "Epoch [888/1000], Loss: 0.4185\n",
      "Epoch [888/1000], Loss: 0.3163\n",
      "Epoch [888/1000], Loss: 0.1237\n",
      "Epoch [888/1000], Loss: 0.2976\n",
      "Epoch [888/1000], Loss: 0.4897\n",
      "tensor(2.6129, grad_fn=<MeanBackward0>)\n",
      "888\n",
      "Epoch [889/1000], Loss: 0.5603\n",
      "Epoch [889/1000], Loss: 0.5285\n",
      "Epoch [889/1000], Loss: 0.3337\n",
      "Epoch [889/1000], Loss: 0.1379\n",
      "Epoch [889/1000], Loss: 0.1473\n",
      "Epoch [889/1000], Loss: 0.2828\n",
      "Epoch [889/1000], Loss: 0.2881\n",
      "Epoch [889/1000], Loss: 0.2673\n",
      "Epoch [889/1000], Loss: 0.2051\n",
      "Epoch [889/1000], Loss: 0.1924\n",
      "Epoch [889/1000], Loss: 0.1535\n",
      "tensor(2.3275, grad_fn=<MeanBackward0>)\n",
      "889\n",
      "Epoch [890/1000], Loss: 0.1491\n",
      "Epoch [890/1000], Loss: 0.1654\n",
      "Epoch [890/1000], Loss: 0.1364\n",
      "Epoch [890/1000], Loss: 0.1378\n",
      "Epoch [890/1000], Loss: 0.1590\n",
      "Epoch [890/1000], Loss: 0.1580\n",
      "Epoch [890/1000], Loss: 0.1260\n",
      "Epoch [890/1000], Loss: 0.0972\n",
      "Epoch [890/1000], Loss: 0.1195\n",
      "Epoch [890/1000], Loss: 0.1635\n",
      "Epoch [890/1000], Loss: 0.2103\n",
      "tensor(2.2307, grad_fn=<MeanBackward0>)\n",
      "890\n",
      "Epoch [891/1000], Loss: 0.1543\n",
      "Epoch [891/1000], Loss: 0.1127\n",
      "Epoch [891/1000], Loss: 0.0959\n",
      "Epoch [891/1000], Loss: 0.1274\n",
      "Epoch [891/1000], Loss: 0.1300\n",
      "Epoch [891/1000], Loss: 0.1238\n",
      "Epoch [891/1000], Loss: 0.1001\n",
      "Epoch [891/1000], Loss: 0.0739\n",
      "Epoch [891/1000], Loss: 0.0608\n",
      "Epoch [891/1000], Loss: 0.0735\n",
      "Epoch [891/1000], Loss: 0.0832\n",
      "tensor(2.2223, grad_fn=<MeanBackward0>)\n",
      "891\n",
      "Epoch [892/1000], Loss: 0.0764\n",
      "Epoch [892/1000], Loss: 0.0825\n",
      "Epoch [892/1000], Loss: 0.0992\n",
      "Epoch [892/1000], Loss: 0.0746\n",
      "Epoch [892/1000], Loss: 0.0744\n",
      "Epoch [892/1000], Loss: 0.1031\n",
      "Epoch [892/1000], Loss: 0.1382\n",
      "Epoch [892/1000], Loss: 0.1525\n",
      "Epoch [892/1000], Loss: 0.1346\n",
      "Epoch [892/1000], Loss: 0.0785\n",
      "Epoch [892/1000], Loss: 0.0876\n",
      "tensor(2.1642, grad_fn=<MeanBackward0>)\n",
      "892\n",
      "Epoch [893/1000], Loss: 0.0948\n",
      "Epoch [893/1000], Loss: 0.1524\n",
      "Epoch [893/1000], Loss: 0.2233\n",
      "Epoch [893/1000], Loss: 0.2734\n",
      "Epoch [893/1000], Loss: 0.2650\n",
      "Epoch [893/1000], Loss: 0.2306\n",
      "Epoch [893/1000], Loss: 0.1496\n",
      "Epoch [893/1000], Loss: 0.1594\n",
      "Epoch [893/1000], Loss: 0.3531\n",
      "Epoch [893/1000], Loss: 0.5221\n",
      "Epoch [893/1000], Loss: 0.6307\n",
      "tensor(2.6130, grad_fn=<MeanBackward0>)\n",
      "893\n",
      "Epoch [894/1000], Loss: 0.6132\n",
      "Epoch [894/1000], Loss: 0.4877\n",
      "Epoch [894/1000], Loss: 0.2436\n",
      "Epoch [894/1000], Loss: 0.1933\n",
      "Epoch [894/1000], Loss: 0.3703\n",
      "Epoch [894/1000], Loss: 0.5639\n",
      "Epoch [894/1000], Loss: 0.5528\n",
      "Epoch [894/1000], Loss: 0.6140\n",
      "Epoch [894/1000], Loss: 0.5274\n",
      "Epoch [894/1000], Loss: 0.3081\n",
      "Epoch [894/1000], Loss: 0.2244\n",
      "tensor(2.3711, grad_fn=<MeanBackward0>)\n",
      "894\n",
      "Epoch [895/1000], Loss: 0.1620\n",
      "Epoch [895/1000], Loss: 0.2013\n",
      "Epoch [895/1000], Loss: 0.1894\n",
      "Epoch [895/1000], Loss: 0.1859\n",
      "Epoch [895/1000], Loss: 0.1724\n",
      "Epoch [895/1000], Loss: 0.1706\n",
      "Epoch [895/1000], Loss: 0.1363\n",
      "Epoch [895/1000], Loss: 0.1298\n",
      "Epoch [895/1000], Loss: 0.1370\n",
      "Epoch [895/1000], Loss: 0.1375\n",
      "Epoch [895/1000], Loss: 0.1584\n",
      "tensor(2.2073, grad_fn=<MeanBackward0>)\n",
      "895\n",
      "Epoch [896/1000], Loss: 0.1853\n",
      "Epoch [896/1000], Loss: 0.1790\n",
      "Epoch [896/1000], Loss: 0.1580\n",
      "Epoch [896/1000], Loss: 0.1398\n",
      "Epoch [896/1000], Loss: 0.1838\n",
      "Epoch [896/1000], Loss: 0.2348\n",
      "Epoch [896/1000], Loss: 0.2482\n",
      "Epoch [896/1000], Loss: 0.1554\n",
      "Epoch [896/1000], Loss: 0.1328\n",
      "Epoch [896/1000], Loss: 0.1091\n",
      "Epoch [896/1000], Loss: 0.1198\n",
      "tensor(2.1393, grad_fn=<MeanBackward0>)\n",
      "896\n",
      "Epoch [897/1000], Loss: 0.1017\n",
      "Epoch [897/1000], Loss: 0.1443\n",
      "Epoch [897/1000], Loss: 0.1849\n",
      "Epoch [897/1000], Loss: 0.2292\n",
      "Epoch [897/1000], Loss: 0.2068\n",
      "Epoch [897/1000], Loss: 0.1845\n",
      "Epoch [897/1000], Loss: 0.1626\n",
      "Epoch [897/1000], Loss: 0.1757\n",
      "Epoch [897/1000], Loss: 0.2882\n",
      "Epoch [897/1000], Loss: 0.3948\n",
      "Epoch [897/1000], Loss: 0.4405\n",
      "tensor(2.5281, grad_fn=<MeanBackward0>)\n",
      "897\n",
      "Epoch [898/1000], Loss: 0.3972\n",
      "Epoch [898/1000], Loss: 0.3307\n",
      "Epoch [898/1000], Loss: 0.1941\n",
      "Epoch [898/1000], Loss: 0.2393\n",
      "Epoch [898/1000], Loss: 0.4162\n",
      "Epoch [898/1000], Loss: 0.5887\n",
      "Epoch [898/1000], Loss: 0.5852\n",
      "Epoch [898/1000], Loss: 0.6300\n",
      "Epoch [898/1000], Loss: 0.5169\n",
      "Epoch [898/1000], Loss: 0.2623\n",
      "Epoch [898/1000], Loss: 0.1415\n",
      "tensor(2.4077, grad_fn=<MeanBackward0>)\n",
      "898\n",
      "Epoch [899/1000], Loss: 0.2004\n",
      "Epoch [899/1000], Loss: 0.2914\n",
      "Epoch [899/1000], Loss: 0.2438\n",
      "Epoch [899/1000], Loss: 0.1503\n",
      "Epoch [899/1000], Loss: 0.1299\n",
      "Epoch [899/1000], Loss: 0.1770\n",
      "Epoch [899/1000], Loss: 0.1912\n",
      "Epoch [899/1000], Loss: 0.1470\n",
      "Epoch [899/1000], Loss: 0.1241\n",
      "Epoch [899/1000], Loss: 0.1378\n",
      "Epoch [899/1000], Loss: 0.1149\n",
      "tensor(2.2979, grad_fn=<MeanBackward0>)\n",
      "899\n",
      "Epoch [900/1000], Loss: 0.1292\n",
      "Epoch [900/1000], Loss: 0.1373\n",
      "Epoch [900/1000], Loss: 0.1414\n",
      "Epoch [900/1000], Loss: 0.0990\n",
      "Epoch [900/1000], Loss: 0.1395\n",
      "Epoch [900/1000], Loss: 0.1600\n",
      "Epoch [900/1000], Loss: 0.1187\n",
      "Epoch [900/1000], Loss: 0.0800\n",
      "Epoch [900/1000], Loss: 0.0866\n",
      "Epoch [900/1000], Loss: 0.0939\n",
      "Epoch [900/1000], Loss: 0.1259\n",
      "tensor(2.2199, grad_fn=<MeanBackward0>)\n",
      "900\n",
      "Epoch [901/1000], Loss: 0.1062\n",
      "Epoch [901/1000], Loss: 0.1005\n",
      "Epoch [901/1000], Loss: 0.0875\n",
      "Epoch [901/1000], Loss: 0.0878\n",
      "Epoch [901/1000], Loss: 0.0998\n",
      "Epoch [901/1000], Loss: 0.0850\n",
      "Epoch [901/1000], Loss: 0.0759\n",
      "Epoch [901/1000], Loss: 0.0723\n",
      "Epoch [901/1000], Loss: 0.0871\n",
      "Epoch [901/1000], Loss: 0.0781\n",
      "Epoch [901/1000], Loss: 0.0756\n",
      "tensor(2.1980, grad_fn=<MeanBackward0>)\n",
      "901\n",
      "Epoch [902/1000], Loss: 0.1084\n",
      "Epoch [902/1000], Loss: 0.1435\n",
      "Epoch [902/1000], Loss: 0.1599\n",
      "Epoch [902/1000], Loss: 0.1196\n",
      "Epoch [902/1000], Loss: 0.0974\n",
      "Epoch [902/1000], Loss: 0.0851\n",
      "Epoch [902/1000], Loss: 0.1071\n",
      "Epoch [902/1000], Loss: 0.1336\n",
      "Epoch [902/1000], Loss: 0.2028\n",
      "Epoch [902/1000], Loss: 0.2631\n",
      "Epoch [902/1000], Loss: 0.2760\n",
      "tensor(2.3272, grad_fn=<MeanBackward0>)\n",
      "902\n",
      "Epoch [903/1000], Loss: 0.1869\n",
      "Epoch [903/1000], Loss: 0.1123\n",
      "Epoch [903/1000], Loss: 0.2080\n",
      "Epoch [903/1000], Loss: 0.3777\n",
      "Epoch [903/1000], Loss: 0.5230\n",
      "Epoch [903/1000], Loss: 0.6351\n",
      "Epoch [903/1000], Loss: 0.5617\n",
      "Epoch [903/1000], Loss: 0.5339\n",
      "Epoch [903/1000], Loss: 0.3799\n",
      "Epoch [903/1000], Loss: 0.1851\n",
      "Epoch [903/1000], Loss: 0.2417\n",
      "tensor(2.5389, grad_fn=<MeanBackward0>)\n",
      "903\n",
      "Epoch [904/1000], Loss: 0.3764\n",
      "Epoch [904/1000], Loss: 0.4528\n",
      "Epoch [904/1000], Loss: 0.4123\n",
      "Epoch [904/1000], Loss: 0.2699\n",
      "Epoch [904/1000], Loss: 0.1630\n",
      "Epoch [904/1000], Loss: 0.2192\n",
      "Epoch [904/1000], Loss: 0.2831\n",
      "Epoch [904/1000], Loss: 0.2502\n",
      "Epoch [904/1000], Loss: 0.2229\n",
      "Epoch [904/1000], Loss: 0.1875\n",
      "Epoch [904/1000], Loss: 0.1643\n",
      "tensor(2.3054, grad_fn=<MeanBackward0>)\n",
      "904\n",
      "Epoch [905/1000], Loss: 0.1465\n",
      "Epoch [905/1000], Loss: 0.1619\n",
      "Epoch [905/1000], Loss: 0.1428\n",
      "Epoch [905/1000], Loss: 0.1433\n",
      "Epoch [905/1000], Loss: 0.1244\n",
      "Epoch [905/1000], Loss: 0.1410\n",
      "Epoch [905/1000], Loss: 0.1112\n",
      "Epoch [905/1000], Loss: 0.0793\n",
      "Epoch [905/1000], Loss: 0.1043\n",
      "Epoch [905/1000], Loss: 0.1201\n",
      "Epoch [905/1000], Loss: 0.1243\n",
      "tensor(2.2465, grad_fn=<MeanBackward0>)\n",
      "905\n",
      "Epoch [906/1000], Loss: 0.1027\n",
      "Epoch [906/1000], Loss: 0.0785\n",
      "Epoch [906/1000], Loss: 0.1111\n",
      "Epoch [906/1000], Loss: 0.0811\n",
      "Epoch [906/1000], Loss: 0.0929\n",
      "Epoch [906/1000], Loss: 0.0766\n",
      "Epoch [906/1000], Loss: 0.0781\n",
      "Epoch [906/1000], Loss: 0.0682\n",
      "Epoch [906/1000], Loss: 0.0743\n",
      "Epoch [906/1000], Loss: 0.0637\n",
      "Epoch [906/1000], Loss: 0.0904\n",
      "tensor(2.2263, grad_fn=<MeanBackward0>)\n",
      "906\n",
      "Epoch [907/1000], Loss: 0.1134\n",
      "Epoch [907/1000], Loss: 0.1429\n",
      "Epoch [907/1000], Loss: 0.1383\n",
      "Epoch [907/1000], Loss: 0.1045\n",
      "Epoch [907/1000], Loss: 0.0565\n",
      "Epoch [907/1000], Loss: 0.0955\n",
      "Epoch [907/1000], Loss: 0.1215\n",
      "Epoch [907/1000], Loss: 0.1320\n",
      "Epoch [907/1000], Loss: 0.1806\n",
      "Epoch [907/1000], Loss: 0.1487\n",
      "Epoch [907/1000], Loss: 0.1143\n",
      "tensor(2.2148, grad_fn=<MeanBackward0>)\n",
      "907\n",
      "Epoch [908/1000], Loss: 0.0914\n",
      "Epoch [908/1000], Loss: 0.1387\n",
      "Epoch [908/1000], Loss: 0.2180\n",
      "Epoch [908/1000], Loss: 0.2926\n",
      "Epoch [908/1000], Loss: 0.3392\n",
      "Epoch [908/1000], Loss: 0.3595\n",
      "Epoch [908/1000], Loss: 0.2707\n",
      "Epoch [908/1000], Loss: 0.2309\n",
      "Epoch [908/1000], Loss: 0.1693\n",
      "Epoch [908/1000], Loss: 0.3256\n",
      "Epoch [908/1000], Loss: 0.5514\n",
      "tensor(2.6698, grad_fn=<MeanBackward0>)\n",
      "908\n",
      "Epoch [909/1000], Loss: 0.6163\n",
      "Epoch [909/1000], Loss: 0.5883\n",
      "Epoch [909/1000], Loss: 0.4391\n",
      "Epoch [909/1000], Loss: 0.2067\n",
      "Epoch [909/1000], Loss: 0.1715\n",
      "Epoch [909/1000], Loss: 0.3279\n",
      "Epoch [909/1000], Loss: 0.4004\n",
      "Epoch [909/1000], Loss: 0.4767\n",
      "Epoch [909/1000], Loss: 0.4364\n",
      "Epoch [909/1000], Loss: 0.3276\n",
      "Epoch [909/1000], Loss: 0.2233\n",
      "tensor(2.2930, grad_fn=<MeanBackward0>)\n",
      "909\n",
      "Epoch [910/1000], Loss: 0.1375\n",
      "Epoch [910/1000], Loss: 0.1560\n",
      "Epoch [910/1000], Loss: 0.1665\n",
      "Epoch [910/1000], Loss: 0.1731\n",
      "Epoch [910/1000], Loss: 0.1589\n",
      "Epoch [910/1000], Loss: 0.1344\n",
      "Epoch [910/1000], Loss: 0.1123\n",
      "Epoch [910/1000], Loss: 0.1131\n",
      "Epoch [910/1000], Loss: 0.1091\n",
      "Epoch [910/1000], Loss: 0.1164\n",
      "Epoch [910/1000], Loss: 0.1411\n",
      "tensor(2.2417, grad_fn=<MeanBackward0>)\n",
      "910\n",
      "Epoch [911/1000], Loss: 0.1286\n",
      "Epoch [911/1000], Loss: 0.1049\n",
      "Epoch [911/1000], Loss: 0.1023\n",
      "Epoch [911/1000], Loss: 0.1111\n",
      "Epoch [911/1000], Loss: 0.1582\n",
      "Epoch [911/1000], Loss: 0.1677\n",
      "Epoch [911/1000], Loss: 0.1492\n",
      "Epoch [911/1000], Loss: 0.0819\n",
      "Epoch [911/1000], Loss: 0.0842\n",
      "Epoch [911/1000], Loss: 0.0781\n",
      "Epoch [911/1000], Loss: 0.0880\n",
      "tensor(2.2013, grad_fn=<MeanBackward0>)\n",
      "911\n",
      "Epoch [912/1000], Loss: 0.0854\n",
      "Epoch [912/1000], Loss: 0.0996\n",
      "Epoch [912/1000], Loss: 0.1191\n",
      "Epoch [912/1000], Loss: 0.1017\n",
      "Epoch [912/1000], Loss: 0.0618\n",
      "Epoch [912/1000], Loss: 0.0959\n",
      "Epoch [912/1000], Loss: 0.1557\n",
      "Epoch [912/1000], Loss: 0.1582\n",
      "Epoch [912/1000], Loss: 0.1689\n",
      "Epoch [912/1000], Loss: 0.1232\n",
      "Epoch [912/1000], Loss: 0.0792\n",
      "tensor(2.2350, grad_fn=<MeanBackward0>)\n",
      "912\n",
      "Epoch [913/1000], Loss: 0.1045\n",
      "Epoch [913/1000], Loss: 0.1326\n",
      "Epoch [913/1000], Loss: 0.2020\n",
      "Epoch [913/1000], Loss: 0.3013\n",
      "Epoch [913/1000], Loss: 0.3651\n",
      "Epoch [913/1000], Loss: 0.4073\n",
      "Epoch [913/1000], Loss: 0.3097\n",
      "Epoch [913/1000], Loss: 0.1973\n",
      "Epoch [913/1000], Loss: 0.1333\n",
      "Epoch [913/1000], Loss: 0.3466\n",
      "Epoch [913/1000], Loss: 0.5387\n",
      "tensor(2.6335, grad_fn=<MeanBackward0>)\n",
      "913\n",
      "Epoch [914/1000], Loss: 0.6058\n",
      "Epoch [914/1000], Loss: 0.5518\n",
      "Epoch [914/1000], Loss: 0.3765\n",
      "Epoch [914/1000], Loss: 0.1677\n",
      "Epoch [914/1000], Loss: 0.1697\n",
      "Epoch [914/1000], Loss: 0.3460\n",
      "Epoch [914/1000], Loss: 0.3893\n",
      "Epoch [914/1000], Loss: 0.4550\n",
      "Epoch [914/1000], Loss: 0.4196\n",
      "Epoch [914/1000], Loss: 0.2966\n",
      "Epoch [914/1000], Loss: 0.2165\n",
      "tensor(2.2981, grad_fn=<MeanBackward0>)\n",
      "914\n",
      "Epoch [915/1000], Loss: 0.1469\n",
      "Epoch [915/1000], Loss: 0.1471\n",
      "Epoch [915/1000], Loss: 0.1471\n",
      "Epoch [915/1000], Loss: 0.1447\n",
      "Epoch [915/1000], Loss: 0.1389\n",
      "Epoch [915/1000], Loss: 0.1365\n",
      "Epoch [915/1000], Loss: 0.1311\n",
      "Epoch [915/1000], Loss: 0.1491\n",
      "Epoch [915/1000], Loss: 0.1244\n",
      "Epoch [915/1000], Loss: 0.1184\n",
      "Epoch [915/1000], Loss: 0.1684\n",
      "tensor(2.2013, grad_fn=<MeanBackward0>)\n",
      "915\n",
      "Epoch [916/1000], Loss: 0.1878\n",
      "Epoch [916/1000], Loss: 0.1878\n",
      "Epoch [916/1000], Loss: 0.1532\n",
      "Epoch [916/1000], Loss: 0.1400\n",
      "Epoch [916/1000], Loss: 0.1772\n",
      "Epoch [916/1000], Loss: 0.2278\n",
      "Epoch [916/1000], Loss: 0.2527\n",
      "Epoch [916/1000], Loss: 0.1539\n",
      "Epoch [916/1000], Loss: 0.1324\n",
      "Epoch [916/1000], Loss: 0.1200\n",
      "Epoch [916/1000], Loss: 0.1055\n",
      "tensor(2.1521, grad_fn=<MeanBackward0>)\n",
      "916\n",
      "Epoch [917/1000], Loss: 0.1087\n",
      "Epoch [917/1000], Loss: 0.1184\n",
      "Epoch [917/1000], Loss: 0.1642\n",
      "Epoch [917/1000], Loss: 0.2026\n",
      "Epoch [917/1000], Loss: 0.1883\n",
      "Epoch [917/1000], Loss: 0.1785\n",
      "Epoch [917/1000], Loss: 0.1394\n",
      "Epoch [917/1000], Loss: 0.1364\n",
      "Epoch [917/1000], Loss: 0.2460\n",
      "Epoch [917/1000], Loss: 0.3336\n",
      "Epoch [917/1000], Loss: 0.3977\n",
      "tensor(2.5447, grad_fn=<MeanBackward0>)\n",
      "917\n",
      "Epoch [918/1000], Loss: 0.3689\n",
      "Epoch [918/1000], Loss: 0.3131\n",
      "Epoch [918/1000], Loss: 0.2039\n",
      "Epoch [918/1000], Loss: 0.2389\n",
      "Epoch [918/1000], Loss: 0.3791\n",
      "Epoch [918/1000], Loss: 0.5324\n",
      "Epoch [918/1000], Loss: 0.5468\n",
      "Epoch [918/1000], Loss: 0.5948\n",
      "Epoch [918/1000], Loss: 0.5090\n",
      "Epoch [918/1000], Loss: 0.2768\n",
      "Epoch [918/1000], Loss: 0.1225\n",
      "tensor(2.4277, grad_fn=<MeanBackward0>)\n",
      "918\n",
      "Epoch [919/1000], Loss: 0.2151\n",
      "Epoch [919/1000], Loss: 0.3320\n",
      "Epoch [919/1000], Loss: 0.2971\n",
      "Epoch [919/1000], Loss: 0.1792\n",
      "Epoch [919/1000], Loss: 0.1083\n",
      "Epoch [919/1000], Loss: 0.1807\n",
      "Epoch [919/1000], Loss: 0.1940\n",
      "Epoch [919/1000], Loss: 0.1652\n",
      "Epoch [919/1000], Loss: 0.1331\n",
      "Epoch [919/1000], Loss: 0.1366\n",
      "Epoch [919/1000], Loss: 0.1194\n",
      "tensor(2.2958, grad_fn=<MeanBackward0>)\n",
      "919\n",
      "Epoch [920/1000], Loss: 0.1181\n",
      "Epoch [920/1000], Loss: 0.1231\n",
      "Epoch [920/1000], Loss: 0.1197\n",
      "Epoch [920/1000], Loss: 0.0888\n",
      "Epoch [920/1000], Loss: 0.1211\n",
      "Epoch [920/1000], Loss: 0.1268\n",
      "Epoch [920/1000], Loss: 0.1042\n",
      "Epoch [920/1000], Loss: 0.0751\n",
      "Epoch [920/1000], Loss: 0.0685\n",
      "Epoch [920/1000], Loss: 0.0733\n",
      "Epoch [920/1000], Loss: 0.0911\n",
      "tensor(2.2443, grad_fn=<MeanBackward0>)\n",
      "920\n",
      "Epoch [921/1000], Loss: 0.0920\n",
      "Epoch [921/1000], Loss: 0.0993\n",
      "Epoch [921/1000], Loss: 0.1045\n",
      "Epoch [921/1000], Loss: 0.0969\n",
      "Epoch [921/1000], Loss: 0.1051\n",
      "Epoch [921/1000], Loss: 0.1005\n",
      "Epoch [921/1000], Loss: 0.1025\n",
      "Epoch [921/1000], Loss: 0.0918\n",
      "Epoch [921/1000], Loss: 0.1023\n",
      "Epoch [921/1000], Loss: 0.0715\n",
      "Epoch [921/1000], Loss: 0.0645\n",
      "tensor(2.2217, grad_fn=<MeanBackward0>)\n",
      "921\n",
      "Epoch [922/1000], Loss: 0.0956\n",
      "Epoch [922/1000], Loss: 0.1290\n",
      "Epoch [922/1000], Loss: 0.1504\n",
      "Epoch [922/1000], Loss: 0.1425\n",
      "Epoch [922/1000], Loss: 0.1130\n",
      "Epoch [922/1000], Loss: 0.1001\n",
      "Epoch [922/1000], Loss: 0.1015\n",
      "Epoch [922/1000], Loss: 0.1072\n",
      "Epoch [922/1000], Loss: 0.1813\n",
      "Epoch [922/1000], Loss: 0.2523\n",
      "Epoch [922/1000], Loss: 0.2587\n",
      "tensor(2.3623, grad_fn=<MeanBackward0>)\n",
      "922\n",
      "Epoch [923/1000], Loss: 0.2147\n",
      "Epoch [923/1000], Loss: 0.1434\n",
      "Epoch [923/1000], Loss: 0.1252\n",
      "Epoch [923/1000], Loss: 0.2880\n",
      "Epoch [923/1000], Loss: 0.4355\n",
      "Epoch [923/1000], Loss: 0.5660\n",
      "Epoch [923/1000], Loss: 0.5298\n",
      "Epoch [923/1000], Loss: 0.5334\n",
      "Epoch [923/1000], Loss: 0.4308\n",
      "Epoch [923/1000], Loss: 0.2165\n",
      "Epoch [923/1000], Loss: 0.1828\n",
      "tensor(2.5336, grad_fn=<MeanBackward0>)\n",
      "923\n",
      "Epoch [924/1000], Loss: 0.3337\n",
      "Epoch [924/1000], Loss: 0.4502\n",
      "Epoch [924/1000], Loss: 0.4652\n",
      "Epoch [924/1000], Loss: 0.3243\n",
      "Epoch [924/1000], Loss: 0.2187\n",
      "Epoch [924/1000], Loss: 0.2460\n",
      "Epoch [924/1000], Loss: 0.2867\n",
      "Epoch [924/1000], Loss: 0.2778\n",
      "Epoch [924/1000], Loss: 0.3076\n",
      "Epoch [924/1000], Loss: 0.2828\n",
      "Epoch [924/1000], Loss: 0.2636\n",
      "tensor(2.2580, grad_fn=<MeanBackward0>)\n",
      "924\n",
      "Epoch [925/1000], Loss: 0.1674\n",
      "Epoch [925/1000], Loss: 0.1669\n",
      "Epoch [925/1000], Loss: 0.1888\n",
      "Epoch [925/1000], Loss: 0.2055\n",
      "Epoch [925/1000], Loss: 0.1932\n",
      "Epoch [925/1000], Loss: 0.1966\n",
      "Epoch [925/1000], Loss: 0.1470\n",
      "Epoch [925/1000], Loss: 0.1009\n",
      "Epoch [925/1000], Loss: 0.1157\n",
      "Epoch [925/1000], Loss: 0.1602\n",
      "Epoch [925/1000], Loss: 0.1984\n",
      "tensor(2.1901, grad_fn=<MeanBackward0>)\n",
      "925\n",
      "Epoch [926/1000], Loss: 0.1922\n",
      "Epoch [926/1000], Loss: 0.1250\n",
      "Epoch [926/1000], Loss: 0.0907\n",
      "Epoch [926/1000], Loss: 0.0931\n",
      "Epoch [926/1000], Loss: 0.1095\n",
      "Epoch [926/1000], Loss: 0.1009\n",
      "Epoch [926/1000], Loss: 0.0832\n",
      "Epoch [926/1000], Loss: 0.0772\n",
      "Epoch [926/1000], Loss: 0.0724\n",
      "Epoch [926/1000], Loss: 0.0585\n",
      "Epoch [926/1000], Loss: 0.0770\n",
      "tensor(2.2360, grad_fn=<MeanBackward0>)\n",
      "926\n",
      "Epoch [927/1000], Loss: 0.1030\n",
      "Epoch [927/1000], Loss: 0.1320\n",
      "Epoch [927/1000], Loss: 0.1350\n",
      "Epoch [927/1000], Loss: 0.1342\n",
      "Epoch [927/1000], Loss: 0.0864\n",
      "Epoch [927/1000], Loss: 0.0873\n",
      "Epoch [927/1000], Loss: 0.1163\n",
      "Epoch [927/1000], Loss: 0.1381\n",
      "Epoch [927/1000], Loss: 0.1598\n",
      "Epoch [927/1000], Loss: 0.1419\n",
      "Epoch [927/1000], Loss: 0.0884\n",
      "tensor(2.1989, grad_fn=<MeanBackward0>)\n",
      "927\n",
      "Epoch [928/1000], Loss: 0.0927\n",
      "Epoch [928/1000], Loss: 0.1229\n",
      "Epoch [928/1000], Loss: 0.1799\n",
      "Epoch [928/1000], Loss: 0.2484\n",
      "Epoch [928/1000], Loss: 0.2691\n",
      "Epoch [928/1000], Loss: 0.2740\n",
      "Epoch [928/1000], Loss: 0.2117\n",
      "Epoch [928/1000], Loss: 0.1642\n",
      "Epoch [928/1000], Loss: 0.1547\n",
      "Epoch [928/1000], Loss: 0.3528\n",
      "Epoch [928/1000], Loss: 0.5345\n",
      "tensor(2.6655, grad_fn=<MeanBackward0>)\n",
      "928\n",
      "Epoch [929/1000], Loss: 0.5994\n",
      "Epoch [929/1000], Loss: 0.5764\n",
      "Epoch [929/1000], Loss: 0.4202\n",
      "Epoch [929/1000], Loss: 0.1975\n",
      "Epoch [929/1000], Loss: 0.1609\n",
      "Epoch [929/1000], Loss: 0.3409\n",
      "Epoch [929/1000], Loss: 0.4224\n",
      "Epoch [929/1000], Loss: 0.5266\n",
      "Epoch [929/1000], Loss: 0.5063\n",
      "Epoch [929/1000], Loss: 0.3595\n",
      "Epoch [929/1000], Loss: 0.2502\n",
      "tensor(2.2695, grad_fn=<MeanBackward0>)\n",
      "929\n",
      "Epoch [930/1000], Loss: 0.1502\n",
      "Epoch [930/1000], Loss: 0.1680\n",
      "Epoch [930/1000], Loss: 0.1902\n",
      "Epoch [930/1000], Loss: 0.1834\n",
      "Epoch [930/1000], Loss: 0.1688\n",
      "Epoch [930/1000], Loss: 0.1681\n",
      "Epoch [930/1000], Loss: 0.1538\n",
      "Epoch [930/1000], Loss: 0.1455\n",
      "Epoch [930/1000], Loss: 0.1358\n",
      "Epoch [930/1000], Loss: 0.1694\n",
      "Epoch [930/1000], Loss: 0.2142\n",
      "tensor(2.1607, grad_fn=<MeanBackward0>)\n",
      "930\n",
      "Epoch [931/1000], Loss: 0.2250\n",
      "Epoch [931/1000], Loss: 0.1861\n",
      "Epoch [931/1000], Loss: 0.1489\n",
      "Epoch [931/1000], Loss: 0.1077\n",
      "Epoch [931/1000], Loss: 0.1799\n",
      "Epoch [931/1000], Loss: 0.2455\n",
      "Epoch [931/1000], Loss: 0.2335\n",
      "Epoch [931/1000], Loss: 0.1221\n",
      "Epoch [931/1000], Loss: 0.0946\n",
      "Epoch [931/1000], Loss: 0.1165\n",
      "Epoch [931/1000], Loss: 0.1056\n",
      "tensor(2.2083, grad_fn=<MeanBackward0>)\n",
      "931\n",
      "Epoch [932/1000], Loss: 0.0885\n",
      "Epoch [932/1000], Loss: 0.0991\n",
      "Epoch [932/1000], Loss: 0.0962\n",
      "Epoch [932/1000], Loss: 0.1460\n",
      "Epoch [932/1000], Loss: 0.1606\n",
      "Epoch [932/1000], Loss: 0.1658\n",
      "Epoch [932/1000], Loss: 0.1025\n",
      "Epoch [932/1000], Loss: 0.0927\n",
      "Epoch [932/1000], Loss: 0.2215\n",
      "Epoch [932/1000], Loss: 0.2692\n",
      "Epoch [932/1000], Loss: 0.2850\n",
      "tensor(2.4532, grad_fn=<MeanBackward0>)\n",
      "932\n",
      "Epoch [933/1000], Loss: 0.2075\n",
      "Epoch [933/1000], Loss: 0.1541\n",
      "Epoch [933/1000], Loss: 0.1709\n",
      "Epoch [933/1000], Loss: 0.2504\n",
      "Epoch [933/1000], Loss: 0.3463\n",
      "Epoch [933/1000], Loss: 0.4450\n",
      "Epoch [933/1000], Loss: 0.4394\n",
      "Epoch [933/1000], Loss: 0.4904\n",
      "Epoch [933/1000], Loss: 0.3886\n",
      "Epoch [933/1000], Loss: 0.1781\n",
      "Epoch [933/1000], Loss: 0.1275\n",
      "tensor(2.4633, grad_fn=<MeanBackward0>)\n",
      "933\n",
      "Epoch [934/1000], Loss: 0.2528\n",
      "Epoch [934/1000], Loss: 0.3587\n",
      "Epoch [934/1000], Loss: 0.3270\n",
      "Epoch [934/1000], Loss: 0.2096\n",
      "Epoch [934/1000], Loss: 0.0902\n",
      "Epoch [934/1000], Loss: 0.1778\n",
      "Epoch [934/1000], Loss: 0.2101\n",
      "Epoch [934/1000], Loss: 0.2091\n",
      "Epoch [934/1000], Loss: 0.1569\n",
      "Epoch [934/1000], Loss: 0.1414\n",
      "Epoch [934/1000], Loss: 0.1340\n",
      "tensor(2.3545, grad_fn=<MeanBackward0>)\n",
      "934\n",
      "Epoch [935/1000], Loss: 0.1229\n",
      "Epoch [935/1000], Loss: 0.1192\n",
      "Epoch [935/1000], Loss: 0.1133\n",
      "Epoch [935/1000], Loss: 0.1138\n",
      "Epoch [935/1000], Loss: 0.0998\n",
      "Epoch [935/1000], Loss: 0.1301\n",
      "Epoch [935/1000], Loss: 0.1291\n",
      "Epoch [935/1000], Loss: 0.1153\n",
      "Epoch [935/1000], Loss: 0.1029\n",
      "Epoch [935/1000], Loss: 0.0978\n",
      "Epoch [935/1000], Loss: 0.1415\n",
      "tensor(2.2210, grad_fn=<MeanBackward0>)\n",
      "935\n",
      "Epoch [936/1000], Loss: 0.1626\n",
      "Epoch [936/1000], Loss: 0.1321\n",
      "Epoch [936/1000], Loss: 0.0784\n",
      "Epoch [936/1000], Loss: 0.0902\n",
      "Epoch [936/1000], Loss: 0.1210\n",
      "Epoch [936/1000], Loss: 0.1270\n",
      "Epoch [936/1000], Loss: 0.1046\n",
      "Epoch [936/1000], Loss: 0.0954\n",
      "Epoch [936/1000], Loss: 0.0836\n",
      "Epoch [936/1000], Loss: 0.0829\n",
      "Epoch [936/1000], Loss: 0.0715\n",
      "tensor(2.2309, grad_fn=<MeanBackward0>)\n",
      "936\n",
      "Epoch [937/1000], Loss: 0.0709\n",
      "Epoch [937/1000], Loss: 0.1011\n",
      "Epoch [937/1000], Loss: 0.1420\n",
      "Epoch [937/1000], Loss: 0.1400\n",
      "Epoch [937/1000], Loss: 0.1029\n",
      "Epoch [937/1000], Loss: 0.0718\n",
      "Epoch [937/1000], Loss: 0.0954\n",
      "Epoch [937/1000], Loss: 0.1373\n",
      "Epoch [937/1000], Loss: 0.1540\n",
      "Epoch [937/1000], Loss: 0.1442\n",
      "Epoch [937/1000], Loss: 0.1075\n",
      "tensor(2.2787, grad_fn=<MeanBackward0>)\n",
      "937\n",
      "Epoch [938/1000], Loss: 0.0918\n",
      "Epoch [938/1000], Loss: 0.1023\n",
      "Epoch [938/1000], Loss: 0.1392\n",
      "Epoch [938/1000], Loss: 0.2061\n",
      "Epoch [938/1000], Loss: 0.2749\n",
      "Epoch [938/1000], Loss: 0.3422\n",
      "Epoch [938/1000], Loss: 0.2900\n",
      "Epoch [938/1000], Loss: 0.2484\n",
      "Epoch [938/1000], Loss: 0.1396\n",
      "Epoch [938/1000], Loss: 0.2163\n",
      "Epoch [938/1000], Loss: 0.4027\n",
      "tensor(2.6086, grad_fn=<MeanBackward0>)\n",
      "938\n",
      "Epoch [939/1000], Loss: 0.5285\n",
      "Epoch [939/1000], Loss: 0.5884\n",
      "Epoch [939/1000], Loss: 0.4625\n",
      "Epoch [939/1000], Loss: 0.2925\n",
      "Epoch [939/1000], Loss: 0.1017\n",
      "Epoch [939/1000], Loss: 0.2265\n",
      "Epoch [939/1000], Loss: 0.3336\n",
      "Epoch [939/1000], Loss: 0.4250\n",
      "Epoch [939/1000], Loss: 0.4408\n",
      "Epoch [939/1000], Loss: 0.3828\n",
      "Epoch [939/1000], Loss: 0.2996\n",
      "tensor(2.2300, grad_fn=<MeanBackward0>)\n",
      "939\n",
      "Epoch [940/1000], Loss: 0.1680\n",
      "Epoch [940/1000], Loss: 0.1232\n",
      "Epoch [940/1000], Loss: 0.1593\n",
      "Epoch [940/1000], Loss: 0.1883\n",
      "Epoch [940/1000], Loss: 0.1867\n",
      "Epoch [940/1000], Loss: 0.1849\n",
      "Epoch [940/1000], Loss: 0.1341\n",
      "Epoch [940/1000], Loss: 0.1117\n",
      "Epoch [940/1000], Loss: 0.1149\n",
      "Epoch [940/1000], Loss: 0.1659\n",
      "Epoch [940/1000], Loss: 0.2342\n",
      "tensor(2.1474, grad_fn=<MeanBackward0>)\n",
      "940\n",
      "Epoch [941/1000], Loss: 0.2173\n",
      "Epoch [941/1000], Loss: 0.1647\n",
      "Epoch [941/1000], Loss: 0.1655\n",
      "Epoch [941/1000], Loss: 0.0986\n",
      "Epoch [941/1000], Loss: 0.1412\n",
      "Epoch [941/1000], Loss: 0.1948\n",
      "Epoch [941/1000], Loss: 0.1811\n",
      "Epoch [941/1000], Loss: 0.1338\n",
      "Epoch [941/1000], Loss: 0.0779\n",
      "Epoch [941/1000], Loss: 0.0859\n",
      "Epoch [941/1000], Loss: 0.1008\n",
      "tensor(2.2178, grad_fn=<MeanBackward0>)\n",
      "941\n",
      "Epoch [942/1000], Loss: 0.0795\n",
      "Epoch [942/1000], Loss: 0.0913\n",
      "Epoch [942/1000], Loss: 0.1017\n",
      "Epoch [942/1000], Loss: 0.1224\n",
      "Epoch [942/1000], Loss: 0.1129\n",
      "Epoch [942/1000], Loss: 0.1130\n",
      "Epoch [942/1000], Loss: 0.1181\n",
      "Epoch [942/1000], Loss: 0.1165\n",
      "Epoch [942/1000], Loss: 0.1864\n",
      "Epoch [942/1000], Loss: 0.1976\n",
      "Epoch [942/1000], Loss: 0.1608\n",
      "tensor(2.3763, grad_fn=<MeanBackward0>)\n",
      "942\n",
      "Epoch [943/1000], Loss: 0.1577\n",
      "Epoch [943/1000], Loss: 0.1288\n",
      "Epoch [943/1000], Loss: 0.1780\n",
      "Epoch [943/1000], Loss: 0.2628\n",
      "Epoch [943/1000], Loss: 0.3510\n",
      "Epoch [943/1000], Loss: 0.4369\n",
      "Epoch [943/1000], Loss: 0.4130\n",
      "Epoch [943/1000], Loss: 0.4547\n",
      "Epoch [943/1000], Loss: 0.3388\n",
      "Epoch [943/1000], Loss: 0.1357\n",
      "Epoch [943/1000], Loss: 0.1799\n",
      "tensor(2.5302, grad_fn=<MeanBackward0>)\n",
      "943\n",
      "Epoch [944/1000], Loss: 0.3512\n",
      "Epoch [944/1000], Loss: 0.4392\n",
      "Epoch [944/1000], Loss: 0.3964\n",
      "Epoch [944/1000], Loss: 0.2691\n",
      "Epoch [944/1000], Loss: 0.0921\n",
      "Epoch [944/1000], Loss: 0.1727\n",
      "Epoch [944/1000], Loss: 0.2535\n",
      "Epoch [944/1000], Loss: 0.2904\n",
      "Epoch [944/1000], Loss: 0.2624\n",
      "Epoch [944/1000], Loss: 0.2331\n",
      "Epoch [944/1000], Loss: 0.1999\n",
      "tensor(2.2764, grad_fn=<MeanBackward0>)\n",
      "944\n",
      "Epoch [945/1000], Loss: 0.1426\n",
      "Epoch [945/1000], Loss: 0.1256\n",
      "Epoch [945/1000], Loss: 0.1440\n",
      "Epoch [945/1000], Loss: 0.1556\n",
      "Epoch [945/1000], Loss: 0.1621\n",
      "Epoch [945/1000], Loss: 0.2030\n",
      "Epoch [945/1000], Loss: 0.1583\n",
      "Epoch [945/1000], Loss: 0.1398\n",
      "Epoch [945/1000], Loss: 0.1234\n",
      "Epoch [945/1000], Loss: 0.1492\n",
      "Epoch [945/1000], Loss: 0.2788\n",
      "tensor(2.0798, grad_fn=<MeanBackward0>)\n",
      "945\n",
      "Epoch [946/1000], Loss: 0.3157\n",
      "Epoch [946/1000], Loss: 0.3023\n",
      "Epoch [946/1000], Loss: 0.2885\n",
      "Epoch [946/1000], Loss: 0.1530\n",
      "Epoch [946/1000], Loss: 0.1165\n",
      "Epoch [946/1000], Loss: 0.2276\n",
      "Epoch [946/1000], Loss: 0.2590\n",
      "Epoch [946/1000], Loss: 0.2060\n",
      "Epoch [946/1000], Loss: 0.1983\n",
      "Epoch [946/1000], Loss: 0.1564\n",
      "Epoch [946/1000], Loss: 0.0993\n",
      "tensor(2.1779, grad_fn=<MeanBackward0>)\n",
      "946\n",
      "Epoch [947/1000], Loss: 0.1135\n",
      "Epoch [947/1000], Loss: 0.1295\n",
      "Epoch [947/1000], Loss: 0.1518\n",
      "Epoch [947/1000], Loss: 0.1744\n",
      "Epoch [947/1000], Loss: 0.2009\n",
      "Epoch [947/1000], Loss: 0.1888\n",
      "Epoch [947/1000], Loss: 0.1551\n",
      "Epoch [947/1000], Loss: 0.1275\n",
      "Epoch [947/1000], Loss: 0.1781\n",
      "Epoch [947/1000], Loss: 0.2698\n",
      "Epoch [947/1000], Loss: 0.3482\n",
      "tensor(2.5671, grad_fn=<MeanBackward0>)\n",
      "947\n",
      "Epoch [948/1000], Loss: 0.3711\n",
      "Epoch [948/1000], Loss: 0.2976\n",
      "Epoch [948/1000], Loss: 0.1950\n",
      "Epoch [948/1000], Loss: 0.2079\n",
      "Epoch [948/1000], Loss: 0.3065\n",
      "Epoch [948/1000], Loss: 0.4352\n",
      "Epoch [948/1000], Loss: 0.4611\n",
      "Epoch [948/1000], Loss: 0.5821\n",
      "Epoch [948/1000], Loss: 0.5530\n",
      "Epoch [948/1000], Loss: 0.3461\n",
      "Epoch [948/1000], Loss: 0.2380\n",
      "tensor(2.3514, grad_fn=<MeanBackward0>)\n",
      "948\n",
      "Epoch [949/1000], Loss: 0.1349\n",
      "Epoch [949/1000], Loss: 0.2328\n",
      "Epoch [949/1000], Loss: 0.2892\n",
      "Epoch [949/1000], Loss: 0.2605\n",
      "Epoch [949/1000], Loss: 0.1676\n",
      "Epoch [949/1000], Loss: 0.1433\n",
      "Epoch [949/1000], Loss: 0.1513\n",
      "Epoch [949/1000], Loss: 0.1315\n",
      "Epoch [949/1000], Loss: 0.1300\n",
      "Epoch [949/1000], Loss: 0.1432\n",
      "Epoch [949/1000], Loss: 0.1569\n",
      "tensor(2.1711, grad_fn=<MeanBackward0>)\n",
      "949\n",
      "Epoch [950/1000], Loss: 0.1693\n",
      "Epoch [950/1000], Loss: 0.1510\n",
      "Epoch [950/1000], Loss: 0.1322\n",
      "Epoch [950/1000], Loss: 0.1112\n",
      "Epoch [950/1000], Loss: 0.1480\n",
      "Epoch [950/1000], Loss: 0.2296\n",
      "Epoch [950/1000], Loss: 0.2004\n",
      "Epoch [950/1000], Loss: 0.1140\n",
      "Epoch [950/1000], Loss: 0.0740\n",
      "Epoch [950/1000], Loss: 0.0998\n",
      "Epoch [950/1000], Loss: 0.1442\n",
      "tensor(2.1952, grad_fn=<MeanBackward0>)\n",
      "950\n",
      "Epoch [951/1000], Loss: 0.1369\n",
      "Epoch [951/1000], Loss: 0.1164\n",
      "Epoch [951/1000], Loss: 0.1241\n",
      "Epoch [951/1000], Loss: 0.0980\n",
      "Epoch [951/1000], Loss: 0.0895\n",
      "Epoch [951/1000], Loss: 0.0775\n",
      "Epoch [951/1000], Loss: 0.0885\n",
      "Epoch [951/1000], Loss: 0.0796\n",
      "Epoch [951/1000], Loss: 0.1196\n",
      "Epoch [951/1000], Loss: 0.1321\n",
      "Epoch [951/1000], Loss: 0.1325\n",
      "tensor(2.3152, grad_fn=<MeanBackward0>)\n",
      "951\n",
      "Epoch [952/1000], Loss: 0.0917\n",
      "Epoch [952/1000], Loss: 0.1061\n",
      "Epoch [952/1000], Loss: 0.1451\n",
      "Epoch [952/1000], Loss: 0.1986\n",
      "Epoch [952/1000], Loss: 0.2383\n",
      "Epoch [952/1000], Loss: 0.2478\n",
      "Epoch [952/1000], Loss: 0.1794\n",
      "Epoch [952/1000], Loss: 0.1528\n",
      "Epoch [952/1000], Loss: 0.1256\n",
      "Epoch [952/1000], Loss: 0.2734\n",
      "Epoch [952/1000], Loss: 0.3776\n",
      "tensor(2.6049, grad_fn=<MeanBackward0>)\n",
      "952\n",
      "Epoch [953/1000], Loss: 0.4751\n",
      "Epoch [953/1000], Loss: 0.4829\n",
      "Epoch [953/1000], Loss: 0.3365\n",
      "Epoch [953/1000], Loss: 0.2102\n",
      "Epoch [953/1000], Loss: 0.1787\n",
      "Epoch [953/1000], Loss: 0.3059\n",
      "Epoch [953/1000], Loss: 0.3604\n",
      "Epoch [953/1000], Loss: 0.5055\n",
      "Epoch [953/1000], Loss: 0.5012\n",
      "Epoch [953/1000], Loss: 0.3736\n",
      "Epoch [953/1000], Loss: 0.2605\n",
      "tensor(2.2690, grad_fn=<MeanBackward0>)\n",
      "953\n",
      "Epoch [954/1000], Loss: 0.1637\n",
      "Epoch [954/1000], Loss: 0.1686\n",
      "Epoch [954/1000], Loss: 0.2138\n",
      "Epoch [954/1000], Loss: 0.1975\n",
      "Epoch [954/1000], Loss: 0.1509\n",
      "Epoch [954/1000], Loss: 0.1453\n",
      "Epoch [954/1000], Loss: 0.1371\n",
      "Epoch [954/1000], Loss: 0.1049\n",
      "Epoch [954/1000], Loss: 0.1037\n",
      "Epoch [954/1000], Loss: 0.1357\n",
      "Epoch [954/1000], Loss: 0.1962\n",
      "tensor(2.1224, grad_fn=<MeanBackward0>)\n",
      "954\n",
      "Epoch [955/1000], Loss: 0.2218\n",
      "Epoch [955/1000], Loss: 0.2249\n",
      "Epoch [955/1000], Loss: 0.2338\n",
      "Epoch [955/1000], Loss: 0.1419\n",
      "Epoch [955/1000], Loss: 0.1302\n",
      "Epoch [955/1000], Loss: 0.2629\n",
      "Epoch [955/1000], Loss: 0.3071\n",
      "Epoch [955/1000], Loss: 0.3150\n",
      "Epoch [955/1000], Loss: 0.2539\n",
      "Epoch [955/1000], Loss: 0.1330\n",
      "Epoch [955/1000], Loss: 0.0973\n",
      "tensor(2.1593, grad_fn=<MeanBackward0>)\n",
      "955\n",
      "Epoch [956/1000], Loss: 0.1743\n",
      "Epoch [956/1000], Loss: 0.2212\n",
      "Epoch [956/1000], Loss: 0.2382\n",
      "Epoch [956/1000], Loss: 0.2231\n",
      "Epoch [956/1000], Loss: 0.1635\n",
      "Epoch [956/1000], Loss: 0.1158\n",
      "Epoch [956/1000], Loss: 0.1210\n",
      "Epoch [956/1000], Loss: 0.1191\n",
      "Epoch [956/1000], Loss: 0.1853\n",
      "Epoch [956/1000], Loss: 0.2140\n",
      "Epoch [956/1000], Loss: 0.2382\n",
      "tensor(2.3980, grad_fn=<MeanBackward0>)\n",
      "956\n",
      "Epoch [957/1000], Loss: 0.2587\n",
      "Epoch [957/1000], Loss: 0.1899\n",
      "Epoch [957/1000], Loss: 0.1123\n",
      "Epoch [957/1000], Loss: 0.2057\n",
      "Epoch [957/1000], Loss: 0.3494\n",
      "Epoch [957/1000], Loss: 0.4971\n",
      "Epoch [957/1000], Loss: 0.4925\n",
      "Epoch [957/1000], Loss: 0.5498\n",
      "Epoch [957/1000], Loss: 0.4678\n",
      "Epoch [957/1000], Loss: 0.2502\n",
      "Epoch [957/1000], Loss: 0.1230\n",
      "tensor(2.5050, grad_fn=<MeanBackward0>)\n",
      "957\n",
      "Epoch [958/1000], Loss: 0.2470\n",
      "Epoch [958/1000], Loss: 0.3792\n",
      "Epoch [958/1000], Loss: 0.3980\n",
      "Epoch [958/1000], Loss: 0.3234\n",
      "Epoch [958/1000], Loss: 0.2227\n",
      "Epoch [958/1000], Loss: 0.2215\n",
      "Epoch [958/1000], Loss: 0.2579\n",
      "Epoch [958/1000], Loss: 0.2442\n",
      "Epoch [958/1000], Loss: 0.2284\n",
      "Epoch [958/1000], Loss: 0.1937\n",
      "Epoch [958/1000], Loss: 0.1645\n",
      "tensor(2.2253, grad_fn=<MeanBackward0>)\n",
      "958\n",
      "Epoch [959/1000], Loss: 0.1524\n",
      "Epoch [959/1000], Loss: 0.1389\n",
      "Epoch [959/1000], Loss: 0.1616\n",
      "Epoch [959/1000], Loss: 0.1410\n",
      "Epoch [959/1000], Loss: 0.1372\n",
      "Epoch [959/1000], Loss: 0.2325\n",
      "Epoch [959/1000], Loss: 0.2597\n",
      "Epoch [959/1000], Loss: 0.2636\n",
      "Epoch [959/1000], Loss: 0.1727\n",
      "Epoch [959/1000], Loss: 0.1055\n",
      "Epoch [959/1000], Loss: 0.1687\n",
      "tensor(2.1316, grad_fn=<MeanBackward0>)\n",
      "959\n",
      "Epoch [960/1000], Loss: 0.2734\n",
      "Epoch [960/1000], Loss: 0.3214\n",
      "Epoch [960/1000], Loss: 0.3393\n",
      "Epoch [960/1000], Loss: 0.2627\n",
      "Epoch [960/1000], Loss: 0.1472\n",
      "Epoch [960/1000], Loss: 0.1266\n",
      "Epoch [960/1000], Loss: 0.1897\n",
      "Epoch [960/1000], Loss: 0.2059\n",
      "Epoch [960/1000], Loss: 0.2172\n",
      "Epoch [960/1000], Loss: 0.2323\n",
      "Epoch [960/1000], Loss: 0.1745\n",
      "tensor(2.3216, grad_fn=<MeanBackward0>)\n",
      "960\n",
      "Epoch [961/1000], Loss: 0.1095\n",
      "Epoch [961/1000], Loss: 0.1029\n",
      "Epoch [961/1000], Loss: 0.1379\n",
      "Epoch [961/1000], Loss: 0.2038\n",
      "Epoch [961/1000], Loss: 0.2539\n",
      "Epoch [961/1000], Loss: 0.3192\n",
      "Epoch [961/1000], Loss: 0.2894\n",
      "Epoch [961/1000], Loss: 0.3203\n",
      "Epoch [961/1000], Loss: 0.2548\n",
      "Epoch [961/1000], Loss: 0.1655\n",
      "Epoch [961/1000], Loss: 0.3104\n",
      "tensor(2.6387, grad_fn=<MeanBackward0>)\n",
      "961\n",
      "Epoch [962/1000], Loss: 0.5087\n",
      "Epoch [962/1000], Loss: 0.5658\n",
      "Epoch [962/1000], Loss: 0.5198\n",
      "Epoch [962/1000], Loss: 0.3927\n",
      "Epoch [962/1000], Loss: 0.1940\n",
      "Epoch [962/1000], Loss: 0.1613\n",
      "Epoch [962/1000], Loss: 0.2679\n",
      "Epoch [962/1000], Loss: 0.4169\n",
      "Epoch [962/1000], Loss: 0.4231\n",
      "Epoch [962/1000], Loss: 0.3341\n",
      "Epoch [962/1000], Loss: 0.2649\n",
      "tensor(2.2338, grad_fn=<MeanBackward0>)\n",
      "962\n",
      "Epoch [963/1000], Loss: 0.1528\n",
      "Epoch [963/1000], Loss: 0.1290\n",
      "Epoch [963/1000], Loss: 0.1738\n",
      "Epoch [963/1000], Loss: 0.1724\n",
      "Epoch [963/1000], Loss: 0.1444\n",
      "Epoch [963/1000], Loss: 0.1571\n",
      "Epoch [963/1000], Loss: 0.1551\n",
      "Epoch [963/1000], Loss: 0.1544\n",
      "Epoch [963/1000], Loss: 0.1099\n",
      "Epoch [963/1000], Loss: 0.1237\n",
      "Epoch [963/1000], Loss: 0.1795\n",
      "tensor(2.0957, grad_fn=<MeanBackward0>)\n",
      "963\n",
      "Epoch [964/1000], Loss: 0.2541\n",
      "Epoch [964/1000], Loss: 0.2909\n",
      "Epoch [964/1000], Loss: 0.2701\n",
      "Epoch [964/1000], Loss: 0.1883\n",
      "Epoch [964/1000], Loss: 0.0987\n",
      "Epoch [964/1000], Loss: 0.2076\n",
      "Epoch [964/1000], Loss: 0.2732\n",
      "Epoch [964/1000], Loss: 0.2892\n",
      "Epoch [964/1000], Loss: 0.2931\n",
      "Epoch [964/1000], Loss: 0.2457\n",
      "Epoch [964/1000], Loss: 0.1229\n",
      "tensor(2.2039, grad_fn=<MeanBackward0>)\n",
      "964\n",
      "Epoch [965/1000], Loss: 0.1242\n",
      "Epoch [965/1000], Loss: 0.1786\n",
      "Epoch [965/1000], Loss: 0.2569\n",
      "Epoch [965/1000], Loss: 0.2607\n",
      "Epoch [965/1000], Loss: 0.2515\n",
      "Epoch [965/1000], Loss: 0.2143\n",
      "Epoch [965/1000], Loss: 0.1574\n",
      "Epoch [965/1000], Loss: 0.1451\n",
      "Epoch [965/1000], Loss: 0.1550\n",
      "Epoch [965/1000], Loss: 0.2456\n",
      "Epoch [965/1000], Loss: 0.3575\n",
      "tensor(2.5752, grad_fn=<MeanBackward0>)\n",
      "965\n",
      "Epoch [966/1000], Loss: 0.4499\n",
      "Epoch [966/1000], Loss: 0.4287\n",
      "Epoch [966/1000], Loss: 0.3180\n",
      "Epoch [966/1000], Loss: 0.1683\n",
      "Epoch [966/1000], Loss: 0.1726\n",
      "Epoch [966/1000], Loss: 0.3165\n",
      "Epoch [966/1000], Loss: 0.4077\n",
      "Epoch [966/1000], Loss: 0.5399\n",
      "Epoch [966/1000], Loss: 0.5246\n",
      "Epoch [966/1000], Loss: 0.4109\n",
      "Epoch [966/1000], Loss: 0.2942\n",
      "tensor(2.2644, grad_fn=<MeanBackward0>)\n",
      "966\n",
      "Epoch [967/1000], Loss: 0.1645\n",
      "Epoch [967/1000], Loss: 0.1965\n",
      "Epoch [967/1000], Loss: 0.2708\n",
      "Epoch [967/1000], Loss: 0.2619\n",
      "Epoch [967/1000], Loss: 0.2324\n",
      "Epoch [967/1000], Loss: 0.1985\n",
      "Epoch [967/1000], Loss: 0.2149\n",
      "Epoch [967/1000], Loss: 0.1686\n",
      "Epoch [967/1000], Loss: 0.1504\n",
      "Epoch [967/1000], Loss: 0.1237\n",
      "Epoch [967/1000], Loss: 0.1478\n",
      "tensor(2.2331, grad_fn=<MeanBackward0>)\n",
      "967\n",
      "Epoch [968/1000], Loss: 0.1418\n",
      "Epoch [968/1000], Loss: 0.1278\n",
      "Epoch [968/1000], Loss: 0.1706\n",
      "Epoch [968/1000], Loss: 0.1420\n",
      "Epoch [968/1000], Loss: 0.1337\n",
      "Epoch [968/1000], Loss: 0.1587\n",
      "Epoch [968/1000], Loss: 0.1802\n",
      "Epoch [968/1000], Loss: 0.2476\n",
      "Epoch [968/1000], Loss: 0.2456\n",
      "Epoch [968/1000], Loss: 0.1673\n",
      "Epoch [968/1000], Loss: 0.0813\n",
      "tensor(2.2521, grad_fn=<MeanBackward0>)\n",
      "968\n",
      "Epoch [969/1000], Loss: 0.1210\n",
      "Epoch [969/1000], Loss: 0.2086\n",
      "Epoch [969/1000], Loss: 0.2444\n",
      "Epoch [969/1000], Loss: 0.2768\n",
      "Epoch [969/1000], Loss: 0.2379\n",
      "Epoch [969/1000], Loss: 0.1785\n",
      "Epoch [969/1000], Loss: 0.1216\n",
      "Epoch [969/1000], Loss: 0.1013\n",
      "Epoch [969/1000], Loss: 0.1330\n",
      "Epoch [969/1000], Loss: 0.2113\n",
      "Epoch [969/1000], Loss: 0.2831\n",
      "tensor(2.5086, grad_fn=<MeanBackward0>)\n",
      "969\n",
      "Epoch [970/1000], Loss: 0.3615\n",
      "Epoch [970/1000], Loss: 0.3333\n",
      "Epoch [970/1000], Loss: 0.2418\n",
      "Epoch [970/1000], Loss: 0.1249\n",
      "Epoch [970/1000], Loss: 0.1930\n",
      "Epoch [970/1000], Loss: 0.3681\n",
      "Epoch [970/1000], Loss: 0.4455\n",
      "Epoch [970/1000], Loss: 0.5773\n",
      "Epoch [970/1000], Loss: 0.5606\n",
      "Epoch [970/1000], Loss: 0.4370\n",
      "Epoch [970/1000], Loss: 0.2812\n",
      "tensor(2.3297, grad_fn=<MeanBackward0>)\n",
      "970\n",
      "Epoch [971/1000], Loss: 0.1177\n",
      "Epoch [971/1000], Loss: 0.2185\n",
      "Epoch [971/1000], Loss: 0.3027\n",
      "Epoch [971/1000], Loss: 0.3023\n",
      "Epoch [971/1000], Loss: 0.2490\n",
      "Epoch [971/1000], Loss: 0.2026\n",
      "Epoch [971/1000], Loss: 0.2088\n",
      "Epoch [971/1000], Loss: 0.1978\n",
      "Epoch [971/1000], Loss: 0.2249\n",
      "Epoch [971/1000], Loss: 0.1892\n",
      "Epoch [971/1000], Loss: 0.1676\n",
      "tensor(2.2108, grad_fn=<MeanBackward0>)\n",
      "971\n",
      "Epoch [972/1000], Loss: 0.1083\n",
      "Epoch [972/1000], Loss: 0.1213\n",
      "Epoch [972/1000], Loss: 0.1342\n",
      "Epoch [972/1000], Loss: 0.1230\n",
      "Epoch [972/1000], Loss: 0.1253\n",
      "Epoch [972/1000], Loss: 0.1623\n",
      "Epoch [972/1000], Loss: 0.1792\n",
      "Epoch [972/1000], Loss: 0.2402\n",
      "Epoch [972/1000], Loss: 0.1746\n",
      "Epoch [972/1000], Loss: 0.1208\n",
      "Epoch [972/1000], Loss: 0.1550\n",
      "tensor(2.1469, grad_fn=<MeanBackward0>)\n",
      "972\n",
      "Epoch [973/1000], Loss: 0.2195\n",
      "Epoch [973/1000], Loss: 0.3245\n",
      "Epoch [973/1000], Loss: 0.3305\n",
      "Epoch [973/1000], Loss: 0.2874\n",
      "Epoch [973/1000], Loss: 0.1479\n",
      "Epoch [973/1000], Loss: 0.1305\n",
      "Epoch [973/1000], Loss: 0.2088\n",
      "Epoch [973/1000], Loss: 0.2613\n",
      "Epoch [973/1000], Loss: 0.2840\n",
      "Epoch [973/1000], Loss: 0.2731\n",
      "Epoch [973/1000], Loss: 0.1656\n",
      "tensor(2.3065, grad_fn=<MeanBackward0>)\n",
      "973\n",
      "Epoch [974/1000], Loss: 0.0952\n",
      "Epoch [974/1000], Loss: 0.1426\n",
      "Epoch [974/1000], Loss: 0.2241\n",
      "Epoch [974/1000], Loss: 0.2644\n",
      "Epoch [974/1000], Loss: 0.2586\n",
      "Epoch [974/1000], Loss: 0.2457\n",
      "Epoch [974/1000], Loss: 0.2040\n",
      "Epoch [974/1000], Loss: 0.2058\n",
      "Epoch [974/1000], Loss: 0.1621\n",
      "Epoch [974/1000], Loss: 0.1768\n",
      "Epoch [974/1000], Loss: 0.3061\n",
      "tensor(2.6354, grad_fn=<MeanBackward0>)\n",
      "974\n",
      "Epoch [975/1000], Loss: 0.4704\n",
      "Epoch [975/1000], Loss: 0.4905\n",
      "Epoch [975/1000], Loss: 0.4341\n",
      "Epoch [975/1000], Loss: 0.2911\n",
      "Epoch [975/1000], Loss: 0.1542\n",
      "Epoch [975/1000], Loss: 0.1861\n",
      "Epoch [975/1000], Loss: 0.3294\n",
      "Epoch [975/1000], Loss: 0.4810\n",
      "Epoch [975/1000], Loss: 0.5242\n",
      "Epoch [975/1000], Loss: 0.4856\n",
      "Epoch [975/1000], Loss: 0.3903\n",
      "tensor(2.1775, grad_fn=<MeanBackward0>)\n",
      "975\n",
      "Epoch [976/1000], Loss: 0.2231\n",
      "Epoch [976/1000], Loss: 0.1417\n",
      "Epoch [976/1000], Loss: 0.1705\n",
      "Epoch [976/1000], Loss: 0.2274\n",
      "Epoch [976/1000], Loss: 0.2435\n",
      "Epoch [976/1000], Loss: 0.2054\n",
      "Epoch [976/1000], Loss: 0.1508\n",
      "Epoch [976/1000], Loss: 0.0953\n",
      "Epoch [976/1000], Loss: 0.1102\n",
      "Epoch [976/1000], Loss: 0.1180\n",
      "Epoch [976/1000], Loss: 0.1203\n",
      "tensor(2.2222, grad_fn=<MeanBackward0>)\n",
      "976\n",
      "Epoch [977/1000], Loss: 0.1231\n",
      "Epoch [977/1000], Loss: 0.1300\n",
      "Epoch [977/1000], Loss: 0.1489\n",
      "Epoch [977/1000], Loss: 0.1166\n",
      "Epoch [977/1000], Loss: 0.1012\n",
      "Epoch [977/1000], Loss: 0.1669\n",
      "Epoch [977/1000], Loss: 0.1991\n",
      "Epoch [977/1000], Loss: 0.2566\n",
      "Epoch [977/1000], Loss: 0.2128\n",
      "Epoch [977/1000], Loss: 0.1038\n",
      "Epoch [977/1000], Loss: 0.0803\n",
      "tensor(2.1335, grad_fn=<MeanBackward0>)\n",
      "977\n",
      "Epoch [978/1000], Loss: 0.1960\n",
      "Epoch [978/1000], Loss: 0.2758\n",
      "Epoch [978/1000], Loss: 0.3308\n",
      "Epoch [978/1000], Loss: 0.3296\n",
      "Epoch [978/1000], Loss: 0.2294\n",
      "Epoch [978/1000], Loss: 0.1073\n",
      "Epoch [978/1000], Loss: 0.0980\n",
      "Epoch [978/1000], Loss: 0.1824\n",
      "Epoch [978/1000], Loss: 0.2642\n",
      "Epoch [978/1000], Loss: 0.2874\n",
      "Epoch [978/1000], Loss: 0.2533\n",
      "tensor(2.3932, grad_fn=<MeanBackward0>)\n",
      "978\n",
      "Epoch [979/1000], Loss: 0.1780\n",
      "Epoch [979/1000], Loss: 0.1109\n",
      "Epoch [979/1000], Loss: 0.1713\n",
      "Epoch [979/1000], Loss: 0.2475\n",
      "Epoch [979/1000], Loss: 0.3071\n",
      "Epoch [979/1000], Loss: 0.3637\n",
      "Epoch [979/1000], Loss: 0.3533\n",
      "Epoch [979/1000], Loss: 0.4282\n",
      "Epoch [979/1000], Loss: 0.3390\n",
      "Epoch [979/1000], Loss: 0.1973\n",
      "Epoch [979/1000], Loss: 0.1624\n",
      "tensor(2.5745, grad_fn=<MeanBackward0>)\n",
      "979\n",
      "Epoch [980/1000], Loss: 0.3508\n",
      "Epoch [980/1000], Loss: 0.4921\n",
      "Epoch [980/1000], Loss: 0.5031\n",
      "Epoch [980/1000], Loss: 0.4548\n",
      "Epoch [980/1000], Loss: 0.3281\n",
      "Epoch [980/1000], Loss: 0.1739\n",
      "Epoch [980/1000], Loss: 0.1761\n",
      "Epoch [980/1000], Loss: 0.2858\n",
      "Epoch [980/1000], Loss: 0.3703\n",
      "Epoch [980/1000], Loss: 0.3467\n",
      "Epoch [980/1000], Loss: 0.3396\n",
      "tensor(2.1364, grad_fn=<MeanBackward0>)\n",
      "980\n",
      "Epoch [981/1000], Loss: 0.2330\n",
      "Epoch [981/1000], Loss: 0.1580\n",
      "Epoch [981/1000], Loss: 0.1216\n",
      "Epoch [981/1000], Loss: 0.1192\n",
      "Epoch [981/1000], Loss: 0.1514\n",
      "Epoch [981/1000], Loss: 0.1569\n",
      "Epoch [981/1000], Loss: 0.1275\n",
      "Epoch [981/1000], Loss: 0.1168\n",
      "Epoch [981/1000], Loss: 0.0869\n",
      "Epoch [981/1000], Loss: 0.0834\n",
      "Epoch [981/1000], Loss: 0.1086\n",
      "tensor(2.2242, grad_fn=<MeanBackward0>)\n",
      "981\n",
      "Epoch [982/1000], Loss: 0.1277\n",
      "Epoch [982/1000], Loss: 0.1869\n",
      "Epoch [982/1000], Loss: 0.2147\n",
      "Epoch [982/1000], Loss: 0.1987\n",
      "Epoch [982/1000], Loss: 0.1233\n",
      "Epoch [982/1000], Loss: 0.0930\n",
      "Epoch [982/1000], Loss: 0.1415\n",
      "Epoch [982/1000], Loss: 0.2227\n",
      "Epoch [982/1000], Loss: 0.2574\n",
      "Epoch [982/1000], Loss: 0.2436\n",
      "Epoch [982/1000], Loss: 0.1716\n",
      "tensor(2.2714, grad_fn=<MeanBackward0>)\n",
      "982\n",
      "Epoch [983/1000], Loss: 0.0916\n",
      "Epoch [983/1000], Loss: 0.1095\n",
      "Epoch [983/1000], Loss: 0.2185\n",
      "Epoch [983/1000], Loss: 0.2903\n",
      "Epoch [983/1000], Loss: 0.2878\n",
      "Epoch [983/1000], Loss: 0.2485\n",
      "Epoch [983/1000], Loss: 0.1857\n",
      "Epoch [983/1000], Loss: 0.1310\n",
      "Epoch [983/1000], Loss: 0.1172\n",
      "Epoch [983/1000], Loss: 0.1808\n",
      "Epoch [983/1000], Loss: 0.2844\n",
      "tensor(2.6193, grad_fn=<MeanBackward0>)\n",
      "983\n",
      "Epoch [984/1000], Loss: 0.4093\n",
      "Epoch [984/1000], Loss: 0.4292\n",
      "Epoch [984/1000], Loss: 0.3736\n",
      "Epoch [984/1000], Loss: 0.2346\n",
      "Epoch [984/1000], Loss: 0.2204\n",
      "Epoch [984/1000], Loss: 0.2512\n",
      "Epoch [984/1000], Loss: 0.3045\n",
      "Epoch [984/1000], Loss: 0.4956\n",
      "Epoch [984/1000], Loss: 0.5536\n",
      "Epoch [984/1000], Loss: 0.5152\n",
      "Epoch [984/1000], Loss: 0.4360\n",
      "tensor(2.2088, grad_fn=<MeanBackward0>)\n",
      "984\n",
      "Epoch [985/1000], Loss: 0.2427\n",
      "Epoch [985/1000], Loss: 0.1486\n",
      "Epoch [985/1000], Loss: 0.2160\n",
      "Epoch [985/1000], Loss: 0.2591\n",
      "Epoch [985/1000], Loss: 0.2741\n",
      "Epoch [985/1000], Loss: 0.2105\n",
      "Epoch [985/1000], Loss: 0.1913\n",
      "Epoch [985/1000], Loss: 0.1450\n",
      "Epoch [985/1000], Loss: 0.1916\n",
      "Epoch [985/1000], Loss: 0.1881\n",
      "Epoch [985/1000], Loss: 0.1715\n",
      "tensor(2.2390, grad_fn=<MeanBackward0>)\n",
      "985\n",
      "Epoch [986/1000], Loss: 0.1085\n",
      "Epoch [986/1000], Loss: 0.1221\n",
      "Epoch [986/1000], Loss: 0.0963\n",
      "Epoch [986/1000], Loss: 0.0820\n",
      "Epoch [986/1000], Loss: 0.0932\n",
      "Epoch [986/1000], Loss: 0.0917\n",
      "Epoch [986/1000], Loss: 0.0944\n",
      "Epoch [986/1000], Loss: 0.0846\n",
      "Epoch [986/1000], Loss: 0.0990\n",
      "Epoch [986/1000], Loss: 0.1050\n",
      "Epoch [986/1000], Loss: 0.0892\n",
      "tensor(2.3229, grad_fn=<MeanBackward0>)\n",
      "986\n",
      "Epoch [987/1000], Loss: 0.0872\n",
      "Epoch [987/1000], Loss: 0.1154\n",
      "Epoch [987/1000], Loss: 0.1476\n",
      "Epoch [987/1000], Loss: 0.1561\n",
      "Epoch [987/1000], Loss: 0.1126\n",
      "Epoch [987/1000], Loss: 0.0799\n",
      "Epoch [987/1000], Loss: 0.0771\n",
      "Epoch [987/1000], Loss: 0.1008\n",
      "Epoch [987/1000], Loss: 0.1281\n",
      "Epoch [987/1000], Loss: 0.1262\n",
      "Epoch [987/1000], Loss: 0.1250\n",
      "tensor(2.3230, grad_fn=<MeanBackward0>)\n",
      "987\n",
      "Epoch [988/1000], Loss: 0.0734\n",
      "Epoch [988/1000], Loss: 0.0655\n",
      "Epoch [988/1000], Loss: 0.1290\n",
      "Epoch [988/1000], Loss: 0.1749\n",
      "Epoch [988/1000], Loss: 0.1699\n",
      "Epoch [988/1000], Loss: 0.1457\n",
      "Epoch [988/1000], Loss: 0.1264\n",
      "Epoch [988/1000], Loss: 0.0857\n",
      "Epoch [988/1000], Loss: 0.0802\n",
      "Epoch [988/1000], Loss: 0.1262\n",
      "Epoch [988/1000], Loss: 0.1879\n",
      "tensor(2.5022, grad_fn=<MeanBackward0>)\n",
      "988\n",
      "Epoch [989/1000], Loss: 0.2523\n",
      "Epoch [989/1000], Loss: 0.2457\n",
      "Epoch [989/1000], Loss: 0.1700\n",
      "Epoch [989/1000], Loss: 0.1214\n",
      "Epoch [989/1000], Loss: 0.1780\n",
      "Epoch [989/1000], Loss: 0.2337\n",
      "Epoch [989/1000], Loss: 0.2817\n",
      "Epoch [989/1000], Loss: 0.3833\n",
      "Epoch [989/1000], Loss: 0.4292\n",
      "Epoch [989/1000], Loss: 0.3530\n",
      "Epoch [989/1000], Loss: 0.2314\n",
      "tensor(2.3814, grad_fn=<MeanBackward0>)\n",
      "989\n",
      "Epoch [990/1000], Loss: 0.1053\n",
      "Epoch [990/1000], Loss: 0.2594\n",
      "Epoch [990/1000], Loss: 0.3673\n",
      "Epoch [990/1000], Loss: 0.4090\n",
      "Epoch [990/1000], Loss: 0.3570\n",
      "Epoch [990/1000], Loss: 0.2344\n",
      "Epoch [990/1000], Loss: 0.1363\n",
      "Epoch [990/1000], Loss: 0.1667\n",
      "Epoch [990/1000], Loss: 0.3038\n",
      "Epoch [990/1000], Loss: 0.3229\n",
      "Epoch [990/1000], Loss: 0.3428\n",
      "tensor(2.1731, grad_fn=<MeanBackward0>)\n",
      "990\n",
      "Epoch [991/1000], Loss: 0.2436\n",
      "Epoch [991/1000], Loss: 0.1347\n",
      "Epoch [991/1000], Loss: 0.0919\n",
      "Epoch [991/1000], Loss: 0.1528\n",
      "Epoch [991/1000], Loss: 0.2004\n",
      "Epoch [991/1000], Loss: 0.1810\n",
      "Epoch [991/1000], Loss: 0.1265\n",
      "Epoch [991/1000], Loss: 0.1063\n",
      "Epoch [991/1000], Loss: 0.0798\n",
      "Epoch [991/1000], Loss: 0.0909\n",
      "Epoch [991/1000], Loss: 0.0968\n",
      "tensor(2.2528, grad_fn=<MeanBackward0>)\n",
      "991\n",
      "Epoch [992/1000], Loss: 0.0869\n",
      "Epoch [992/1000], Loss: 0.0730\n",
      "Epoch [992/1000], Loss: 0.0823\n",
      "Epoch [992/1000], Loss: 0.0879\n",
      "Epoch [992/1000], Loss: 0.0801\n",
      "Epoch [992/1000], Loss: 0.1011\n",
      "Epoch [992/1000], Loss: 0.0888\n",
      "Epoch [992/1000], Loss: 0.1328\n",
      "Epoch [992/1000], Loss: 0.1028\n",
      "Epoch [992/1000], Loss: 0.0641\n",
      "Epoch [992/1000], Loss: 0.0675\n",
      "tensor(2.2253, grad_fn=<MeanBackward0>)\n",
      "992\n",
      "Epoch [993/1000], Loss: 0.1127\n",
      "Epoch [993/1000], Loss: 0.1347\n",
      "Epoch [993/1000], Loss: 0.1564\n",
      "Epoch [993/1000], Loss: 0.1291\n",
      "Epoch [993/1000], Loss: 0.0775\n",
      "Epoch [993/1000], Loss: 0.1050\n",
      "Epoch [993/1000], Loss: 0.0936\n",
      "Epoch [993/1000], Loss: 0.1169\n",
      "Epoch [993/1000], Loss: 0.1053\n",
      "Epoch [993/1000], Loss: 0.0638\n",
      "Epoch [993/1000], Loss: 0.0552\n",
      "tensor(2.2472, grad_fn=<MeanBackward0>)\n",
      "993\n",
      "Epoch [994/1000], Loss: 0.0612\n",
      "Epoch [994/1000], Loss: 0.0985\n",
      "Epoch [994/1000], Loss: 0.1164\n",
      "Epoch [994/1000], Loss: 0.1320\n",
      "Epoch [994/1000], Loss: 0.0950\n",
      "Epoch [994/1000], Loss: 0.0662\n",
      "Epoch [994/1000], Loss: 0.0826\n",
      "Epoch [994/1000], Loss: 0.1369\n",
      "Epoch [994/1000], Loss: 0.1582\n",
      "Epoch [994/1000], Loss: 0.1383\n",
      "Epoch [994/1000], Loss: 0.1002\n",
      "tensor(2.3132, grad_fn=<MeanBackward0>)\n",
      "994\n",
      "Epoch [995/1000], Loss: 0.0780\n",
      "Epoch [995/1000], Loss: 0.0789\n",
      "Epoch [995/1000], Loss: 0.1485\n",
      "Epoch [995/1000], Loss: 0.1921\n",
      "Epoch [995/1000], Loss: 0.1979\n",
      "Epoch [995/1000], Loss: 0.1860\n",
      "Epoch [995/1000], Loss: 0.1558\n",
      "Epoch [995/1000], Loss: 0.1201\n",
      "Epoch [995/1000], Loss: 0.0764\n",
      "Epoch [995/1000], Loss: 0.1259\n",
      "Epoch [995/1000], Loss: 0.2688\n",
      "tensor(2.5504, grad_fn=<MeanBackward0>)\n",
      "995\n",
      "Epoch [996/1000], Loss: 0.3563\n",
      "Epoch [996/1000], Loss: 0.3662\n",
      "Epoch [996/1000], Loss: 0.2995\n",
      "Epoch [996/1000], Loss: 0.1792\n",
      "Epoch [996/1000], Loss: 0.1191\n",
      "Epoch [996/1000], Loss: 0.2101\n",
      "Epoch [996/1000], Loss: 0.2994\n",
      "Epoch [996/1000], Loss: 0.4536\n",
      "Epoch [996/1000], Loss: 0.5264\n",
      "Epoch [996/1000], Loss: 0.4822\n",
      "Epoch [996/1000], Loss: 0.3677\n",
      "tensor(2.2623, grad_fn=<MeanBackward0>)\n",
      "996\n",
      "Epoch [997/1000], Loss: 0.1818\n",
      "Epoch [997/1000], Loss: 0.1236\n",
      "Epoch [997/1000], Loss: 0.2414\n",
      "Epoch [997/1000], Loss: 0.2990\n",
      "Epoch [997/1000], Loss: 0.3201\n",
      "Epoch [997/1000], Loss: 0.2257\n",
      "Epoch [997/1000], Loss: 0.1991\n",
      "Epoch [997/1000], Loss: 0.1533\n",
      "Epoch [997/1000], Loss: 0.2121\n",
      "Epoch [997/1000], Loss: 0.2119\n",
      "Epoch [997/1000], Loss: 0.2009\n",
      "tensor(2.2439, grad_fn=<MeanBackward0>)\n",
      "997\n",
      "Epoch [998/1000], Loss: 0.1116\n",
      "Epoch [998/1000], Loss: 0.1034\n",
      "Epoch [998/1000], Loss: 0.0967\n",
      "Epoch [998/1000], Loss: 0.1122\n",
      "Epoch [998/1000], Loss: 0.1182\n",
      "Epoch [998/1000], Loss: 0.0785\n",
      "Epoch [998/1000], Loss: 0.0874\n",
      "Epoch [998/1000], Loss: 0.0749\n",
      "Epoch [998/1000], Loss: 0.0812\n",
      "Epoch [998/1000], Loss: 0.0787\n",
      "Epoch [998/1000], Loss: 0.1216\n",
      "tensor(2.3666, grad_fn=<MeanBackward0>)\n",
      "998\n",
      "Epoch [999/1000], Loss: 0.1133\n",
      "Epoch [999/1000], Loss: 0.0811\n",
      "Epoch [999/1000], Loss: 0.1018\n",
      "Epoch [999/1000], Loss: 0.1238\n",
      "Epoch [999/1000], Loss: 0.0923\n",
      "Epoch [999/1000], Loss: 0.0715\n",
      "Epoch [999/1000], Loss: 0.0692\n",
      "Epoch [999/1000], Loss: 0.0971\n",
      "Epoch [999/1000], Loss: 0.1323\n",
      "Epoch [999/1000], Loss: 0.1162\n",
      "Epoch [999/1000], Loss: 0.0881\n",
      "tensor(2.3149, grad_fn=<MeanBackward0>)\n",
      "999\n",
      "Epoch [1000/1000], Loss: 0.0697\n",
      "Epoch [1000/1000], Loss: 0.0518\n",
      "Epoch [1000/1000], Loss: 0.0880\n",
      "Epoch [1000/1000], Loss: 0.1173\n",
      "Epoch [1000/1000], Loss: 0.0944\n",
      "Epoch [1000/1000], Loss: 0.0866\n",
      "Epoch [1000/1000], Loss: 0.0726\n",
      "Epoch [1000/1000], Loss: 0.0601\n",
      "Epoch [1000/1000], Loss: 0.0611\n",
      "Epoch [1000/1000], Loss: 0.0574\n",
      "Epoch [1000/1000], Loss: 0.0985\n",
      "tensor(2.3893, grad_fn=<MeanBackward0>)\n",
      "MAE: 1.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "#model.to(device)\n",
    "n_epochs=1000\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size= 100\n",
    "\n",
    "train_shape = img_data[np.where(np.isin(date_list, indicators_train['date']))].shape[0]\n",
    "batch_start = torch.arange(0, train_shape, batch_size)\n",
    "\n",
    "\n",
    "# Hold the best model\n",
    "best_mae= np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# Train the model\n",
    "#writer = SummaryWriter()\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            print(epoch)\n",
    "            for start in bar:\n",
    "                #print(start)\n",
    "            #for i, (inputs, targets) in enumerate(regression_task.trainloader):\n",
    "                inputs = torch.from_numpy(img_data[np.where(np.isin(date_list, indicators_train['date']))])[start:start+batch_size]\n",
    "                targets = torch.from_numpy(np.array(labels_list,dtype=np.float32)[np.where(np.isin(date_list, indicators_train['date']))[0]]).reshape(-1,1)[start:start+batch_size]\n",
    "                #print(inputs.shape)\n",
    "                #print(targets.shape)\n",
    "                # Zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                #outputs = model(inputs.to(device))\n",
    "                outputs = model(inputs)\n",
    "                #print(outputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #writer.add_scalar('Train Loss', loss.item(), i)\n",
    "\n",
    "                # Print training statistics\n",
    "                #if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "# evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    #X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    #print('val')\n",
    "    #print(X_val.shape)\n",
    "    X_val = torch.from_numpy(img_data[np.where(np.isin(date_list, indicators_val['date']))])\n",
    "    y_val = torch.from_numpy(np.array(labels_list,dtype=np.float32)[np.where(np.isin(date_list, indicators_val['date']))[0]]).reshape(-1,1)\n",
    "\n",
    "    y_pred = model(X_val)\n",
    "\n",
    "    mae = criterion(y_pred, y_val)\n",
    "    print(mae)\n",
    "    mae = float(mae)\n",
    "    history.append(mae)\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MAE: %.2f\" % best_mae)\n",
    "#print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_test = torch.from_numpy(img_data[np.where(np.isin(date_list, indicators_test['date']))])\n",
    "outputs_test =  torch.from_numpy(np.array(labels_list,dtype=np.float32)[np.where(np.isin(date_list, indicators_test['date']))[0]]).reshape(-1,1)\n",
    "y_pred = model(inputs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_test = np.array(date_list)[np.where(np.isin(date_list, indicators_test['date']))[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dates_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3147,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(outputs_test.reshape(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.concat([pd.Series(dates_test),pd.Series(outputs_test.reshape(-1)),pd.Series(y_pred.detach().numpy().reshape(-1))],axis=1).\\\n",
    "rename(columns={0:'date',1:'true',2:'pred'})\n",
    "out_daily = out.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.924612"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(out_daily['pred'] - out_daily['true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-08-07</th>\n",
       "      <td>2.125</td>\n",
       "      <td>5.490951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-08</th>\n",
       "      <td>5.875</td>\n",
       "      <td>4.910165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-09</th>\n",
       "      <td>5.375</td>\n",
       "      <td>5.573785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-10</th>\n",
       "      <td>6.500</td>\n",
       "      <td>4.839980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-12</th>\n",
       "      <td>5.750</td>\n",
       "      <td>5.293156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-08-13</th>\n",
       "      <td>5.250</td>\n",
       "      <td>4.801107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             true      pred\n",
       "date                       \n",
       "2024-08-07  2.125  5.490951\n",
       "2024-08-08  5.875  4.910165\n",
       "2024-08-09  5.375  5.573785\n",
       "2024-08-10  6.500  4.839980\n",
       "2024-08-12  5.750  5.293156\n",
       "2024-08-13  5.250  4.801107"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cis_subjective_fatigue'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('/Users/htr365/no_icloud/quantified_self_all/quantified_self/data_processing/ml_pipeline/prediction/amanda_spec_fatigue_predictions_relaxed.csv')\n",
    "out_daily.to_csv('/Users/htr365/no_icloud/quantified_self_all/quantified_self/data_processing/ml_pipeline/prediction/amanda_spec_daily_fatigue_prediction_relaxed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muscles",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
